{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "This Module is responsible for:\n",
        "\n",
        "\n",
        "*   **Hyperparameter tuning** (*hyperparameters of CNN-GRU including Number of filters, Kernel size, Pool size, Number of GRU units*) using GWO-WOA and comparing with other metaheuristic optimizers \n",
        "*   **Classifying the Sentiments of documents using the CNN-GRU**\n",
        "\n"
      ],
      "metadata": {
        "id": "eCvOzvqPWm2w"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zvdmGeAOlfBq"
      },
      "source": [
        "# Configuration"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install sklearn-deap\n",
        "! pip install np\n",
        "! pip install sklearn\n",
        "! pip install sklearn_nature_inspired_algorithms==0.4.3"
      ],
      "metadata": {
        "id": "W0tRuo8XFqKA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install Niapy"
      ],
      "metadata": {
        "id": "2ff26cPxFxB9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from math import sqrt\n",
        "\n",
        "from keras.wrappers.scikit_learn import KerasRegressor\n",
        "from sklearn_nature_inspired_algorithms.model_selection.nature_inspired_search_cv import NatureInspiredSearchCV\n",
        "from sklearn_nature_inspired_algorithms.helpers import score_by_generation_lineplot\n",
        "\n",
        "from evolutionary_search import EvolutionaryAlgorithmSearchCV"
      ],
      "metadata": {
        "id": "CsKgsvZ6F0oN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZB9bcnDNlqkA"
      },
      "outputs": [],
      "source": [
        "\n",
        "import string\n",
        "from __future__ import print_function\n",
        "\n",
        "import keras\n",
        "from keras.callbacks import EarlyStopping\n",
        "from keras.preprocessing import sequence\n",
        "from keras.models import Sequential\n",
        "from keras.layers import SimpleRNN, Dense, Dropout, Embedding, LSTM, GRU, Bidirectional, Activation, Conv1D, MaxPooling1D, GlobalMaxPooling1D, BatchNormalization\n",
        "from keras.optimizers import SGD\n",
        "from keras.layers import SpatialDropout1D, Flatten\n",
        "from keras.callbacks import Callback\n",
        "from keras.callbacks import ReduceLROnPlateau\n",
        "from keras import backend\n",
        "from keras.initializers import Constant\n",
        "from keras.regularizers import l2\n",
        "from sklearn.metrics import precision_recall_fscore_support as score\n",
        "\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "from matplotlib.pyplot import plot\n",
        "\n",
        "\n",
        "\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zO3HA-xLlxpV"
      },
      "outputs": [],
      "source": [
        "w2vMovieTxt = '/MYDRIVE/My Drive/Colab Notebooks/Proposal/Data/w2vMovie-MR-99-07-25.txt'\n",
        "w2vMovieM = '/MYDRIVE/My Drive/Colab Notebooks/Proposal/Data/w2vMovie-MR-99-07-25.h5'\n",
        "\n",
        "lstmModelTxt = '/MYDRIVE/My Drive/Colab Notebooks/Proposal/Data/lstm-99-07-25.txt'\n",
        "lstmModelH = '/MYDRIVE/My Drive/Colab Notebooks/Proposal/Data/lstm-99-07-25.h5'\n",
        "\n",
        "gruModelTxt = '/MYDRIVE/My Drive/Colab Notebooks/Proposal/Data/gru-99-07-25.txt'\n",
        "gruModelH = '/MYDRIVE/My Drive/Colab Notebooks/Proposal/Data/gru-99-07-25.h5'\n",
        "\n",
        "bilstmModelTxt = '/MYDRIVE/My Drive/Colab Notebooks/Proposal/Data/bilstm-99-07-25.txt'\n",
        "bilstmModelH = '/MYDRIVE/My Drive/Colab Notebooks/Proposal/Data/bilstm-99-07-25.h5'\n",
        "\n",
        "conv1ModelTxt = '/MYDRIVE/My Drive/Colab Notebooks/Proposal/Data/conv1-99-07-25.txt'\n",
        "conv1ModelH = '/MYDRIVE/My Drive/Colab Notebooks/Proposal/Data/conv1-99-07-25.h5'\n",
        "\n",
        "CLModelTxt = '/MYDRIVE/My Drive/Colab Notebooks/Proposal/Data/Cl-99-07-25.txt'\n",
        "CLModelH = '/MYDRIVE/My Drive/Colab Notebooks/Proposal/Data/Cl-99-07-25.h5'\n",
        "\n",
        "#This file includes tweets and topic number\n",
        "tweetsTopics = '/MYDRIVE/My Drive/Colab Notebooks/Proposal/Data/Tweets_Topics_990725.csv'\n",
        "\n",
        "#mr8Similar = '/MYDRIVE/My Drive/Colab Notebooks/Proposal/Data/JokerMostSimilar-99-07-26.csv'\n",
        "mr8Similar = '/MYDRIVE/My Drive/Colab Notebooks/Proposal/Data/Topics_8_MostSimilar.csv'\n",
        "mr20Similar = '/MYDRIVE/My Drive/Colab Notebooks/Proposal/Data/Topics_20_MostSimilar.csv'\n",
        "mr40Similar = '/MYDRIVE/My Drive/Colab Notebooks/Proposal/Data/Topics_40_MostSimilar.csv'\n",
        "mr80Similar = '/MYDRIVE/My Drive/Colab Notebooks/Proposal/Data/Topics_80_MostSimilar.csv'\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#allTrainingData = '/MYDRIVE/My Drive/Colab Notebooks/Proposal/Data/M-A-S140-Processed-98-11-09-03.csv'\n",
        "#allTrainingData = '/MYDRIVE/My Drive/Colab Notebooks/Proposal/Data/movie-pang-processed-98-12-08.csv'\n",
        "\n",
        "allTrainingData = '/MYDRIVE/My Drive/Colab Notebooks/Proposal/Data/Processed-Merged-Dataset-99-08-28.csv'\n",
        "pangLabelledDataset = '/MYDRIVE/My Drive/Colab Notebooks/Proposal/Data/MovieReview-Preprocessed-Pang-Ver2-Labelled.csv'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "04h5x_Cql87b"
      },
      "source": [
        "# Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vdjNqm4gmBHl"
      },
      "outputs": [],
      "source": [
        "#Making Word2vec\n",
        "embeddingDim = 500\n",
        "maxLen = embeddingDim\n",
        "testSize=0.2\n",
        "windowSize=5\n",
        "minCount=1 \n",
        "iterCount=10\n",
        "sG = 0 #skip gram wil be used\n",
        "\n",
        "\n",
        "#Making Model\n",
        "validationSplit = 0.2\n",
        "#maxFeatures = numWords\n",
        "maxLen = 500\n",
        "embeddingSize = 100\n",
        "isTrainable = False\n",
        "batchSize = 32\n",
        "\n",
        "\n",
        "\n",
        "epochsNum = 100\n",
        "\n",
        "unitsNum = 20\n",
        "\n",
        "\n",
        "# Convolution\n",
        "#kernelSize = 3\n",
        "kernelSize = 4\n",
        "\n",
        "filtersNum = 500\n",
        "\n",
        "\n",
        "poolSize = 2\n",
        "stridesNum = 1\n",
        "hidden_dims = 100\n",
        "\n",
        "# LSTM\n",
        "lstm_output_size = 70"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2mHnbyLJmI6x"
      },
      "source": [
        "# Log in to Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M59h9jOvmNwi"
      },
      "outputs": [],
      "source": [
        "#No need in local\n",
        "from google.colab import drive\n",
        "drive.mount('/MYDRIVE', force_remount=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FXEnjAczmYu4"
      },
      "source": [
        "# Loading Data to make Word2Vec Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bMYgLeylmdj6"
      },
      "outputs": [],
      "source": [
        "dfMR8 = pd.DataFrame()\n",
        "dfMR8 = pd.read_csv(mr80Similar, encoding = 'utf-8')\n",
        "print(dfMR8.head(3))\n",
        "dfMR81 = dfMR8\n",
        "print(len(dfMR81))\n",
        "\n",
        "print(dfMR81.columns)\n",
        "\n",
        "dfMR81.columns = [ 'i', 'docid','Probability']\n",
        "\n",
        "print(dfMR81.columns)\n",
        "\n",
        "print(dfMR81.head(3))\n",
        "\n",
        "print(len(dfMR81))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kC8i1TC2mlPO"
      },
      "outputs": [],
      "source": [
        "len(dfMR81['docid'].unique())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tWdYMMlzmp3_"
      },
      "source": [
        "# Loading All Training Labelled Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AbHLTsTcmuci"
      },
      "outputs": [],
      "source": [
        "allTrainingData"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_SxeYILzmwwv"
      },
      "outputs": [],
      "source": [
        "dfALLTraining = pd.DataFrame()\n",
        "dfALLTraining = pd.read_csv(allTrainingData, encoding = 'utf-8', header=None)\n",
        "print(dfALLTraining.head(3))\n",
        "print(len(dfALLTraining))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ymS6wFsnRcM"
      },
      "source": [
        "# Finding movie docs from training data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QZWcXHXCnV-I"
      },
      "outputs": [],
      "source": [
        "#This data is similar to topic 11\n",
        "dfTrainingMovies = pd.DataFrame()\n",
        "\n",
        "dfTrainingMovies = dfALLTraining.loc[dfALLTraining[0].isin(dfMR81['docid'])]\n",
        "print(len(dfTrainingMovies))\n",
        "print(dfTrainingMovies.head(3))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Prt55QgSnbjx"
      },
      "source": [
        "# Making Word2Vec Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pxO_qJnanfpb"
      },
      "outputs": [],
      "source": [
        "#embeddingDim = 100\n",
        "#Remove repeated rows\n",
        "dfMoviesNoRep = dfTrainingMovies\n",
        "dfMoviesNoRep.columns = ['i','TweetId', 'Tweet', 'Sentiment']\n",
        "print(dfMoviesNoRep.head(3))  \n",
        "dfMoviesNoRep.sort_values([\"TweetId\"], inplace = True) \n",
        "print(dfMoviesNoRep.head(3))  \n",
        "# dropping ALL duplicte values \n",
        "dfMoviesNoRep.drop_duplicates(subset =\"TweetId\", \n",
        "                     keep = 'first', inplace = True) \n",
        "print(len(dfMoviesNoRep))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4UOhx4mQnl1J"
      },
      "outputs": [],
      "source": [
        "print(len(dfMoviesNoRep[dfMoviesNoRep['Sentiment'] == 0]))\n",
        "print(len(dfMoviesNoRep[dfMoviesNoRep['Sentiment'] == 1]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NgXFcVMBnucc"
      },
      "outputs": [],
      "source": [
        "# Making a balance dataset with equal number of positive and negative tweets\n",
        "df1 = dfMoviesNoRep[dfMoviesNoRep['Sentiment'] == 1].sample(14089)\n",
        "df2 = dfMoviesNoRep[dfMoviesNoRep['Sentiment'] == 0].sample(14089)\n",
        "df = df1.append(df2)\n",
        "print(len(df[df['Sentiment'] == 1]))\n",
        "print(df.head())\n",
        "print(len(df))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-CE791dGn1Un"
      },
      "outputs": [],
      "source": [
        "dfPang = pd.DataFrame()\n",
        "dfPang = pd.read_csv(pangLabelledDataset, encoding = 'utf-8', header=None)\n",
        "print(dfPang.head(3))\n",
        "print(len(dfPang))\n",
        "dfPang.insert(0,'i',0)\n",
        "dfPang.columns = ['i' , 'TweetId', 'Tweet', 'Sentiment']\n",
        "df = df.append(dfPang)\n",
        "print(len(df))\n",
        "print(len(df[df['Sentiment'] == 1]))\n",
        "print(len(df[df['Sentiment'] == 0]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "56tTQIeIoAQ-"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "x_train_m, x_test_m, y_train_m, y_test_m = train_test_split( df['Tweet'], df['Sentiment'], test_size=0.2, random_state=10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U1fQz45doJSi"
      },
      "outputs": [],
      "source": [
        "print(len(x_test_m))\n",
        "print(len(x_train_m))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f33fNnvpoJ-Z"
      },
      "outputs": [],
      "source": [
        "print(len(y_test_m[y_test_m == 0]))\n",
        "print(len(y_test_m[y_test_m == 1]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qo2vIXCsoNvs"
      },
      "outputs": [],
      "source": [
        "print(len(y_train_m[y_train_m == 0]))\n",
        "print(len(y_train_m[y_train_m == 1]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_cnIcy3boRZC"
      },
      "outputs": [],
      "source": [
        "reviewMovieLines = list()\n",
        "for line in x_train_m:\n",
        "  reviewMovieLines.append( str(line).split() )\n",
        "\n",
        "for line in x_test_m:\n",
        "  reviewMovieLines.append( str(line).split() )\n",
        "\n",
        "print(len(reviewMovieLines))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zE_P6YtXoYtb"
      },
      "outputs": [],
      "source": [
        "import gensim\n",
        "\n",
        "\n",
        "w2vMovie = gensim.models.Word2Vec( sentences=reviewMovieLines,  window=windowSize, vector_size = embeddingDim, workers=4, min_count=minCount, epochs=iterCount, sg = sG )\n",
        "\n",
        "w2vGensimModel = gensim.models.Word2Vec()\n",
        "#vocab size\n",
        "\n",
        "#words = list( w2vMovie.wv.vocab )\n",
        "\n",
        "#print('Vocabulary size: %d' % len(words))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lbYZo7ycodl5"
      },
      "outputs": [],
      "source": [
        "w2vMovie.wv.save_word2vec_format( w2vMovieTxt, binary = False)\n",
        "w2vMovie.save(w2vMovieM)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s_oob1iFofxI"
      },
      "outputs": [],
      "source": [
        "w2vMovie.wv.most_similar('film')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FpDKO1xxogeg"
      },
      "outputs": [],
      "source": [
        "w2vMovie.wv.most_similar('movi')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rYCI6S2RovYo"
      },
      "source": [
        "# Using Word2Vec to input network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KOZBNdg5o1Tr"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "embeddingsIndex = {}\n",
        "f = open(os.path.join('', w2vMovieTxt ), encoding = \"utf-8\")\n",
        "\n",
        "for line in f:\n",
        "  values = line.split()\n",
        "  word = values[0]\n",
        "  coefs = np.asarray(values[1:])\n",
        "  embeddingsIndex[word] = coefs\n",
        "f.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "64ncNYi_o2fR"
      },
      "outputs": [],
      "source": [
        "tokenizerObj = Tokenizer()\n",
        "tokenizerObj.fit_on_texts(reviewMovieLines)\n",
        "sequences = tokenizerObj.texts_to_sequences(reviewMovieLines)\n",
        "\n",
        "print(type(sequences))\n",
        "\n",
        "reviewPad = pad_sequences( sequences, maxlen=maxLen)\n",
        "\n",
        "wordIndex = tokenizerObj.word_index\n",
        "print('Found %s unique tokens.' % len(wordIndex))\n",
        "\n",
        "#reviewPad = pad_sequences( sequences, maxlen=maxLen)\n",
        "#sentiment = df ['Sentiment'].values\n",
        "print(type(reviewPad))\n",
        "print(len(reviewPad))\n",
        "\n",
        "x_train_pad = reviewPad[ : len(x_train_m)]\n",
        "x_test_pad = reviewPad[ len(x_train_m) : ]\n",
        "y_train = y_train_m\n",
        "y_test = y_test_m\n",
        "\n",
        "print(type(x_train_pad))\n",
        "\n",
        "print('Shape of x_train_pad tensor:' , len(x_train_pad))\n",
        "print('Shape of y_train tensor:' , len(y_train))\n",
        "\n",
        "\n",
        "print('Shape of x_test_pad tensor:' , len(x_test_pad))\n",
        "print('Shape of y_test tensor:' , len( y_test ))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BTMZbW2So_De"
      },
      "outputs": [],
      "source": [
        "x = y_test[y_test == 0]\n",
        "print(len(x))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y6M2T5B0pBG6"
      },
      "outputs": [],
      "source": [
        "x = y_test[y_test == 1]\n",
        "print(len(x))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xI79h7F9pDkK"
      },
      "outputs": [],
      "source": [
        "x = y_train[y_train == 0]\n",
        "print(len(x))\n",
        "x = y_train[y_train == 1]\n",
        "print(len(x))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EzCruJ4wpGbW"
      },
      "outputs": [],
      "source": [
        "numWords = len(wordIndex) + 1\n",
        "print(numWords)\n",
        "embeddingMatrix = np.zeros( (numWords, embeddingDim) )\n",
        "#print(embeddingMatrix.shape)\n",
        "for word, i in wordIndex.items():\n",
        "  if i > numWords:\n",
        "    continue\n",
        "  embeddingVector = embeddingsIndex.get(word)\n",
        "  if embeddingVector is not None:\n",
        "    # words not found in embedding index will be all-zeros.\n",
        "    embeddingMatrix[i] = embeddingVector"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yZSlo_2KpJu3"
      },
      "outputs": [],
      "source": [
        "print(embeddingMatrix.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GtIBT5WGpKlS"
      },
      "outputs": [],
      "source": [
        "max_features = numWords"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o2zyH5I_2Msi"
      },
      "source": [
        "# HPO Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c49fxmw_2Qzq"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split,cross_val_score\n",
        "from sklearn.ensemble import RandomForestClassifier,RandomForestRegressor\n",
        "from sklearn.metrics import classification_report,confusion_matrix,accuracy_score\n",
        "from sklearn.neighbors import KNeighborsClassifier,KNeighborsRegressor\n",
        "from sklearn import datasets\n",
        "import scipy.stats as stats\n",
        "from keras.models import Sequential, Model\n",
        "from keras.layers import Dense, Input\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from keras.wrappers.scikit_learn import KerasClassifier\n",
        "from keras.callbacks import EarlyStopping\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn_nature_inspired_algorithms.model_selection.nature_inspired_search_cv import NatureInspiredSearchCV\n",
        "from sklearn_nature_inspired_algorithms.helpers import score_by_generation_lineplot"
      ],
      "metadata": {
        "id": "A1ldavMX4sqv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I7wPN6b7Bg-n"
      },
      "source": [
        "#  CNN + GRU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QjZG2V6kBmrq"
      },
      "outputs": [],
      "source": [
        "#def CNNGRU(neurons1=20, neurons2=10,batch_size=500,epochs=20,activation='relu',patience=3 ,loss='binary_crossentropy'):\n",
        "def CNNGRU(numFilters = 20, kernelSize = 5, poolSize = 2, gruUnits = 20):\n",
        "    print('Build model...')\n",
        "    modelCGRU = Sequential(name='CNNGRU')\n",
        "    modelCGRU.add(Embedding(numWords, embeddingDim , \n",
        "                      embeddings_initializer = Constant( embeddingMatrix), \n",
        "                      input_length=maxLen,\n",
        "                      trainable = isTrainable))\n",
        "    modelCGRU.add(Conv1D(filters= numFilters, kernel_size= kernelSize, strides= stridesNum, padding='same', activation='relu'))\n",
        "    modelCGRU.add(Dropout(0.4))\n",
        "    modelCGRU.add(MaxPooling1D(pool_size = poolSize))\n",
        "    modelCGRU.add(Dropout(0.6))\n",
        "    modelCGRU.add(GRU( gruUnits , dropout=0.6 ))\n",
        "    modelCGRU.add(Dropout(0.6))\n",
        "    modelCGRU.add(Dense(1, activation='sigmoid'))\n",
        "    modelCGRU.compile(loss='binary_crossentropy',\n",
        "              optimizer= 'adam',\n",
        "              metrics=['accuracy'])\n",
        "    print('Train...')\n",
        "\n",
        "    early_stopping = EarlyStopping(monitor=\"loss\", patience = 10)# early stop patience\n",
        "\n",
        "    historyCGRU = modelCGRU.fit(x_train_pad, y_train,\n",
        "          batch_size=32,\n",
        "          epochs=100,\n",
        "          callbacks = [early_stopping],\n",
        "          validation_data=(x_test_pad, y_test), verbose = 0 )#, callbacks=[rlrp])    \n",
        "    \n",
        "    \n",
        "    return modelCGRU"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "rf_params = {\n",
        "    'numFilters': [32, 64],\n",
        "    'kernelSize':[3, 4, 5, 6, 7],\n",
        "    'poolSize':[2, 3, 4],\n",
        "    'gruUnits':[10, 15, 20, 25]\n",
        "}"
      ],
      "metadata": {
        "id": "bjdb1Ehq9oRt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# GA"
      ],
      "metadata": {
        "id": "8GEu0skbim5B"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UvO1IXht2XDc"
      },
      "outputs": [],
      "source": [
        "from evolutionary_search import EvolutionaryAlgorithmSearchCV\n",
        "# Define the hyperparameter configuration space\n",
        "\n",
        "clf = KerasClassifier(build_fn=CNNGRU, verbose=0)\n",
        "# Set the hyperparameters of GA    \n",
        "ga1 = EvolutionaryAlgorithmSearchCV(estimator=clf,\n",
        "                                   params=rf_params,\n",
        "                                   scoring=\"accuracy\",\n",
        "                                   cv=3,\n",
        "                                   verbose=1,\n",
        "                                   population_size=20,\n",
        "                                   gene_mutation_prob=0.10,\n",
        "                                   gene_crossover_prob=0.5,\n",
        "                                   tournament_size=3,\n",
        "                                   generations_number=5,\n",
        "                                   n_jobs=1)\n",
        "ga1.fit(x_train_pad, y_train)\n",
        "print(ga1.best_params_)\n",
        "print(\"Accuracy:\"+ str(ga1.best_score_))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sxl4G9UhpWo3"
      },
      "source": [
        "# PSO"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from NiaPy.algorithms.basic import ParticleSwarmOptimization\n",
        "#clf = RandomForestClassifier(random_state=42)\n",
        "clf = KerasClassifier(build_fn=CNNGRU, verbose=0)\n",
        "\n",
        "algorithm = ParticleSwarmOptimization() # when custom algorithm is provided random_state is ignored\n",
        "\n",
        "nia_search = NatureInspiredSearchCV(\n",
        "  clf,\n",
        "    rf_params,\n",
        "    cv=3,\n",
        "    verbose=1,\n",
        "    algorithm=algorithm,\n",
        "    population_size=20,\n",
        "    runs=5,\n",
        "    n_jobs=-1,\n",
        "    scoring='accuracy')\n",
        "nia_search.fit(x_train_pad, y_train)\n",
        "print(nia_search.best_params_)\n",
        "print(\"accuracy:\"+ str(nia_search.best_score_))"
      ],
      "metadata": {
        "id": "GX2Rl8iN9iTq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WoZguVD6n03w"
      },
      "source": [
        "# FA"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "clf = KerasClassifier(build_fn=CNNGRU, verbose=0)\n",
        "\n",
        "nia_search = NatureInspiredSearchCV(\n",
        "    clf,\n",
        "    rf_params,\n",
        "    cv=3,\n",
        "    verbose=1,\n",
        "    algorithm='fa',\n",
        "    population_size=20,\n",
        "    n_jobs=-1   \n",
        "    ,scoring='accuracy')\n",
        "nia_search.fit(x_train_pad, y_train)\n",
        "print(nia_search.best_params_)\n",
        "print(\"accuracy:\"+ str(nia_search.best_score_))"
      ],
      "metadata": {
        "id": "ayD0Pv9Y-FyL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# GWO"
      ],
      "metadata": {
        "id": "xQq0oSTL0n6m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "clf = KerasClassifier(build_fn=CNNGRU, verbose=0)\n",
        "\n",
        "nia_search = NatureInspiredSearchCV(\n",
        "    clf,\n",
        "    rf_params,\n",
        "    cv=3,\n",
        "    verbose=1,\n",
        "    algorithm='gwo',\n",
        "    population_size=20,\n",
        "    runs=5,\n",
        "    n_jobs=-1,\n",
        "   scoring='accuracy')\n",
        "nia_search.fit(x_train_pad, y_train)\n",
        "print(nia_search.best_params_)\n",
        "print(\"accuracy:\"+ str(nia_search.best_score_))"
      ],
      "metadata": {
        "id": "Tj4ziMPT-3pN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# GWO-WOA"
      ],
      "metadata": {
        "id": "1k7wud8D0vPM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "86eXsaQVmPLx"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import numpy\n",
        "import matplotlib.pyplot as plt\n",
        "from math import sqrt\n",
        "from sklearn.metrics import mean_squared_error , mean_absolute_error, accuracy_score\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from keras.wrappers.scikit_learn import KerasClassifier\n",
        "\n",
        "import os\n",
        "import datetime\n",
        "\n",
        "import IPython\n",
        "import IPython.display\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import tensorflow as tf\n",
        "\n",
        "mpl.rcParams['figure.figsize'] = (8, 6)\n",
        "mpl.rcParams['axes.grid'] = False\n",
        "\n",
        "from tqdm import tqdm\n",
        "from scipy import stats\n",
        "from statsmodels.tsa.statespace.varmax import VARMAX\n",
        "\n",
        "import statsmodels.api as sm\n",
        "\n",
        "import IPython\n",
        "import IPython.display\n",
        "\n",
        "\n",
        "import warnings \n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vB8t1XB07ClN"
      },
      "source": [
        "#Making DNN\n",
        "import tensorflow as tf\n",
        "def CNNGRU2(numFilters = 20, kernelSize = 5, poolSize = 2, gruUnits = 20):\n",
        "    CNN_GRU_Model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Embedding(numWords, embeddingDim , \n",
        "                      embeddings_initializer = Constant( embeddingMatrix), \n",
        "                      input_length=maxLen,\n",
        "                      trainable = isTrainable)\n",
        "    ,\n",
        "    tf.keras.layers.Conv1D(filters= numFilters, kernel_size= kernelSize, strides= 1, padding='same', activation='relu')\n",
        "    ,\n",
        "    tf.keras.layers.Dropout(0.4)\n",
        "    ,\n",
        "    tf.keras.layers.MaxPooling1D(pool_size = poolSize)\n",
        "    ,\n",
        "    tf.keras.layers.Dropout(0.4)\n",
        "    ,\n",
        "    tf.keras.layers.GRU( gruUnits , dropout=0.6 )\n",
        "    ,\n",
        "    tf.keras.layers.Dropout(0.6)\n",
        "    ,\n",
        "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
        "\n",
        "])\n",
        "\n",
        "    #LSTM_MLP_Model.compile(optimizer = \"adam\", loss=loss, metrics=[tf.metrics.MeanAbsoluteError()])\n",
        "    CNN_GRU_Model.compile(loss='binary_crossentropy',\n",
        "                optimizer='adam',\n",
        "                metrics=['accuracy'])\n",
        "\n",
        "    early_stopping = tf.keras.callbacks.EarlyStopping(monitor='loss',\n",
        "                                                      patience=10)\n",
        "    \n",
        "    hist = CNN_GRU_Model.fit(x_train_pad, y_train, epochs=100, batch_size=32, validation_data=(x_test_pad, y_test), verbose = 0) \n",
        "    \n",
        "    return hist,CNN_GRU_Model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rf_params = {\n",
        "    'numFilters': [32, 64],\n",
        "    'kernelSize':[3, 4, 5, 6, 7],\n",
        "    'poolSize':[2, 3, 4],\n",
        "    'gruUnits':[10, 15, 20, 25]\n",
        "}"
      ],
      "metadata": {
        "id": "aCbxeAl-PY7L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# sphere function\n",
        "def shpere_fun(x):\n",
        "    z=np.sum(x**2)\n",
        "    return z"
      ],
      "metadata": {
        "id": "f7tgkrexPegW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_integer_value(x,L):\n",
        "  div_val=np.linspace(0, 1, L)\n",
        "  idx=np.argwhere(x>=div_val)\n",
        "  return idx[-1][0]"
      ],
      "metadata": {
        "id": "O96of_xhPhT9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def cost_fun(x):\n",
        "  \n",
        "  # get par base x value\n",
        "  select_par=dict()\n",
        "  counter=0\n",
        "  for key in rf_params:\n",
        "    val_key=rf_params.get(key)# get value each params\n",
        "    L=len(val_key)\n",
        "    idx_slecect=get_integer_value(x[counter],L) # get integer idx by x value\n",
        "    select_par[key]=val_key[idx_slecect] # add select param to select_par\n",
        "    counter+=1\n",
        "  print(select_par)\n",
        "  hist,model=CNNGRU2(**select_par)\n",
        "  accuracy=hist.history['accuracy'][-1]\n",
        "  \n",
        "  return accuracy"
      ],
      "metadata": {
        "id": "sUQGjfiWPlqL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_para(x):\n",
        "  \n",
        "  # get par base x value\n",
        "  select_par=dict()\n",
        "  counter=0\n",
        "  for key in rf_params:\n",
        "    val_key=rf_params.get(key)# get value each params\n",
        "    L=len(val_key)\n",
        "    idx_slecect=get_integer_value(x[counter],L) # get integer idx by x value\n",
        "    select_par[key]=val_key[idx_slecect] # add select param to select_par\n",
        "    counter+=1\n",
        "  return select_par"
      ],
      "metadata": {
        "id": "WytrbD-uP9Fk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "select_par=get_para([32, 3 ,4, 50])\n",
        "select_par\n",
        "#{'activation': 'relu', 'neurons1': 20, 'neurons2': 25}"
      ],
      "metadata": {
        "id": "fLpTug7AQDvB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## GWO-pop"
      ],
      "metadata": {
        "id": "K0FZmzpOQWT-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def GWO_POP(objf, lb, ub, dim, SearchAgents_no, it,Positions,\n",
        "            Alpha_pos,Alpha_score,Beta_pos,Beta_score,Delta_pos,Delta_score):\n",
        "  l=it\n",
        "  for i in range(0, SearchAgents_no):\n",
        "    # Return back the search agents that go beyond the boundaries of the search space\n",
        "    for j in range(dim):\n",
        "        Positions[i, j] = numpy.clip(Positions[i, j], lb[j], ub[j])\n",
        "\n",
        "    # Calculate objective function for each search agent\n",
        "    fitness = objf(Positions[i, :])\n",
        "    # Update Alpha, Beta, and Delta\n",
        "    if fitness < Alpha_score:\n",
        "        Delta_score = Beta_score  # Update delte\n",
        "        Delta_pos = Beta_pos.copy()\n",
        "        Beta_score = Alpha_score  # Update beta\n",
        "        Beta_pos = Alpha_pos.copy()\n",
        "        Alpha_score = fitness\n",
        "        # Update alpha\n",
        "        Alpha_pos = Positions[i, :].copy()\n",
        "\n",
        "    if fitness > Alpha_score and fitness < Beta_score:\n",
        "        Delta_score = Beta_score  # Update delte\n",
        "        Delta_pos = Beta_pos.copy()\n",
        "        Beta_score = fitness  # Update beta\n",
        "        Beta_pos = Positions[i, :].copy()\n",
        "\n",
        "    if fitness > Alpha_score and fitness > Beta_score and fitness < Delta_score:\n",
        "        Delta_score = fitness  # Update delta\n",
        "        Delta_pos = Positions[i, :].copy()\n",
        "\n",
        "    a = 2 - (l+1) * ((2) / Max_iter)\n",
        "    # a decreases linearly fron 2 to 0\n",
        "\n",
        "    # Update the Position of search agents including omegas\n",
        "    for i in range(0, SearchAgents_no):\n",
        "      for j in range(0, dim):\n",
        "\n",
        "          r1 = numpy.random.rand()  # r1 is a random number in [0,1]\n",
        "          r2 = numpy.random.rand()  # r2 is a random number in [0,1]\n",
        "\n",
        "          A1 = 2 * a * r1 - a\n",
        "          # Equation (3.3)\n",
        "          C1 = 2 * r2\n",
        "          # Equation (3.4)\n",
        "\n",
        "          D_alpha = abs(C1 * Alpha_pos[j] - Positions[i, j])\n",
        "          # Equation (3.5)-part 1\n",
        "          X1 = Alpha_pos[j] - A1 * D_alpha\n",
        "          # Equation (3.6)-part 1\n",
        "\n",
        "          r1 = numpy.random.rand()\n",
        "          r2 = numpy.random.rand()\n",
        "\n",
        "          A2 = 2 * a * r1 - a\n",
        "          # Equation (3.3)\n",
        "          C2 = 2 * r2\n",
        "          # Equation (3.4)\n",
        "\n",
        "          D_beta = abs(C2 * Beta_pos[j] - Positions[i, j])\n",
        "          # Equation (3.5)-part 2\n",
        "          X2 = Beta_pos[j] - A2 * D_beta\n",
        "          # Equation (3.6)-part 2\n",
        "\n",
        "          r1 = numpy.random.rand()\n",
        "          r2 = numpy.random.rand()\n",
        "\n",
        "          A3 = 2 * a * r1 - a\n",
        "          # Equation (3.3)\n",
        "          C3 = 2 * r2\n",
        "          # Equation (3.4)\n",
        "\n",
        "          D_delta = abs(C3 * Delta_pos[j] - Positions[i, j])\n",
        "          # Equation (3.5)-part 3\n",
        "          X3 = Delta_pos[j] - A3 * D_delta\n",
        "          # Equation (3.5)-part 3\n",
        "\n",
        "          Positions[i, j] = (X1 + X2 + X3) / 3  # Equation (3.7)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  return Positions,Alpha_pos,Alpha_score,Beta_pos,Beta_score,Delta_pos,Delta_score\n"
      ],
      "metadata": {
        "id": "kkrFSNDGQZRv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## WOA-POP"
      ],
      "metadata": {
        "id": "pSECvfnwQez_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def WOA_POP(objf, lb, ub, dim, SearchAgents_no, t,Positions,Leader_pos,Leader_score):\n",
        "\n",
        "\n",
        "  for i in range(0, SearchAgents_no):\n",
        "      for j in range(dim):\n",
        "          Positions[i, j] = np.clip(Positions[i, j], lb[j], ub[j])\n",
        "          \n",
        "    # Calculate objective function for each search agent\n",
        "      fitness = objf(Positions[i, :])\n",
        "\n",
        "        # Update Alpha, Beta, and Delta\n",
        "      if fitness < Leader_score:\n",
        "          Leader_score = fitness  # Update delte\n",
        "          Leader_pos = Positions[i, :].copy()\n",
        "\n",
        "\n",
        "  a = 2-(t+1)*((2)/Max_iter);           # linearly decreased from 2 to 0\n",
        "  a2=-1+(t+1)*((-1)/Max_iter);\n",
        "  \n",
        "  \n",
        "  \n",
        "  for i in range(SearchAgents_no):\n",
        "      r1 = np.random.rand()\n",
        "      r2 = np.random.rand()\n",
        "      A = 2 * a * r1 - a\n",
        "      C = 2 * r2\n",
        "      l = (a2-1)*np.random.rand()+1\n",
        "      b=1\n",
        "      p = np.random.rand()\n",
        "      for j in range(dim):\n",
        "        \n",
        "      \n",
        "          if ( p<0.5) :\n",
        "              \n",
        "\n",
        "            if(np.abs(A)>=1) :\n",
        "                rand_leader_index = int(np.floor(SearchAgents_no*np.random.rand()+1)-1)\n",
        "                x_rand = Positions[rand_leader_index,:]\n",
        "                D_X_rand = np.abs(C * x_rand[j] - Positions[i][j])\n",
        "                Positions[i,j] = (x_rand[j] - A *D_X_rand)\n",
        "            elif(np.abs(A) < 1):\n",
        "                D_Leader = np.abs(C * Leader_pos[j]- Positions[i][j] )\n",
        "                Positions[i,j] = Leader_pos[j] - A * D_Leader\n",
        "          elif(p>=0.5):\n",
        "            distance2Leader = np.abs(Leader_pos[j]- Positions[i][j])\n",
        "            Positions[i,j] = distance2Leader * np.exp(b * l) * np.cos(2 * np.pi * l) + Leader_pos[j]\n",
        "\n",
        "\n",
        "\n",
        "  \n",
        "\n",
        "  return Positions,Leader_pos,Leader_score"
      ],
      "metadata": {
        "id": "7nH0mDvSQhrd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Hybrid Serial GWO-WOA"
      ],
      "metadata": {
        "id": "MxrUGU4DQlwl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#setting parameter\n",
        "Max_iter=1\n",
        "lb=0\n",
        "ub=1\n",
        "dim=len(rf_params)\n",
        "SearchAgents_no=4\n",
        "objf=cost_fun\n",
        "# initialize alpha, beta, and delta_pos\n",
        "Alpha_pos = numpy.zeros(dim)\n",
        "Alpha_score = float(\"inf\")\n",
        "Beta_pos = numpy.zeros(dim)\n",
        "Beta_score = float(\"inf\")\n",
        "Delta_pos = numpy.zeros(dim)\n",
        "Delta_score = float(\"inf\")\n",
        "\n",
        "if not isinstance(lb, list):\n",
        "    lb = [lb] * dim\n",
        "if not isinstance(ub, list):\n",
        "    ub = [ub] * dim\n",
        "\n",
        "# Initialize the positions of search agents\n",
        "Positions = numpy.zeros((SearchAgents_no, dim))\n",
        "for i in range(dim):\n",
        "    Positions[:, i] = (numpy.random.uniform(0, 1, SearchAgents_no) * (ub[i] - lb[i]) + lb[i])\n",
        "\n",
        "#------main loop\n",
        "Convergence_curve = numpy.zeros(Max_iter)\n",
        "\n",
        "for it in range(Max_iter):\n",
        "  Positions,Alpha_pos,Alpha_score,Beta_pos,Beta_score,Delta_pos,Delta_score=GWO_POP(objf, lb, ub, dim, SearchAgents_no, it,Positions,\n",
        "                              Alpha_pos,Alpha_score,Beta_pos,Beta_score,Delta_pos,Delta_score)\n",
        "  \n",
        "  Positions,Alpha_pos,Alpha_score=WOA_POP(objf, lb, ub, dim, SearchAgents_no, it,Positions,Alpha_pos,Alpha_score)\n",
        "  Convergence_curve[it] = Alpha_score\n",
        "\n",
        "  print([\"At iteration \" + str(it) + \" the best fitness is \" + str(Alpha_score)])\n",
        "\n",
        "serial_GWO_WOA_parar=get_para(Alpha_pos)\n",
        "print('\\n_______________best parameter_____________________')\n",
        "print(get_para(Alpha_pos))\n",
        "print('accuracy: {0}'.format(Alpha_score))"
      ],
      "metadata": {
        "id": "uVX4Y4WkQq9t"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "collapsed_sections": [
        "04h5x_Cql87b",
        "2mHnbyLJmI6x",
        "FXEnjAczmYu4",
        "tWdYMMlzmp3_",
        "4ymS6wFsnRcM",
        "Prt55QgSnbjx",
        "rYCI6S2RovYo",
        "o2zyH5I_2Msi",
        "I7wPN6b7Bg-n",
        "8GEu0skbim5B",
        "sxl4G9UhpWo3",
        "WoZguVD6n03w",
        "xQq0oSTL0n6m",
        "1k7wud8D0vPM",
        "K0FZmzpOQWT-",
        "pSECvfnwQez_"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}