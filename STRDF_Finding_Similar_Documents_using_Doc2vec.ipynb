{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "HiSEEeOX5UxO",
        "QmK-tAmXydXh",
        "S2kue_J3ls55",
        "jyxXR41GIsLN",
        "w-S1wuAojt6G",
        "pDJllF_IIzKf",
        "qVjIzwqtwQ1Z",
        "98SqeBvTlXd_",
        "mtsMUKBeJ_72",
        "xT_dos0QpBpW",
        "p_dXsWu-kGPl",
        "iZ485pbtei8Q",
        "3xTX52eC94Wr",
        "zszrHKMx9FtI",
        "rQT8TtiuFS9j",
        "OyME1mZ5FUrl",
        "yDAPVn0mFWWU",
        "n9oj1SUTFYWm",
        "Mye0KNDdFZm4",
        "7RHcSo4-Fa_a",
        "kAWPmqdjFcjw"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "This module is responsible for: \n",
        "\n",
        "\n",
        "1.   Making Doc2vec model\n",
        "2.   Concatenating Doc2vec Models\n",
        "3.   Finding semantically topic-related documents correspondinng to  the topics\n",
        "\n",
        "\n",
        "**Note:**\n",
        "\n",
        "Set the dataset path in inputFileName variable\n",
        "\n",
        "Set the topics in csvTopic8 variable\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "e7_CyYoM-_-5"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kuu6z-ZZoZHq"
      },
      "source": [
        "## Configurations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fbkcv3irn-mi"
      },
      "source": [
        "%matplotlib inline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qf6N4pe_oSu6"
      },
      "source": [
        "import logging\n",
        "#logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HOZnc3w7CA9Y"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import gensim\n",
        "from sklearn.model_selection import train_test_split\n",
        "import multiprocessing\n",
        "from collections import OrderedDict\n",
        "\n",
        "import gensim.models.doc2vec\n",
        "assert gensim.models.doc2vec.FAST_VERSION > -1, \"This will be painfully slow otherwise\"\n",
        "\n",
        "from gensim.models.doc2vec import Doc2Vec"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gxGpI5Zq6IQl"
      },
      "source": [
        "pip install testfixtures"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3GiKOztIlqIP"
      },
      "source": [
        "import collections\n",
        "\n",
        "SentimentDocument = collections.namedtuple('SentimentDocument', 'words tags split sentiment')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q0FG4Qzc_bYD"
      },
      "source": [
        "inputFileName = '/MYDRIVE/My Drive/Colab Notebooks/Proposal/Data/Processed-Merged-Dataset-99-08-28.csv'\n",
        "#inputFileName = '/MYDRIVE/My Drive/Colab Notebooks/Proposal/Data/Balenced-Dataset-00-06-26.csv'\n",
        "\n",
        "outputLabelledTTDS= '/MYDRIVE/My Drive/Colab Notebooks/Proposal/Data/Balenced-Dataset-00-06-26.csv'\n",
        "outputDocsFileName = '/MYDRIVE/My Drive/Colab Notebooks/Proposal/Data/DocsReadyForD2V-00-06-26.csv'\n",
        "\n",
        "d2vModelFNmPos = '/MYDRIVE/My Drive/Colab Notebooks/Proposal/Data/d2vModelPositive-00-06-26.model'\n",
        "d2vModelFNtPos = '/MYDRIVE/My Drive/Colab Notebooks/Proposal/Data/d2vModelPositive-00-06-26.txt' \n",
        "\n",
        "d2vModelFNmNeg = '/MYDRIVE/My Drive/Colab Notebooks/Proposal/Data/d2vModelNegative-00-06-26.model'\n",
        "d2vModelFNtNeg = '/MYDRIVE/My Drive/Colab Notebooks/Proposal/Data/d2vModelNegative-00-06-26.txt' \n",
        "\n",
        "csvTopic8 = '/MYDRIVE/My Drive/Colab Notebooks/Proposal/Data/Topics_8.csv'\n",
        "csvTopic20 = '/MYDRIVE/My Drive/Colab Notebooks/Proposal/Data/Topics_20.csv'\n",
        "csvTopic40 = '/MYDRIVE/My Drive/Colab Notebooks/Proposal/Data/Topics_40.csv'\n",
        "csvTopic80 = '/MYDRIVE/My Drive/Colab Notebooks/Proposal/Data/Topics_80.csv'\n",
        "csvTopic50 = '/MYDRIVE/My Drive/Colab Notebooks/Proposal/Data/Topics_50.csv'\n",
        "csvTopic60 = '/MYDRIVE/My Drive/Colab Notebooks/Proposal/Data/Topics_60.csv'\n",
        "csvTopic65 = '/MYDRIVE/My Drive/Colab Notebooks/Proposal/Data/Topics_65.csv'\n",
        "csvTopic70 = '/MYDRIVE/My Drive/Colab Notebooks/Proposal/Data/Topics_70.csv'\n",
        "\n",
        "\n",
        "d2vModelFNm0 = '/MYDRIVE/My Drive/Colab Notebooks/Proposal/Data/d2vModel0-00-06-23.model'\n",
        "d2vModelFNt0 = '/MYDRIVE/My Drive/Colab Notebooks/Proposal/Data/d2vModel0-00-06-23.txt' \n",
        "\n",
        "d2vModelFNm1 = '/MYDRIVE/My Drive/Colab Notebooks/Proposal/Data/d2vModel1-00-06-23.model'\n",
        "d2vModelFNt1 = '/MYDRIVE/My Drive/Colab Notebooks/Proposal/Data/d2vModel1-00-06-23.txt' \n",
        "\n",
        "d2vModelFNm2 = '/MYDRIVE/My Drive/Colab Notebooks/Proposal/Data/d2vModel2-00-06-23.model'\n",
        "d2vModelFNt2 = '/MYDRIVE/My Drive/Colab Notebooks/Proposal/Data/d2vModel2-00-06-23.txt' \n",
        "\n",
        "d2vModelFNm3 = '/MYDRIVE/My Drive/Colab Notebooks/Proposal/Data/d2vModel3-00-06-23.model'\n",
        "d2vModelFNt3 = '/MYDRIVE/My Drive/Colab Notebooks/Proposal/Data/d2vModel3-00-06-23.txt' \n",
        "\n",
        "csvTopic8MostSimilar = '/MYDRIVE/My Drive/Colab Notebooks/Proposal/Data/SimilarDocsByProposedMethod/Topics_8_MostSimilar_PM.csv'\n",
        "csvTopic20MostSimilar = '/MYDRIVE/My Drive/Colab Notebooks/Proposal/Data/SimilarDocsByProposedMethod/Topics_20_MostSimilar_PM.csv'\n",
        "csvTopic40MostSimilar = '/MYDRIVE/My Drive/Colab Notebooks/Proposal/Data/SimilarDocsByProposedMethod/Topics_40_MostSimilar_PM.csv'\n",
        "csvTopic50MostSimilar = '/MYDRIVE/My Drive/Colab Notebooks/Proposal/Data/SimilarDocsByProposedMethod/Topics_50_MostSimilar_PM.csv'\n",
        "csvTopic60MostSimilar = '/MYDRIVE/My Drive/Colab Notebooks/Proposal/Data/SimilarDocsByProposedMethod/Topics_60_MostSimilar_PM.csv'\n",
        "csvTopic65MostSimilar = '/MYDRIVE/My Drive/Colab Notebooks/Proposal/Data/SimilarDocsByProposedMethod/Topics_65_MostSimilar_PM.csv'\n",
        "csvTopic70MostSimilar = '/MYDRIVE/My Drive/Colab Notebooks/Proposal/Data/SimilarDocsByProposedMethod/Topics_70_MostSimilar_PM.csv'\n",
        "csvTopic80MostSimilar = '/MYDRIVE/My Drive/Colab Notebooks/Proposal/Data/SimilarDocsByProposedMethod/Topics_80_MostSimilar_PM.csv'\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SVI5vELQ6KA8"
      },
      "source": [
        "def create_sentiment_document(index, tex, sent, sp):\n",
        "    #_, split, sentiment_str, _ = name.split('/')\n",
        "    #sentiment = {'pos': 1.0, 'neg': 0.0, 'unsup': None}[sentiment_str]\n",
        "\n",
        "    #if sentiment is None:\n",
        "        #split = 'extra'\n",
        "    #print(tex)\n",
        "    tokens = gensim.utils.to_unicode(tex).split()\n",
        "    #tokens = gensim.utils.sp\n",
        "    return SentimentDocument(tokens, [index], sp, sent)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HiSEEeOX5UxO"
      },
      "source": [
        "# Colab"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8-ztS1ZQ_5Xz"
      },
      "source": [
        "#No need in local\n",
        "from google.colab import drive\n",
        "drive.mount('/MYDRIVE', force_remount=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QmK-tAmXydXh"
      },
      "source": [
        "# Prepair Dataset (Dividing into Test and Train)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b9N4Y2ry_9yW"
      },
      "source": [
        "dfReviews = pd.DataFrame()\n",
        "dfReviews = pd.read_csv(inputFileName, encoding = 'utf-8', header =   'infer' )\n",
        "dfReviews.head(3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P5k6XdrymFqu"
      },
      "source": [
        "inputFileName"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MKonEE_yHIW2"
      },
      "source": [
        "dfReviews.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D0b20GkvxEm1"
      },
      "source": [
        "dfReviews = dfReviews.dropna()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pL4BWFkNxJdr"
      },
      "source": [
        "dfReviews.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qQFrAAjUHpAu"
      },
      "source": [
        "len (dfReviews.loc[dfReviews['Sentiment'] == 0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0RcaSW8uISBH"
      },
      "source": [
        "len (dfReviews.loc[dfReviews['Sentiment'] == 1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1y32RB6tHsGb"
      },
      "source": [
        "len (dfReviews.loc[dfReviews['Type'] == 'train'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pn5Z_uajHxXD"
      },
      "source": [
        "len (dfReviews.loc[dfReviews['Type'] == 'test'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "leTzHA-2aH88"
      },
      "source": [
        "#dfReviews[4] = 'train'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E8bMsPoClaLT"
      },
      "source": [
        "dfReviews.columns"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vFWPmH-nleBf"
      },
      "source": [
        "dfReviews.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZWjQ6aA7bDwI"
      },
      "source": [
        "#testNumber=113320\n",
        "#trainNumber=453280"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "29IU7Y7x98VK"
      },
      "source": [
        "#Making a copy of All REVIEWS\n",
        "dfReviews2 = dfReviews"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_0ro4cAJapmc"
      },
      "source": [
        "#Selecting 500,000 records from alol reviews (250,000 positive and 250,000 negative) to make a balanced dataset\n",
        "dfTemp1 = dfReviews[dfReviews[3] == 1].sample( n = 283300)\n",
        "dfTemp2 = dfReviews[dfReviews[3] == 0].sample( n = 283300)\n",
        "dfTemp = dfTemp1.append(dfTemp2)\n",
        "print( \"length of dfTemp \" + str( len (dfTemp) ))\n",
        "print(len(dfTemp[dfTemp[3]==0]))\n",
        "print(len(dfTemp[dfTemp[3]==1]))\n",
        "dfReviews = dfTemp\n",
        "dfTest = dfTemp[dfTemp[3] == 0].sample( n = 56660)\n",
        "dfTest = dfTest.append(dfTemp[dfTemp[3] == 1].sample( n = 56660))\n",
        "print( \"length of dfTest \" + str( len (dfTest) ) )\n",
        "print(len(dfTest[dfTest[3]==0]))\n",
        "print(len(dfTest[dfTest[3]==1]))\n",
        "\n",
        "print( \"length of dfReviews \" + str( len (dfReviews) ) )\n",
        "print(len(dfReviews[dfReviews[3]==0]))\n",
        "print(len(dfReviews[dfReviews[3]==1]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8qwbWSXQb030"
      },
      "source": [
        "dfTest.head(3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J1EBK1aVwvKa"
      },
      "source": [
        "len (dfReviews.loc[dfReviews[4] == 'train'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O2_sBD-wINy6"
      },
      "source": [
        "len (dfReviews.loc[dfReviews[4] == 'test'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xCylZNA_sOF1"
      },
      "source": [
        "dfReviews.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S2kue_J3ls55"
      },
      "source": [
        "# Preparing test data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zy3JCICyLt-U"
      },
      "source": [
        "dfReviews.loc[dfReviews[1].isin(testRows),4] = 'test'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aZg0r0wXmQsf"
      },
      "source": [
        "dfReviews.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c7kCeO1SnSFD"
      },
      "source": [
        "len (dfReviews.loc[dfReviews[4] == 'test'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uhD8QBUdnS8L"
      },
      "source": [
        "len (dfReviews.loc[dfReviews[4] == 'train'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0htx1J2pTPce"
      },
      "source": [
        "print( \"length of dfReviews \" + str( len (dfReviews) ) )\n",
        "print(len(dfReviews[dfReviews[3]==0]))\n",
        "print(len(dfReviews[dfReviews[3]==1]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iidH4sUiMSG5"
      },
      "source": [
        "dfReviews.iloc[238937,4]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HUXrz6inAcfU"
      },
      "source": [
        "print( \"length of dfReviews \" + str( len (dfReviews) ) )\n",
        "print(len(dfReviews[dfReviews[3]==0]))\n",
        "print(len(dfReviews[dfReviews[3]==1]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NPU7797PAum3"
      },
      "source": [
        "print( \"length of dfReviews \" + str( len (dfReviews) ) )\n",
        "print(len(dfReviews[dfReviews[4]=='test']))\n",
        "print(len(dfReviews[dfReviews[4]=='train']))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FJB-DzIPqSS4"
      },
      "source": [
        "dfReviews.sample(n=5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M-cxmhismuwV"
      },
      "source": [
        "dfReviews.reset_index(drop=True, inplace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YLtbXrvysYm4"
      },
      "source": [
        "dfReviews.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xsPcBccIqWAf"
      },
      "source": [
        "dfReviews.head(3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LD8mDCZOuwHz"
      },
      "source": [
        "dfReviews.drop(columns=[0,1], inplace= True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NJnN29v5u6Hz"
      },
      "source": [
        "dfReviews.columns = ['Text', 'Sentiment', 'Type']\n",
        "dfReviews.head(3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CeiIsboxTfEY"
      },
      "source": [
        "print( \"length of dfReviews \" + str( len (dfReviews) ) )\n",
        "print(len(dfReviews[dfReviews['Sentiment']==0]))\n",
        "print(len(dfReviews[dfReviews['Sentiment']==1]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jyxXR41GIsLN"
      },
      "source": [
        "# Save the divided dataset into csv file\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2xzLaZRyvvIE"
      },
      "source": [
        "dfReviews.to_csv( outputLabelledTTDS, header=True, index=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "voiAjBBhk7rb"
      },
      "source": [
        "outputLabelledTTDS"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oPoYOJQ0Npe0"
      },
      "source": [
        "dfReviews.sample(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w-S1wuAojt6G"
      },
      "source": [
        "# Load Saved LabelledTTDS"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8R-JQlTrjsGQ"
      },
      "source": [
        "dfReviews = pd.read_csv(outputLabelledTTDS, header='infer')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i3B5I6MGksMU"
      },
      "source": [
        "len(dfReviews)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oyOjFMYplDoz"
      },
      "source": [
        "dfReviews.columns"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qYpEtdHfIjlC"
      },
      "source": [
        "dfReviews.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pDJllF_IIzKf"
      },
      "source": [
        "# Make the big list of sentiment and doc to be learned by Doc2Vec"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CYRCC4peOc6A"
      },
      "source": [
        "allDocs = list()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ADeKzhZiQlcx"
      },
      "source": [
        "dfReviews.sample(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f7yynGrJLO1k"
      },
      "source": [
        "i=0\n",
        "for index , row in dfReviews.iterrows():\n",
        "  allDocs.append(  create_sentiment_document( (index) , (row[\"Text\"]) , (row[\"Sentiment\"]) , row[\"Type\"] ) )\n",
        "  #allDocs.append(  create_sentiment_document( (index) , (row[2]) , (row[3]) , row[4] ) )\n",
        "  i = i + 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WTcrPx_mOk60"
      },
      "source": [
        "len(allDocs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JZsfRHeKJ74I"
      },
      "source": [
        "allDocs[500]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8vU-P_PKEzzl"
      },
      "source": [
        "dfReviews.loc[500]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r3X2DGgJSZeh"
      },
      "source": [
        "len(allDocs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ojX6HTVBuZyG"
      },
      "source": [
        "allDocs[400000]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q_KzuFVPtxQK"
      },
      "source": [
        "## Writing all docs to file"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MOeiEbyVvYin"
      },
      "source": [
        "my_df = pd.DataFrame(allDocs)\n",
        "my_df.to_csv(outputDocsFileName,  header=True, index = True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A9ThamM9Lk7J"
      },
      "source": [
        "outputDocsFileName"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qVjIzwqtwQ1Z"
      },
      "source": [
        "# Make, Train, and Save Two doc2vec models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "woCFCV8xJq2v"
      },
      "source": [
        "'''\n",
        "trainPositiveDocs = [doc for doc in allDocs if doc.split == 'train' and doc.sentiment == 1 ]\n",
        "testPositiveDocs = [doc for doc in allDocs if doc.split == 'test' and doc.sentiment == 1]\n",
        "\n",
        "trainNegativeDocs = [doc for doc in allDocs if doc.split == 'train' and doc.sentiment == 0]\n",
        "testNegativeDocs = [doc for doc in allDocs if doc.split == 'test' and doc.sentiment == 0]\n",
        "'''\n",
        "trainDocs = [doc for doc in allDocs if doc.split == 'train' ]\n",
        "testDocs = [doc for doc in allDocs if doc.split == 'test' ]\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mf_p8_GoJgz4"
      },
      "source": [
        "len(trainDocs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o6m1EzjKJkXO"
      },
      "source": [
        "len(testDocs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H1kzLeVbMDhq"
      },
      "source": [
        "'''\n",
        "allPositiveDocs = [doc for doc in allDocs if  doc.sentiment == 1 ]\n",
        "allNegativeDocs = [doc for doc in allDocs if  doc.sentiment == 0 ]\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RkkRBas8HPdS"
      },
      "source": [
        "type(allDocs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "98SqeBvTlXd_"
      },
      "source": [
        "# Error rate"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c-5e1CuhJ-EV"
      },
      "source": [
        "import numpy as np\n",
        "import statsmodels.api as sm\n",
        "from random import sample\n",
        "\n",
        "def logistic_predictor_from_data(train_targets, train_regressors):\n",
        "    \"\"\"Fit a statsmodel logistic predictor on supplied data\"\"\"\n",
        "    logit = sm.Logit(train_targets, train_regressors)\n",
        "    predictor = logit.fit(disp=0)\n",
        "    # print(predictor.summary())\n",
        "    return predictor\n",
        "\n",
        "def error_rate_for_model(test_model, train_set, test_set):\n",
        "    \"\"\"Report error rate on test_doc sentiments, using supplied model and train_docs\"\"\"\n",
        "\n",
        "    train_targets = [doc.sentiment for doc in train_set]\n",
        "    train_regressors = [test_model.docvecs[doc.tags[0]] for doc in train_set]\n",
        "    train_regressors = sm.add_constant(train_regressors)\n",
        "    predictor = logistic_predictor_from_data(train_targets, train_regressors)\n",
        "\n",
        "    test_regressors = [test_model.docvecs[doc.tags[0]] for doc in test_set]\n",
        "    test_regressors = sm.add_constant(test_regressors)\n",
        "\n",
        "    # Predict & evaluate\n",
        "    test_predictions = predictor.predict(test_regressors)\n",
        "    corrects = sum(np.rint(test_predictions) == [doc.sentiment for doc in test_set])\n",
        "    errors = len(test_predictions) - corrects\n",
        "    error_rate = float(errors) / len(test_predictions)\n",
        "    return (error_rate, errors, len(test_predictions), predictor)\n",
        "\n",
        "\n",
        "from collections import defaultdict\n",
        "error_rates = defaultdict(lambda: 1.0)  # To selectively print only best errors achieved"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mtsMUKBeJ_72"
      },
      "source": [
        "# Fine tunning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FATNKlwyL_2d"
      },
      "source": [
        "import multiprocessing\n",
        "from collections import OrderedDict\n",
        "\n",
        "import gensim.models.doc2vec\n",
        "assert gensim.models.doc2vec.FAST_VERSION > -1, \"This will be painfully slow otherwise\"\n",
        "\n",
        "from gensim.models.doc2vec import Doc2Vec\n",
        "\n",
        "common_kwargs = dict(\n",
        "    vector_size=100, epochs=20, min_count=2,\n",
        "    sample=0, workers=multiprocessing.cpu_count(), negative=5, hs=0,\n",
        ")\n",
        "\n",
        "simple_models = [\n",
        "    \n",
        "    Doc2Vec(dm=0,  dm_mean = 0, **common_kwargs),\n",
        "\n",
        "    Doc2Vec(dm=0,  dm_mean = 1, **common_kwargs),\n",
        "\n",
        "    Doc2Vec(dm=1,  dm_mean = 0, **common_kwargs),\n",
        "    \n",
        "    Doc2Vec(dm=1,  dm_mean = 1, **common_kwargs),\n",
        "]\n",
        "\n",
        "for model in simple_models:\n",
        "    model.build_vocab(allDocs)\n",
        "    print(\"%s vocabulary scanned & state initialized\" % model)\n",
        "\n",
        "models_by_name = OrderedDict((str(model), model) for model in simple_models)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0QBcT14uNGX8"
      },
      "source": [
        "from random import shuffle\n",
        "#shuffled_alldocs = allDocs[:]\n",
        "#print(len(shuffled_alldocs))\n",
        "#shuffle(shuffled_alldocs)\n",
        "\n",
        "for model in simple_models:\n",
        "    print(\"Training %s\" % model)\n",
        "    model.train(allDocs, total_examples=len(allDocs), epochs=model.epochs)\n",
        "\n",
        "    print(\"\\nEvaluating %s\" % model)\n",
        "    err_rate, err_count, test_count, predictor = error_rate_for_model(model, trainDocs, testDocs)\n",
        "    error_rates[str(model)] = err_rate\n",
        "    print(\"\\n%f %s\\n\" % (err_rate, model))\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-MmKrjO5oB5H"
      },
      "source": [
        "selectedModel = simple_models[2]#dm=1 dm_mean=0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xT_dos0QpBpW"
      },
      "source": [
        "# Vector Seize"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P-H5MX_IpE5Q"
      },
      "source": [
        "d2v15 = Doc2Vec(dm=1, dm_mean = 0,  vector_size=150, epochs=20, min_count=5,\n",
        "    sample=0, workers=multiprocessing.cpu_count(), negative=5, hs=0, seed=1)\n",
        "d2v15.build_vocab(allDocs)\n",
        "print(\"Training %s\" % d2v15)\n",
        "d2v15.train(allDocs, total_examples=len(allDocs), epochs=d2v15.epochs)\n",
        "print(\"\\nEvaluating %s\" % d2v15)\n",
        "err_rate, err_count, test_count, predictor = error_rate_for_model(d2v15, trainDocs, testDocs)\n",
        "print(\"\\n%f %s\\n\" % (err_rate, d2v15))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "55DS_QYSomcE"
      },
      "source": [
        "d2v200 = Doc2Vec(dm=1, dm_mean = 0,  vector_size=200, epochs=20, min_count=5,\n",
        "    sample=0, workers=multiprocessing.cpu_count(), negative=5, hs=0, seed=1)\n",
        "d2v200.build_vocab(allDocs)\n",
        "print(\"Training %s\" % d2v200)\n",
        "d2v200.train(allDocs, total_examples=len(allDocs), epochs=d2v200.epochs)\n",
        "print(\"\\nEvaluating %s\" % d2v200)\n",
        "err_rate, err_count, test_count, predictor = error_rate_for_model(d2v200, trainDocs, testDocs)\n",
        "print(\"\\n%f %s\\n\" % (err_rate, d2v200))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R96vFeos4SG5"
      },
      "source": [
        "d2v300 = Doc2Vec(dm=1, dm_mean = 0,  vector_size=300, epochs=20, min_count=5,\n",
        "    sample=0, workers=multiprocessing.cpu_count(), negative=5, hs=0, seed=1)\n",
        "d2v300.build_vocab(allDocs)\n",
        "print(\"Training %s\" % d2v300)\n",
        "d2v300.train(allDocs, total_examples=len(allDocs), epochs=d2v300.epochs)\n",
        "print(\"\\nEvaluating %s\" % d2v300)\n",
        "err_rate, err_count, test_count, predictor = error_rate_for_model(d2v300, trainDocs, testDocs)\n",
        "print(\"\\n%f %s\\n\" % (err_rate, d2v300))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JigcZx4f_Kk3"
      },
      "source": [
        "d2v350 = Doc2Vec(dm=1, dm_mean = 0,  vector_size=350, epochs=20, min_count=5,\n",
        "    sample=0, workers=multiprocessing.cpu_count(), negative=5, hs=0, seed=1)\n",
        "d2v350.build_vocab(allDocs)\n",
        "print(\"Training %s\" % d2v350)\n",
        "d2v350.train(allDocs, total_examples=len(allDocs), epochs=d2v350.epochs)\n",
        "print(\"\\nEvaluating %s\" % d2v350)\n",
        "err_rate, err_count, test_count, predictor = error_rate_for_model(d2v350, trainDocs, testDocs)\n",
        "print(\"\\n%f %s\\n\" % (err_rate, d2v350))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B6Z9lStwHJQv"
      },
      "source": [
        "d2v450 = Doc2Vec(dm=1, dm_mean = 0,  vector_size=450, epochs=20, min_count=5,\n",
        "    sample=0, workers=multiprocessing.cpu_count(), negative=5, hs=0, seed=1)\n",
        "d2v450.build_vocab(allDocs)\n",
        "print(\"Training %s\" % d2v450)\n",
        "d2v450.train(allDocs, total_examples=len(allDocs), epochs=d2v450.epochs)\n",
        "print(\"\\nEvaluating %s\" % d2v450)\n",
        "err_rate, err_count, test_count, predictor = error_rate_for_model(d2v450, trainDocs, testDocs)\n",
        "print(\"\\n%f %s\\n\" % (err_rate, d2v450))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eoANVGQOTDi3"
      },
      "source": [
        "d2v600 = Doc2Vec(dm=1, dm_mean = 0,  vector_size=600, epochs=20, min_count=5,\n",
        "    sample=0, workers=multiprocessing.cpu_count(), negative=5, hs=0, seed=1)\n",
        "d2v600.build_vocab(allDocs)\n",
        "print(\"Training %s\" % d2v600)\n",
        "d2v600.train(allDocs, total_examples=len(allDocs), epochs=d2v600.epochs)\n",
        "print(\"\\nEvaluating %s\" % d2v600)\n",
        "err_rate, err_count, test_count, predictor = error_rate_for_model(d2v600, trainDocs, testDocs)\n",
        "print(\"\\n%f %s\\n\" % (err_rate, d2v600))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p_dXsWu-kGPl"
      },
      "source": [
        "# LR"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Lq7Pj-2_HAn"
      },
      "source": [
        "d2v500_lr_3 = Doc2Vec(dm=1, dm_mean = 0,  vector_size=500, epochs=20, min_count=5,\n",
        "    sample=0, workers=multiprocessing.cpu_count(), negative=5, hs=0, seed=1, alpha = 0.001)\n",
        "d2v500_lr_3.build_vocab(allDocs)\n",
        "print(\"Training %s\" % d2v500_lr_3)\n",
        "d2v500_lr_3.train(allDocs, total_examples=len(allDocs), epochs=d2v500_lr_3.epochs)\n",
        "print(\"\\nEvaluating %s\" % d2v500_lr_3)\n",
        "err_rate, err_count, test_count, predictor = error_rate_for_model(d2v500_lr_3, trainDocs, testDocs)\n",
        "print(\"\\n%f %s\\n\" % (err_rate, d2v500_lr_3))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l5jbchuBkNYc"
      },
      "source": [
        "d2v500_lr_1 = Doc2Vec(dm=1, dm_mean = 0,  vector_size=500, epochs=20, min_count=5,\n",
        "    sample=0, workers=multiprocessing.cpu_count(), negative=5, hs=0, seed=1, alpha = 0.05)\n",
        "d2v500_lr_1.build_vocab(allDocs)\n",
        "print(\"Training %s\" % d2v500_lr_1)\n",
        "d2v500_lr_1.train(allDocs, total_examples=len(allDocs), epochs=d2v500_lr_1.epochs)\n",
        "print(\"\\nEvaluating %s\" % d2v500_lr_1)\n",
        "err_rate, err_count, test_count, predictor = error_rate_for_model(d2v500_lr_1, trainDocs, testDocs)\n",
        "print(\"\\n%f %s\\n\" % (err_rate, d2v500_lr_1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "11npXbFXrVpi"
      },
      "source": [
        "d2v500_lr_2 = Doc2Vec(dm=1, dm_mean = 0,  vector_size=500, epochs=20, min_count=5,\n",
        "    sample=0, workers=multiprocessing.cpu_count(), negative=5, hs=0, seed=1, alpha = 0.075)\n",
        "d2v500_lr_2.build_vocab(allDocs)\n",
        "print(\"Training %s\" % d2v500_lr_2)\n",
        "d2v500_lr_2.train(allDocs, total_examples=len(allDocs), epochs=d2v500_lr_2.epochs)\n",
        "print(\"\\nEvaluating %s\" % d2v500_lr_2)\n",
        "err_rate, err_count, test_count, predictor = error_rate_for_model(d2v500_lr_2, trainDocs, testDocs)\n",
        "print(\"\\n%f %s\\n\" % (err_rate, d2v500_lr_2))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SNDDc0wKop1J"
      },
      "source": [
        "## dm = 0, dm_mean=0"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "micjqrDWopQF"
      },
      "source": [
        "d2v400_0 = Doc2Vec(dm=0, dm_mean = 0,  vector_size=400, epochs=20, min_count=5,\n",
        "    sample=0, workers=multiprocessing.cpu_count(), negative=5, hs=0, seed=1)\n",
        "d2v400_0.build_vocab(allDocs)\n",
        "print(\"Training %s\" % d2v400_0)\n",
        "d2v400_0.train(allDocs, total_examples=len(allDocs), epochs=d2v400_0.epochs)\n",
        "print(\"\\nEvaluating %s\" % d2v400_0)\n",
        "err_rate, err_count, test_count, predictor = error_rate_for_model(d2v400_0, trainDocs, testDocs)\n",
        "print(\"\\n%f %s\\n\" % (err_rate, d2v400_0))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kO5NHsyl4cpa"
      },
      "source": [
        "d2v350_0 = Doc2Vec(dm=0, dm_mean = 0,  vector_size=350, epochs=20, min_count=5,\n",
        "    sample=0, workers=multiprocessing.cpu_count(), negative=5, hs=0, seed=1)\n",
        "d2v350_0.build_vocab(allDocs)\n",
        "print(\"Training %s\" % d2v350_0)\n",
        "d2v350_0.train(allDocs, total_examples=len(allDocs), epochs=d2v350_0.epochs)\n",
        "print(\"\\nEvaluating %s\" % d2v350_0)\n",
        "err_rate, err_count, test_count, predictor = error_rate_for_model(d2v350_0, trainDocs, testDocs)\n",
        "print(\"\\n%f %s\\n\" % (err_rate, d2v350_0))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "clGHwUmktgsN"
      },
      "source": [
        "d2v300_0 = Doc2Vec(dm=0, dm_mean = 0,  vector_size=300, epochs=20, min_count=5,\n",
        "    sample=0, workers=multiprocessing.cpu_count(), negative=5, hs=0, seed=1)\n",
        "d2v300_0.build_vocab(allDocs)\n",
        "print(\"Training %s\" % d2v300_0)\n",
        "d2v300_0.train(allDocs, total_examples=len(allDocs), epochs=d2v300_0.epochs)\n",
        "print(\"\\nEvaluating %s\" % d2v300_0)\n",
        "err_rate, err_count, test_count, predictor = error_rate_for_model(d2v300_0, trainDocs, testDocs)\n",
        "print(\"\\n%f %s\\n\" % (err_rate, d2v300_0))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fzW8jc4FxS3I"
      },
      "source": [
        "d2v200_0 = Doc2Vec(dm=0, dm_mean = 0,  vector_size=200, epochs=20, min_count=5,\n",
        "    sample=0, workers=multiprocessing.cpu_count(), negative=5, hs=0, seed=1)\n",
        "d2v200_0.build_vocab(allDocs)\n",
        "print(\"Training %s\" % d2v200_0)\n",
        "d2v200_0.train(allDocs, total_examples=len(allDocs), epochs=d2v200_0.epochs)\n",
        "print(\"\\nEvaluating %s\" % d2v200_0)\n",
        "err_rate, err_count, test_count, predictor = error_rate_for_model(d2v200_0, trainDocs, testDocs)\n",
        "print(\"\\n%f %s\\n\" % (err_rate, d2v200_0))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q7JxKCq-0v0K"
      },
      "source": [
        "d2v150_0 = Doc2Vec(dm=0, dm_mean = 0,  vector_size=150, epochs=20, min_count=5,\n",
        "    sample=0, workers=multiprocessing.cpu_count(), negative=5, hs=0, seed=1)\n",
        "d2v150_0.build_vocab(allDocs)\n",
        "print(\"Training %s\" % d2v150_0)\n",
        "d2v150_0.train(allDocs, total_examples=len(allDocs), epochs=d2v150_0.epochs)\n",
        "print(\"\\nEvaluating %s\" % d2v150_0)\n",
        "err_rate, err_count, test_count, predictor = error_rate_for_model(d2v150_0, trainDocs, testDocs)\n",
        "print(\"\\n%f %s\\n\" % (err_rate, d2v150_0))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iZ485pbtei8Q"
      },
      "source": [
        "# Concat"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qm4VoTccoNeQ"
      },
      "source": [
        "d2v500_0 = Doc2Vec(dm=0, dm_mean = 0,  vector_size=400, epochs=20, min_count=5,\n",
        "    sample=0, workers=multiprocessing.cpu_count(), negative=5, hs=0, seed=1)\n",
        "d2v500_c.build_vocab(allDocs)\n",
        "print(\"Training %s\" % d2v500_c)\n",
        "d2v500_c.train(allDocs, total_examples=len(allDocs), epochs=d2v500_c.epochs)\n",
        "print(\"\\nEvaluating %s\" % d2v500_c)\n",
        "err_rate, err_count, test_count, predictor = error_rate_for_model(d2v500_c, trainDocs, testDocs)\n",
        "print(\"\\n%f %s\\n\" % (err_rate, d2v500_c))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9rIlpNMoenP2"
      },
      "source": [
        "from gensim.test.test_doc2vec import ConcatenatedDoc2Vec\n",
        "concatModel = ConcatenatedDoc2Vec([simple_models[0], simple_models[2]])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kTuzY9L3O84K"
      },
      "source": [
        "import multiprocessing\n",
        "from collections import OrderedDict\n",
        "\n",
        "import gensim.models.doc2vec\n",
        "assert gensim.models.doc2vec.FAST_VERSION > -1, \"This will be painfully slow otherwise\"\n",
        "\n",
        "from gensim.models.doc2vec import Doc2Vec\n",
        "\n",
        "d2vPositiveModel = Doc2Vec(dm=0, dm_mean = 0,  vector_size=100, epochs=20, min_count=5,\n",
        "    sample=0, workers=multiprocessing.cpu_count(), negative=5, hs=0)\n",
        "\n",
        "d2vNegativeModel = Doc2Vec(dm=0, dm_mean = 1,  vector_size=100, epochs=20, min_count=5,\n",
        "    sample=0, workers=multiprocessing.cpu_count(), negative=5, hs=0)\n",
        "\n",
        "d2vPositiveModel.build_vocab(allPositiveDocs)\n",
        "print(\"%s vocabulary scanned & state initialized\" % d2vPositiveModel)\n",
        "\n",
        "d2vNegativeModel.build_vocab(allNegativeDocs)\n",
        "print(\"%s vocabulary scanned & state initialized\" % d2vNegativeModel)\n",
        "\n",
        "#models_by_name = OrderedDict((str(model), model) for model in simple_models)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lxsl7gZJC8kf"
      },
      "source": [
        "from random import shuffle\n",
        "shuffled_positiveDocs = allPositiveDocs[:]\n",
        "shuffled_negativeDocs = allNegativeDocs[:]\n",
        "\n",
        "shuffle(shuffled_positiveDocs)\n",
        "shuffle(shuffled_negativeDocs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dVgd-v-RSgvI"
      },
      "source": [
        "print(len(shuffled_positiveDocs))\n",
        "print(len(shuffled_negativeDocs))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uzRJP-SdC-u0"
      },
      "source": [
        "    print(\"Training %s\" % d2vPositiveModel)\n",
        "    d2vPositiveModel.train(shuffled_positiveDocs, total_examples=len(shuffled_positiveDocs), epochs=10)\n",
        "\n",
        "    print(\"Training %s\" % d2vNegativeModel)\n",
        "    d2vNegativeModel.train(shuffled_negativeDocs, total_examples=len(shuffled_negativeDocs), epochs=10)\n",
        "\n",
        "    d2vNegativeModel.save(d2vModelFNmNeg)\n",
        "    d2vNegativeModel.save(d2vModelFNtNeg)\n",
        "\n",
        "    d2vPositiveModel.save(d2vModelFNmPos)\n",
        "    d2vPositiveModel.save(d2vModelFNtPos)\n",
        "\n",
        "   \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cGQg6jO0DCLK"
      },
      "source": [
        "## Are inferred vectors close to the precalculated ones?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xYqVYUT64uOi"
      },
      "source": [
        "doc_id = np.random.randint(d2vPositiveModel.docvecs.count)  # Pick random doc; re-run cell for more examples\n",
        "print('for doc %d...' % doc_id)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "erUzU82xFSQB"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ljWhtEsz4ybf"
      },
      "source": [
        "allDocs[4]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xR76xQpOV74B"
      },
      "source": [
        "doc_id = np.random.randint(d2vPositiveModel.docvecs.count)\n",
        "print(allDocs[doc_id])\n",
        "print(doc_id)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f1aleipXy12-"
      },
      "source": [
        "inferred_docvec = d2vPositiveModel.infer_vector(allDocs[doc_id].words)\n",
        "print('%s:\\n %s' % (d2vPositiveModel, d2vPositiveModel.docvecs.most_similar([inferred_docvec], topn=3)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pvnXr2wi5EEm"
      },
      "source": [
        "allDocs[429632]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3xTX52eC94Wr"
      },
      "source": [
        "# Find similarities"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zszrHKMx9FtI"
      },
      "source": [
        "## Loading Saved d2v Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iGPkZxmf__Js"
      },
      "source": [
        "#No need in local\n",
        "from google.colab import drive\n",
        "drive.mount('/MYDRIVE', force_remount=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yn4Ulgwh9LZn"
      },
      "source": [
        "d2vPositiveModel = Doc2Vec.load(d2vModelFNtPos)\n",
        "d2vNegativeModel = Doc2Vec.load(d2vModelFNtNeg)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t0_CY1LSEsfI"
      },
      "source": [
        "# Find the most similar docs tn = 8\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0bpvKgWoy59e"
      },
      "source": [
        "dfTopic8 = pd.read_csv(csvTopic8)\n",
        "print(dfTopic8.shape)\n",
        "dfTopic8 = dfTopic8.dropna()\n",
        "dfSimTopic8 = pd.DataFrame()\n",
        "keywordsTopic8 = []\n",
        "\n",
        "keywordsTopic8 = dfTopic8['Keywords']\n",
        "\n",
        "dfSimTopicPos = pd.DataFrame()\n",
        "totalVectorPos = np.zeros(400)\n",
        "\n",
        "dfSimTopicNeg = pd.DataFrame()\n",
        "totalVectorNeg = np.zeros(400)\n",
        "\n",
        "\n",
        "\n",
        "for item in keywordsTopic8 :\n",
        "  item = item.replace('[','')\n",
        "  item = item.replace(']','')\n",
        "  item = item.replace(\"'\",'')\n",
        "  jtems = item.split(',')\n",
        "  for jtem in jtems:\n",
        "      inferred_docvecPos = d2vPositiveModel.infer_vector(jtem)\n",
        "      totalVectorPos = totalVectorPos + inferred_docvecPos\n",
        "\n",
        "      inferred_docvecNeg = d2vNegativeModel.infer_vector(jtem)\n",
        "      totalVectorNeg = totalVectorNeg + inferred_docvecNeg\n",
        "\n",
        "\n",
        "\n",
        "  meanVectorPos = totalVectorPos/15\n",
        "  totalVectorPos = np.zeros(400)\n",
        "\n",
        "  meanVectorNeg = totalVectorNeg/15\n",
        "  totalVectorNeg = np.zeros(400)\n",
        "\n",
        "\n",
        "\n",
        "  dfSimTopicPos = dfSimTopicPos.append( d2vPositiveModel.docvecs.most_similar([meanVectorPos], topn=50000))\n",
        "  dfSimTopicNeg = dfSimTopicNeg.append( d2vNegativeModel.docvecs.most_similar([meanVectorNeg], topn=50000))\n",
        "\n",
        "  #end of for\n",
        "\n",
        "\n",
        "dfSimTopicPos.columns = [\"docid\",\"SimilarityScore\"]\n",
        "dfSimTopicNeg.columns = [\"docid\",\"SimilarityScore\"]\n",
        "\n",
        "print(\"Number of unique Positive documents\", dfSimTopicPos[\"docid\"].nunique())\n",
        "print(\"Number of unique Negative documents\", dfSimTopicNeg[\"docid\"].nunique())\n",
        "\n",
        "dfSimTopicPos.sort_values(by='SimilarityScore', ascending=False, inplace=True)\n",
        "dfSimTopicNeg.sort_values(by='SimilarityScore', ascending=False, inplace=True)\n",
        "\n",
        "dfSimTopicPos.drop_duplicates(subset=[\"docid\"], keep='first', inplace = True)\n",
        "dfSimTopicNeg.drop_duplicates(subset=[\"docid\"], keep='first', inplace = True)\n",
        "\n",
        "print(\"Number of unique Positive documents after droping duplicates\", dfSimTopicPos[\"docid\"].nunique())\n",
        "print(\"Number of unique Negative documents after droping duplicates\", dfSimTopicNeg[\"docid\"].nunique())\n",
        "\n",
        "#Sampling 1000 positive and 1000 negatives\n",
        "dfSimTopicPos.sort_values(by='SimilarityScore', ascending=False, inplace=True)\n",
        "dfSimTopicNeg.sort_values(by='SimilarityScore', ascending=False, inplace=True)\n",
        "\n",
        "dfSimTopicPos = dfSimTopicPos.iloc[0:1000, :]\n",
        "dfSimTopicNeg = dfSimTopicNeg.iloc[0:1000, :]\n",
        "\n",
        "#end Sampling 1000 positive and 1000 negatives\n",
        "\n",
        "\n",
        "print(\"Max Positive:\" , max(dfSimTopicPos[\"SimilarityScore\"]))\n",
        "print(\"Max Negatives:\" , max(dfSimTopicNeg[\"SimilarityScore\"]))\n",
        "\n",
        "print(\"Min Positive:\" , min(dfSimTopicPos[\"SimilarityScore\"]))\n",
        "print(\"Min Negatives:\" , min(dfSimTopicNeg[\"SimilarityScore\"]))\n",
        "\n",
        "print(\"Mean Positive:\", dfSimTopicPos[\"SimilarityScore\"].mean())\n",
        "print(\"Mean Positive:\", dfSimTopicNeg[\"SimilarityScore\"].mean())\n",
        "\n",
        "print(\"Std Positive:\",dfSimTopicPos[\"SimilarityScore\"].std())\n",
        "print(\"Std Negative:\",dfSimTopicNeg[\"SimilarityScore\"].std())\n",
        "\n",
        "\n",
        "print(\"len Positive:\" , len(dfSimTopicPos))\n",
        "print(\"len Negatives:\" , len(dfSimTopicNeg))\n",
        "\n",
        "dfAll8 = dfSimTopicNeg.append(dfSimTopicPos)\n",
        "dfAll8.to_csv(csvTopic8MostSimilar, header=True)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rQT8TtiuFS9j"
      },
      "source": [
        "# Find the most similar docs tn = 20"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Di2OwCl9mS-Q"
      },
      "source": [
        "\n",
        "\n",
        "dfTopic20 = pd.read_csv(csvTopic20)\n",
        "print(dfTopic20.shape)\n",
        "dfTopic20 = dfTopic20.dropna()\n",
        "dfSimTopic20 = pd.DataFrame()\n",
        "keywordsTopic20= []\n",
        "\n",
        "keywordsTopic20= dfTopic20['Keywords']\n",
        "\n",
        "dfSimTopicPos = pd.DataFrame()\n",
        "totalVectorPos = np.zeros(400)\n",
        "\n",
        "dfSimTopicNeg = pd.DataFrame()\n",
        "totalVectorNeg = np.zeros(400)\n",
        "\n",
        "\n",
        "\n",
        "for item in keywordsTopic20:\n",
        "  item = item.replace('[','')\n",
        "  item = item.replace(']','')\n",
        "  item = item.replace(\"'\",'')\n",
        "  jtems = item.split(',')\n",
        "  for jtem in jtems:\n",
        "      inferred_docvecPos = d2vPositiveModel.infer_vector(jtem)\n",
        "      totalVectorPos = totalVectorPos + inferred_docvecPos\n",
        "\n",
        "      inferred_docvecNeg = d2vNegativeModel.infer_vector(jtem)\n",
        "      totalVectorNeg = totalVectorNeg + inferred_docvecNeg\n",
        "\n",
        "\n",
        "\n",
        "  meanVectorPos = totalVectorPos/15\n",
        "  totalVectorPos = np.zeros(400)\n",
        "\n",
        "  meanVectorNeg = totalVectorNeg/15\n",
        "  totalVectorNeg = np.zeros(400)\n",
        "\n",
        "\n",
        "\n",
        "  dfSimTopicPos = dfSimTopicPos.append( d2vPositiveModel.docvecs.most_similar([meanVectorPos], topn=50000))\n",
        "  dfSimTopicNeg = dfSimTopicNeg.append( d2vNegativeModel.docvecs.most_similar([meanVectorNeg], topn=50000))\n",
        "\n",
        "  #end of for\n",
        "\n",
        "\n",
        "dfSimTopicPos.columns = [\"docid\",\"SimilarityScore\"]\n",
        "dfSimTopicNeg.columns = [\"docid\",\"SimilarityScore\"]\n",
        "\n",
        "print(\"Number of unique Positive documents\", dfSimTopicPos[\"docid\"].nunique())\n",
        "print(\"Number of unique Negative documents\", dfSimTopicNeg[\"docid\"].nunique())\n",
        "\n",
        "dfSimTopicPos.sort_values(by='SimilarityScore', ascending=False, inplace=True)\n",
        "dfSimTopicNeg.sort_values(by='SimilarityScore', ascending=False, inplace=True)\n",
        "\n",
        "dfSimTopicPos.drop_duplicates(subset=[\"docid\"], keep='first', inplace = True)\n",
        "dfSimTopicNeg.drop_duplicates(subset=[\"docid\"], keep='first', inplace = True)\n",
        "\n",
        "print(\"Number of unique Positive documents after droping duplicates\", dfSimTopicPos[\"docid\"].nunique())\n",
        "print(\"Number of unique Negative documents after droping duplicates\", dfSimTopicNeg[\"docid\"].nunique())\n",
        "\n",
        "#Sampling 1000 positive and 1000 negatives\n",
        "dfSimTopicPos.sort_values(by='SimilarityScore', ascending=False, inplace=True)\n",
        "dfSimTopicNeg.sort_values(by='SimilarityScore', ascending=False, inplace=True)\n",
        "\n",
        "dfSimTopicPos = dfSimTopicPos.iloc[0:1000, :]\n",
        "dfSimTopicNeg = dfSimTopicNeg.iloc[0:1000, :]\n",
        "\n",
        "#end Sampling 1000 positive and 1000 negatives\n",
        "\n",
        "print(\"Max Positive:\" , max(dfSimTopicPos[\"SimilarityScore\"]))\n",
        "print(\"Max Negatives:\" , max(dfSimTopicNeg[\"SimilarityScore\"]))\n",
        "\n",
        "print(\"Min Positive:\" , min(dfSimTopicPos[\"SimilarityScore\"]))\n",
        "print(\"Min Negatives:\" , min(dfSimTopicNeg[\"SimilarityScore\"]))\n",
        "\n",
        "print(\"Mean Positive:\", dfSimTopicPos[\"SimilarityScore\"].mean())\n",
        "print(\"Mean Positive:\", dfSimTopicNeg[\"SimilarityScore\"].mean())\n",
        "\n",
        "print(\"Std Positive:\",dfSimTopicPos[\"SimilarityScore\"].std())\n",
        "print(\"Std Negative:\",dfSimTopicNeg[\"SimilarityScore\"].std())\n",
        "\n",
        "\n",
        "print(\"len Positive:\" , len(dfSimTopicPos))\n",
        "print(\"len Negatives:\" , len(dfSimTopicNeg))\n",
        "\n",
        "dfAll8 = dfSimTopicNeg.append(dfSimTopicPos)\n",
        "dfAll8.to_csv(csvTopic20MostSimilar, header=True)\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OyME1mZ5FUrl"
      },
      "source": [
        "# Find the most similar docs tn = 40"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cJOS7VAImqiz"
      },
      "source": [
        "\n",
        "\n",
        "dfTopic40 = pd.read_csv(csvTopic40)\n",
        "print(dfTopic40.shape)\n",
        "dfTopic40 = dfTopic40.dropna()\n",
        "dfSimTopic40 = pd.DataFrame()\n",
        "keywordsTopic40= []\n",
        "\n",
        "keywordsTopic40= dfTopic40['Keywords']\n",
        "\n",
        "dfSimTopicPos = pd.DataFrame()\n",
        "totalVectorPos = np.zeros(400)\n",
        "\n",
        "dfSimTopicNeg = pd.DataFrame()\n",
        "totalVectorNeg = np.zeros(400)\n",
        "\n",
        "\n",
        "\n",
        "for item in keywordsTopic40:\n",
        "  item = item.replace('[','')\n",
        "  item = item.replace(']','')\n",
        "  item = item.replace(\"'\",'')\n",
        "  jtems = item.split(',')\n",
        "  for jtem in jtems:\n",
        "      inferred_docvecPos = d2vPositiveModel.infer_vector(jtem)\n",
        "      totalVectorPos = totalVectorPos + inferred_docvecPos\n",
        "\n",
        "      inferred_docvecNeg = d2vNegativeModel.infer_vector(jtem)\n",
        "      totalVectorNeg = totalVectorNeg + inferred_docvecNeg\n",
        "\n",
        "\n",
        "\n",
        "  meanVectorPos = totalVectorPos/15\n",
        "  totalVectorPos = np.zeros(400)\n",
        "\n",
        "  meanVectorNeg = totalVectorNeg/15\n",
        "  totalVectorNeg = np.zeros(400)\n",
        "\n",
        "\n",
        "\n",
        "  dfSimTopicPos = dfSimTopicPos.append( d2vPositiveModel.docvecs.most_similar([meanVectorPos], topn=50000))\n",
        "  dfSimTopicNeg = dfSimTopicNeg.append( d2vNegativeModel.docvecs.most_similar([meanVectorNeg], topn=50000))\n",
        "\n",
        "  #end of for\n",
        "\n",
        "\n",
        "dfSimTopicPos.columns = [\"docid\",\"SimilarityScore\"]\n",
        "dfSimTopicNeg.columns = [\"docid\",\"SimilarityScore\"]\n",
        "\n",
        "print(\"Number of unique Positive documents\", dfSimTopicPos[\"docid\"].nunique())\n",
        "print(\"Number of unique Negative documents\", dfSimTopicNeg[\"docid\"].nunique())\n",
        "\n",
        "dfSimTopicPos.sort_values(by='SimilarityScore', ascending=False, inplace=True)\n",
        "dfSimTopicNeg.sort_values(by='SimilarityScore', ascending=False, inplace=True)\n",
        "\n",
        "dfSimTopicPos.drop_duplicates(subset=[\"docid\"], keep='first', inplace = True)\n",
        "dfSimTopicNeg.drop_duplicates(subset=[\"docid\"], keep='first', inplace = True)\n",
        "\n",
        "print(\"Number of unique Positive documents after droping duplicates\", dfSimTopicPos[\"docid\"].nunique())\n",
        "print(\"Number of unique Negative documents after droping duplicates\", dfSimTopicNeg[\"docid\"].nunique())\n",
        "\n",
        "#Sampling 1000 positive and 1000 negatives\n",
        "dfSimTopicPos.sort_values(by='SimilarityScore', ascending=False, inplace=True)\n",
        "dfSimTopicNeg.sort_values(by='SimilarityScore', ascending=False, inplace=True)\n",
        "\n",
        "dfSimTopicPos = dfSimTopicPos.iloc[0:1000, :]\n",
        "dfSimTopicNeg = dfSimTopicNeg.iloc[0:1000, :]\n",
        "\n",
        "#end Sampling 1000 positive and 1000 negatives\n",
        "\n",
        "print(\"Max Positive:\" , max(dfSimTopicPos[\"SimilarityScore\"]))\n",
        "print(\"Max Negatives:\" , max(dfSimTopicNeg[\"SimilarityScore\"]))\n",
        "\n",
        "print(\"Min Positive:\" , min(dfSimTopicPos[\"SimilarityScore\"]))\n",
        "print(\"Min Negatives:\" , min(dfSimTopicNeg[\"SimilarityScore\"]))\n",
        "\n",
        "print(\"Mean Positive:\", dfSimTopicPos[\"SimilarityScore\"].mean())\n",
        "print(\"Mean Positive:\", dfSimTopicNeg[\"SimilarityScore\"].mean())\n",
        "\n",
        "print(\"Std Positive:\",dfSimTopicPos[\"SimilarityScore\"].std())\n",
        "print(\"Std Negative:\",dfSimTopicNeg[\"SimilarityScore\"].std())\n",
        "\n",
        "\n",
        "print(\"len Positive:\" , len(dfSimTopicPos))\n",
        "print(\"len Negatives:\" , len(dfSimTopicNeg))\n",
        "\n",
        "dfAll8 = dfSimTopicNeg.append(dfSimTopicPos)\n",
        "dfAll8.to_csv(csvTopic40MostSimilar, header=True)\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yDAPVn0mFWWU"
      },
      "source": [
        "# Find the most similar docs tn = 50"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m7aDWFX-nB3f"
      },
      "source": [
        "dfTopic50 = pd.read_csv(csvTopic50)\n",
        "print(dfTopic50.shape)\n",
        "dfTopic50 = dfTopic50.dropna()\n",
        "dfSimTopic50 = pd.DataFrame()\n",
        "keywordsTopic50= []\n",
        "\n",
        "keywordsTopic50= dfTopic50['Keywords']\n",
        "\n",
        "dfSimTopicPos = pd.DataFrame()\n",
        "totalVectorPos = np.zeros(400)\n",
        "\n",
        "dfSimTopicNeg = pd.DataFrame()\n",
        "totalVectorNeg = np.zeros(400)\n",
        "\n",
        "\n",
        "\n",
        "for item in keywordsTopic50:\n",
        "  item = item.replace('[','')\n",
        "  item = item.replace(']','')\n",
        "  item = item.replace(\"'\",'')\n",
        "  jtems = item.split(',')\n",
        "  for jtem in jtems:\n",
        "      inferred_docvecPos = d2vPositiveModel.infer_vector(jtem)\n",
        "      totalVectorPos = totalVectorPos + inferred_docvecPos\n",
        "\n",
        "      inferred_docvecNeg = d2vNegativeModel.infer_vector(jtem)\n",
        "      totalVectorNeg = totalVectorNeg + inferred_docvecNeg\n",
        "\n",
        "\n",
        "\n",
        "  meanVectorPos = totalVectorPos/15\n",
        "  totalVectorPos = np.zeros(400)\n",
        "\n",
        "  meanVectorNeg = totalVectorNeg/15\n",
        "  totalVectorNeg = np.zeros(400)\n",
        "\n",
        "\n",
        "\n",
        "  dfSimTopicPos = dfSimTopicPos.append( d2vPositiveModel.docvecs.most_similar([meanVectorPos], topn=50000))\n",
        "  dfSimTopicNeg = dfSimTopicNeg.append( d2vNegativeModel.docvecs.most_similar([meanVectorNeg], topn=50000))\n",
        "\n",
        "  #end of for\n",
        "\n",
        "\n",
        "dfSimTopicPos.columns = [\"docid\",\"SimilarityScore\"]\n",
        "dfSimTopicNeg.columns = [\"docid\",\"SimilarityScore\"]\n",
        "\n",
        "print(\"Number of unique Positive documents\", dfSimTopicPos[\"docid\"].nunique())\n",
        "print(\"Number of unique Negative documents\", dfSimTopicNeg[\"docid\"].nunique())\n",
        "\n",
        "dfSimTopicPos.sort_values(by='SimilarityScore', ascending=False, inplace=True)\n",
        "dfSimTopicNeg.sort_values(by='SimilarityScore', ascending=False, inplace=True)\n",
        "\n",
        "dfSimTopicPos.drop_duplicates(subset=[\"docid\"], keep='first', inplace = True)\n",
        "dfSimTopicNeg.drop_duplicates(subset=[\"docid\"], keep='first', inplace = True)\n",
        "\n",
        "print(\"Number of unique Positive documents after droping duplicates\", dfSimTopicPos[\"docid\"].nunique())\n",
        "print(\"Number of unique Negative documents after droping duplicates\", dfSimTopicNeg[\"docid\"].nunique())\n",
        "\n",
        "#Sampling 1000 positive and 1000 negatives\n",
        "dfSimTopicPos.sort_values(by='SimilarityScore', ascending=False, inplace=True)\n",
        "dfSimTopicNeg.sort_values(by='SimilarityScore', ascending=False, inplace=True)\n",
        "\n",
        "dfSimTopicPos = dfSimTopicPos.iloc[0:1000, :]\n",
        "dfSimTopicNeg = dfSimTopicNeg.iloc[0:1000, :]\n",
        "\n",
        "#end Sampling 1000 positive and 1000 negatives\n",
        "\n",
        "print(\"Max Positive:\" , max(dfSimTopicPos[\"SimilarityScore\"]))\n",
        "print(\"Max Negatives:\" , max(dfSimTopicNeg[\"SimilarityScore\"]))\n",
        "\n",
        "print(\"Min Positive:\" , min(dfSimTopicPos[\"SimilarityScore\"]))\n",
        "print(\"Min Negatives:\" , min(dfSimTopicNeg[\"SimilarityScore\"]))\n",
        "\n",
        "print(\"Mean Positive:\", dfSimTopicPos[\"SimilarityScore\"].mean())\n",
        "print(\"Mean Positive:\", dfSimTopicNeg[\"SimilarityScore\"].mean())\n",
        "\n",
        "print(\"Std Positive:\",dfSimTopicPos[\"SimilarityScore\"].std())\n",
        "print(\"Std Negative:\",dfSimTopicNeg[\"SimilarityScore\"].std())\n",
        "\n",
        "\n",
        "print(\"len Positive:\" , len(dfSimTopicPos))\n",
        "print(\"len Negatives:\" , len(dfSimTopicNeg))\n",
        "\n",
        "dfAll50 = dfSimTopicNeg.append(dfSimTopicPos)\n",
        "dfAll50.to_csv(csvTopic50MostSimilar, header=True)\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n9oj1SUTFYWm"
      },
      "source": [
        "# Find the most similar docs tn = 60"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B3ktIBbvpYu7"
      },
      "source": [
        "\n",
        "dfTopic60 = pd.read_csv(csvTopic60)\n",
        "print(dfTopic60.shape)\n",
        "dfTopic60 = dfTopic60.dropna()\n",
        "dfSimTopic60 = pd.DataFrame()\n",
        "keywordsTopic60= []\n",
        "\n",
        "keywordsTopic60= dfTopic60['Keywords']\n",
        "\n",
        "dfSimTopicPos = pd.DataFrame()\n",
        "totalVectorPos = np.zeros(400)\n",
        "\n",
        "dfSimTopicNeg = pd.DataFrame()\n",
        "totalVectorNeg = np.zeros(400)\n",
        "\n",
        "\n",
        "\n",
        "for item in keywordsTopic60:\n",
        "  item = item.replace('[','')\n",
        "  item = item.replace(']','')\n",
        "  item = item.replace(\"'\",'')\n",
        "  jtems = item.split(',')\n",
        "  for jtem in jtems:\n",
        "      inferred_docvecPos = d2vPositiveModel.infer_vector(jtem)\n",
        "      totalVectorPos = totalVectorPos + inferred_docvecPos\n",
        "\n",
        "      inferred_docvecNeg = d2vNegativeModel.infer_vector(jtem)\n",
        "      totalVectorNeg = totalVectorNeg + inferred_docvecNeg\n",
        "\n",
        "\n",
        "\n",
        "  meanVectorPos = totalVectorPos/15\n",
        "  totalVectorPos = np.zeros(400)\n",
        "\n",
        "  meanVectorNeg = totalVectorNeg/15\n",
        "  totalVectorNeg = np.zeros(400)\n",
        "\n",
        "\n",
        "\n",
        "  dfSimTopicPos = dfSimTopicPos.append( d2vPositiveModel.docvecs.most_similar([meanVectorPos], topn=50000))\n",
        "  dfSimTopicNeg = dfSimTopicNeg.append( d2vNegativeModel.docvecs.most_similar([meanVectorNeg], topn=50000))\n",
        "\n",
        "  #end of for\n",
        "\n",
        "\n",
        "dfSimTopicPos.columns = [\"docid\",\"SimilarityScore\"]\n",
        "dfSimTopicNeg.columns = [\"docid\",\"SimilarityScore\"]\n",
        "\n",
        "print(\"Number of unique Positive documents\", dfSimTopicPos[\"docid\"].nunique())\n",
        "print(\"Number of unique Negative documents\", dfSimTopicNeg[\"docid\"].nunique())\n",
        "\n",
        "dfSimTopicPos.sort_values(by='SimilarityScore', ascending=False, inplace=True)\n",
        "dfSimTopicNeg.sort_values(by='SimilarityScore', ascending=False, inplace=True)\n",
        "\n",
        "dfSimTopicPos.drop_duplicates(subset=[\"docid\"], keep='first', inplace = True)\n",
        "dfSimTopicNeg.drop_duplicates(subset=[\"docid\"], keep='first', inplace = True)\n",
        "\n",
        "print(\"Number of unique Positive documents after droping duplicates\", dfSimTopicPos[\"docid\"].nunique())\n",
        "print(\"Number of unique Negative documents after droping duplicates\", dfSimTopicNeg[\"docid\"].nunique())\n",
        "\n",
        "#Sampling 1000 positive and 1000 negatives\n",
        "dfSimTopicPos.sort_values(by='SimilarityScore', ascending=False, inplace=True)\n",
        "dfSimTopicNeg.sort_values(by='SimilarityScore', ascending=False, inplace=True)\n",
        "\n",
        "dfSimTopicPos = dfSimTopicPos.iloc[0:1000, :]\n",
        "dfSimTopicNeg = dfSimTopicNeg.iloc[0:1000, :]\n",
        "\n",
        "#end Sampling 1000 positive and 1000 negatives\n",
        "\n",
        "print(\"Max Positive:\" , max(dfSimTopicPos[\"SimilarityScore\"]))\n",
        "print(\"Max Negatives:\" , max(dfSimTopicNeg[\"SimilarityScore\"]))\n",
        "\n",
        "print(\"Min Positive:\" , min(dfSimTopicPos[\"SimilarityScore\"]))\n",
        "print(\"Min Negatives:\" , min(dfSimTopicNeg[\"SimilarityScore\"]))\n",
        "\n",
        "print(\"Mean Positive:\", dfSimTopicPos[\"SimilarityScore\"].mean())\n",
        "print(\"Mean Positive:\", dfSimTopicNeg[\"SimilarityScore\"].mean())\n",
        "\n",
        "print(\"Std Positive:\",dfSimTopicPos[\"SimilarityScore\"].std())\n",
        "print(\"Std Negative:\",dfSimTopicNeg[\"SimilarityScore\"].std())\n",
        "\n",
        "\n",
        "print(\"len Positive:\" , len(dfSimTopicPos))\n",
        "print(\"len Negatives:\" , len(dfSimTopicNeg))\n",
        "\n",
        "dfAll60 = dfSimTopicNeg.append(dfSimTopicPos)\n",
        "dfAll60.to_csv(csvTopic60MostSimilar, header=True)\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mye0KNDdFZm4"
      },
      "source": [
        "# Find the most similar docs tn = 65"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OM4bWg5aprEW"
      },
      "source": [
        "\n",
        "\n",
        "dfTopic65 = pd.read_csv(csvTopic65)\n",
        "print(dfTopic65.shape)\n",
        "dfTopic65 = dfTopic65.dropna()\n",
        "dfSimTopic65 = pd.DataFrame()\n",
        "keywordsTopic65= []\n",
        "\n",
        "keywordsTopic65= dfTopic65['Keywords']\n",
        "\n",
        "dfSimTopicPos = pd.DataFrame()\n",
        "totalVectorPos = np.zeros(400)\n",
        "\n",
        "dfSimTopicNeg = pd.DataFrame()\n",
        "totalVectorNeg = np.zeros(400)\n",
        "\n",
        "\n",
        "\n",
        "for item in keywordsTopic65:\n",
        "  item = item.replace('[','')\n",
        "  item = item.replace(']','')\n",
        "  item = item.replace(\"'\",'')\n",
        "  jtems = item.split(',')\n",
        "  for jtem in jtems:\n",
        "      inferred_docvecPos = d2vPositiveModel.infer_vector(jtem)\n",
        "      totalVectorPos = totalVectorPos + inferred_docvecPos\n",
        "\n",
        "      inferred_docvecNeg = d2vNegativeModel.infer_vector(jtem)\n",
        "      totalVectorNeg = totalVectorNeg + inferred_docvecNeg\n",
        "\n",
        "\n",
        "\n",
        "  meanVectorPos = totalVectorPos/15\n",
        "  totalVectorPos = np.zeros(400)\n",
        "\n",
        "  meanVectorNeg = totalVectorNeg/15\n",
        "  totalVectorNeg = np.zeros(400)\n",
        "\n",
        "\n",
        "\n",
        "  dfSimTopicPos = dfSimTopicPos.append( d2vPositiveModel.docvecs.most_similar([meanVectorPos], topn=50000))\n",
        "  dfSimTopicNeg = dfSimTopicNeg.append( d2vNegativeModel.docvecs.most_similar([meanVectorNeg], topn=50000))\n",
        "\n",
        "  #end of for\n",
        "\n",
        "\n",
        "dfSimTopicPos.columns = [\"docid\",\"SimilarityScore\"]\n",
        "dfSimTopicNeg.columns = [\"docid\",\"SimilarityScore\"]\n",
        "\n",
        "print(\"Number of unique Positive documents\", dfSimTopicPos[\"docid\"].nunique())\n",
        "print(\"Number of unique Negative documents\", dfSimTopicNeg[\"docid\"].nunique())\n",
        "\n",
        "dfSimTopicPos.sort_values(by='SimilarityScore', ascending=False, inplace=True)\n",
        "dfSimTopicNeg.sort_values(by='SimilarityScore', ascending=False, inplace=True)\n",
        "\n",
        "dfSimTopicPos.drop_duplicates(subset=[\"docid\"], keep='first', inplace = True)\n",
        "dfSimTopicNeg.drop_duplicates(subset=[\"docid\"], keep='first', inplace = True)\n",
        "\n",
        "print(\"Number of unique Positive documents after droping duplicates\", dfSimTopicPos[\"docid\"].nunique())\n",
        "print(\"Number of unique Negative documents after droping duplicates\", dfSimTopicNeg[\"docid\"].nunique())\n",
        "\n",
        "#Sampling 1000 positive and 1000 negatives\n",
        "dfSimTopicPos.sort_values(by='SimilarityScore', ascending=False, inplace=True)\n",
        "dfSimTopicNeg.sort_values(by='SimilarityScore', ascending=False, inplace=True)\n",
        "\n",
        "dfSimTopicPos = dfSimTopicPos.iloc[0:1000, :]\n",
        "dfSimTopicNeg = dfSimTopicNeg.iloc[0:1000, :]\n",
        "\n",
        "#end Sampling 1000 positive and 1000 negatives\n",
        "\n",
        "print(\"Max Positive:\" , max(dfSimTopicPos[\"SimilarityScore\"]))\n",
        "print(\"Max Negatives:\" , max(dfSimTopicNeg[\"SimilarityScore\"]))\n",
        "\n",
        "print(\"Min Positive:\" , min(dfSimTopicPos[\"SimilarityScore\"]))\n",
        "print(\"Min Negatives:\" , min(dfSimTopicNeg[\"SimilarityScore\"]))\n",
        "\n",
        "print(\"Mean Positive:\", dfSimTopicPos[\"SimilarityScore\"].mean())\n",
        "print(\"Mean Positive:\", dfSimTopicNeg[\"SimilarityScore\"].mean())\n",
        "\n",
        "print(\"Std Positive:\",dfSimTopicPos[\"SimilarityScore\"].std())\n",
        "print(\"Std Negative:\",dfSimTopicNeg[\"SimilarityScore\"].std())\n",
        "\n",
        "\n",
        "print(\"len Positive:\" , len(dfSimTopicPos))\n",
        "print(\"len Negatives:\" , len(dfSimTopicNeg))\n",
        "\n",
        "dfAll65 = dfSimTopicNeg.append(dfSimTopicPos)\n",
        "dfAll65.to_csv(csvTopic65MostSimilar, header=True)\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7RHcSo4-Fa_a"
      },
      "source": [
        "# Find the most similar docs tn = 70"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NyJDj5QVF6mr"
      },
      "source": [
        "csvTopic70MostSimilar"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YgopwngQqKuk"
      },
      "source": [
        "\n",
        "\n",
        "dfTopic70 = pd.read_csv(csvTopic70)\n",
        "print(dfTopic70.shape)\n",
        "dfTopic70 = dfTopic70.dropna()\n",
        "dfSimTopic70 = pd.DataFrame()\n",
        "keywordsTopic70= []\n",
        "\n",
        "keywordsTopic70= dfTopic70['Keywords']\n",
        "\n",
        "dfSimTopicPos = pd.DataFrame()\n",
        "totalVectorPos = np.zeros(400)\n",
        "\n",
        "dfSimTopicNeg = pd.DataFrame()\n",
        "totalVectorNeg = np.zeros(400)\n",
        "\n",
        "\n",
        "\n",
        "for item in keywordsTopic70:\n",
        "  item = item.replace('[','')\n",
        "  item = item.replace(']','')\n",
        "  item = item.replace(\"'\",'')\n",
        "  jtems = item.split(',')\n",
        "  for jtem in jtems:\n",
        "      inferred_docvecPos = d2vPositiveModel.infer_vector(jtem)\n",
        "      totalVectorPos = totalVectorPos + inferred_docvecPos\n",
        "\n",
        "      inferred_docvecNeg = d2vNegativeModel.infer_vector(jtem)\n",
        "      totalVectorNeg = totalVectorNeg + inferred_docvecNeg\n",
        "\n",
        "\n",
        "\n",
        "  meanVectorPos = totalVectorPos/15\n",
        "  totalVectorPos = np.zeros(400)\n",
        "\n",
        "  meanVectorNeg = totalVectorNeg/15\n",
        "  totalVectorNeg = np.zeros(400)\n",
        "\n",
        "\n",
        "\n",
        "  dfSimTopicPos = dfSimTopicPos.append( d2vPositiveModel.docvecs.most_similar([meanVectorPos], topn=50000))\n",
        "  dfSimTopicNeg = dfSimTopicNeg.append( d2vNegativeModel.docvecs.most_similar([meanVectorNeg], topn=50000))\n",
        "\n",
        "  #end of for\n",
        "\n",
        "\n",
        "dfSimTopicPos.columns = [\"docid\",\"SimilarityScore\"]\n",
        "dfSimTopicNeg.columns = [\"docid\",\"SimilarityScore\"]\n",
        "\n",
        "print(\"Number of unique Positive documents\", dfSimTopicPos[\"docid\"].nunique())\n",
        "print(\"Number of unique Negative documents\", dfSimTopicNeg[\"docid\"].nunique())\n",
        "\n",
        "dfSimTopicPos.sort_values(by='SimilarityScore', ascending=False, inplace=True)\n",
        "dfSimTopicNeg.sort_values(by='SimilarityScore', ascending=False, inplace=True)\n",
        "\n",
        "dfSimTopicPos.drop_duplicates(subset=[\"docid\"], keep='first', inplace = True)\n",
        "dfSimTopicNeg.drop_duplicates(subset=[\"docid\"], keep='first', inplace = True)\n",
        "\n",
        "print(\"Number of unique Positive documents after droping duplicates\", dfSimTopicPos[\"docid\"].nunique())\n",
        "print(\"Number of unique Negative documents after droping duplicates\", dfSimTopicNeg[\"docid\"].nunique())\n",
        "\n",
        "#Sampling 1000 positive and 1000 negatives\n",
        "dfSimTopicPos.sort_values(by='SimilarityScore', ascending=False, inplace=True)\n",
        "dfSimTopicNeg.sort_values(by='SimilarityScore', ascending=False, inplace=True)\n",
        "\n",
        "dfSimTopicPos = dfSimTopicPos.iloc[0:1000, :]\n",
        "dfSimTopicNeg = dfSimTopicNeg.iloc[0:1000, :]\n",
        "\n",
        "#end Sampling 1000 positive and 1000 negatives\n",
        "\n",
        "print(\"Max Positive:\" , max(dfSimTopicPos[\"SimilarityScore\"]))\n",
        "print(\"Max Negatives:\" , max(dfSimTopicNeg[\"SimilarityScore\"]))\n",
        "\n",
        "print(\"Min Positive:\" , min(dfSimTopicPos[\"SimilarityScore\"]))\n",
        "print(\"Min Negatives:\" , min(dfSimTopicNeg[\"SimilarityScore\"]))\n",
        "\n",
        "print(\"Mean Positive:\", dfSimTopicPos[\"SimilarityScore\"].mean())\n",
        "print(\"Mean Positive:\", dfSimTopicNeg[\"SimilarityScore\"].mean())\n",
        "\n",
        "print(\"Std Positive:\",dfSimTopicPos[\"SimilarityScore\"].std())\n",
        "print(\"Std Negative:\",dfSimTopicNeg[\"SimilarityScore\"].std())\n",
        "\n",
        "\n",
        "print(\"len Positive:\" , len(dfSimTopicPos))\n",
        "print(\"len Negatives:\" , len(dfSimTopicNeg))\n",
        "\n",
        "dfAll70 = dfSimTopicNeg.append(dfSimTopicPos)\n",
        "dfAll70.to_csv(csvTopic70MostSimilar, header=True)\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kAWPmqdjFcjw"
      },
      "source": [
        "# Find the most similar docs tn = 80"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "USvy6I4OFoxU"
      },
      "source": [
        "csvTopic80MostSimilar"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R6Fvh6CmqjbE"
      },
      "source": [
        "\n",
        "\n",
        "dfTopic80 = pd.read_csv(csvTopic80)\n",
        "print(dfTopic80.shape)\n",
        "dfTopic80 = dfTopic80.dropna()\n",
        "dfSimTopic80 = pd.DataFrame()\n",
        "keywordsTopic80= []\n",
        "\n",
        "keywordsTopic80= dfTopic80['Keywords']\n",
        "\n",
        "dfSimTopicPos = pd.DataFrame()\n",
        "totalVectorPos = np.zeros(400)\n",
        "\n",
        "dfSimTopicNeg = pd.DataFrame()\n",
        "totalVectorNeg = np.zeros(400)\n",
        "\n",
        "\n",
        "\n",
        "for item in keywordsTopic80:\n",
        "  item = item.replace('[','')\n",
        "  item = item.replace(']','')\n",
        "  item = item.replace(\"'\",'')\n",
        "  jtems = item.split(',')\n",
        "  for jtem in jtems:\n",
        "      inferred_docvecPos = d2vPositiveModel.infer_vector(jtem)\n",
        "      totalVectorPos = totalVectorPos + inferred_docvecPos\n",
        "\n",
        "      inferred_docvecNeg = d2vNegativeModel.infer_vector(jtem)\n",
        "      totalVectorNeg = totalVectorNeg + inferred_docvecNeg\n",
        "\n",
        "\n",
        "\n",
        "  meanVectorPos = totalVectorPos/15\n",
        "  totalVectorPos = np.zeros(400)\n",
        "\n",
        "  meanVectorNeg = totalVectorNeg/15\n",
        "  totalVectorNeg = np.zeros(400)\n",
        "\n",
        "\n",
        "\n",
        "  dfSimTopicPos = dfSimTopicPos.append( d2vPositiveModel.docvecs.most_similar([meanVectorPos], topn=50000))\n",
        "  dfSimTopicNeg = dfSimTopicNeg.append( d2vNegativeModel.docvecs.most_similar([meanVectorNeg], topn=50000))\n",
        "\n",
        "  #end of for\n",
        "\n",
        "\n",
        "dfSimTopicPos.columns = [\"docid\",\"SimilarityScore\"]\n",
        "dfSimTopicNeg.columns = [\"docid\",\"SimilarityScore\"]\n",
        "\n",
        "print(\"Number of unique Positive documents\", dfSimTopicPos[\"docid\"].nunique())\n",
        "print(\"Number of unique Negative documents\", dfSimTopicNeg[\"docid\"].nunique())\n",
        "\n",
        "dfSimTopicPos.sort_values(by='SimilarityScore', ascending=False, inplace=True)\n",
        "dfSimTopicNeg.sort_values(by='SimilarityScore', ascending=False, inplace=True)\n",
        "\n",
        "dfSimTopicPos.drop_duplicates(subset=[\"docid\"], keep='first', inplace = True)\n",
        "dfSimTopicNeg.drop_duplicates(subset=[\"docid\"], keep='first', inplace = True)\n",
        "\n",
        "print(\"Number of unique Positive documents after droping duplicates\", dfSimTopicPos[\"docid\"].nunique())\n",
        "print(\"Number of unique Negative documents after droping duplicates\", dfSimTopicNeg[\"docid\"].nunique())\n",
        "\n",
        "#Sampling 1000 positive and 1000 negatives\n",
        "dfSimTopicPos.sort_values(by='SimilarityScore', ascending=False, inplace=True)\n",
        "dfSimTopicNeg.sort_values(by='SimilarityScore', ascending=False, inplace=True)\n",
        "\n",
        "dfSimTopicPos = dfSimTopicPos.iloc[0:1000, :]\n",
        "dfSimTopicNeg = dfSimTopicNeg.iloc[0:1000, :]\n",
        "\n",
        "#end Sampling 1000 positive and 1000 negatives\n",
        "\n",
        "print(\"Max Positive:\" , max(dfSimTopicPos[\"SimilarityScore\"]))\n",
        "print(\"Max Negatives:\" , max(dfSimTopicNeg[\"SimilarityScore\"]))\n",
        "\n",
        "print(\"Min Positive:\" , min(dfSimTopicPos[\"SimilarityScore\"]))\n",
        "print(\"Min Negatives:\" , min(dfSimTopicNeg[\"SimilarityScore\"]))\n",
        "\n",
        "print(\"Mean Positive:\", dfSimTopicPos[\"SimilarityScore\"].mean())\n",
        "print(\"Mean Positive:\", dfSimTopicNeg[\"SimilarityScore\"].mean())\n",
        "\n",
        "print(\"Std Positive:\",dfSimTopicPos[\"SimilarityScore\"].std())\n",
        "print(\"Std Negative:\",dfSimTopicNeg[\"SimilarityScore\"].std())\n",
        "\n",
        "\n",
        "print(\"len Positive:\" , len(dfSimTopicPos))\n",
        "print(\"len Negatives:\" , len(dfSimTopicNeg))\n",
        "\n",
        "dfAll80 = dfSimTopicNeg.append(dfSimTopicPos)\n",
        "dfAll80.to_csv(csvTopic80MostSimilar, header=True)\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}