{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "e3d7jwGaa-Y9",
        "vZfpicuARSxj",
        "SfY7vI7eQoA1",
        "fP1soi0P7w4B",
        "S_Gh5cAy7-3S",
        "0Og_e-oZ81d0",
        "3t0lR8HC_QCN",
        "0w9ZPgG6w_ng",
        "21gDmcUUGOw2",
        "QGEGeZppj-dk",
        "Z6jisWhaVR9A",
        "9mlePpnIt0rm",
        "u3vqpI5_4aaH",
        "byUw4FUoJeWg",
        "HV0iiCaifvJW",
        "W8v3DRX0MPJF",
        "6gqt70t799a9",
        "eX1hY2cdGwv3",
        "-CQaBfqPeVZN"
      ],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yGhQTu6q5upq"
      },
      "source": [
        "This module is responsible for:\n",
        "\n",
        "*   Classifying semantically topic-related documents using different classifiers\n",
        "\n",
        "**Note:**\n",
        "\n",
        "Set the dataset path using **mr8Similar** variable \n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_BwYl-bpgEdE"
      },
      "source": [
        "\n",
        "topicNumber = 8"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e3d7jwGaa-Y9"
      },
      "source": [
        "# Configurations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4w1SUxFB1jAx"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd \n",
        "\n",
        "import string\n",
        "from __future__ import print_function\n",
        "\n",
        "import keras\n",
        "from keras.callbacks import EarlyStopping\n",
        "from keras.preprocessing import sequence\n",
        "from keras.models import Sequential\n",
        "from keras.layers import SimpleRNN, Dense, Dropout, Embedding, LSTM, GRU, Bidirectional, Activation, Conv1D, MaxPooling1D, GlobalMaxPooling1D, BatchNormalization\n",
        "from keras.optimizers import SGD\n",
        "from keras.layers import SpatialDropout1D, Flatten\n",
        "from keras.callbacks import Callback\n",
        "from keras.callbacks import ReduceLROnPlateau\n",
        "from keras import backend\n",
        "from keras.initializers import Constant\n",
        "from keras.regularizers import l2\n",
        "from sklearn.metrics import precision_recall_fscore_support as score\n",
        "\n",
        "from tensorflow.python.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.python.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "from matplotlib.pyplot import plot\n",
        "\n",
        "\n",
        "\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from IPython.display import clear_output \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mve9CxfcQ4Tf"
      },
      "source": [
        "w2vMovieTxt = '/MYDRIVE/My Drive/Colab Notebooks/Proposal/Data/w2vMovie-MR-99-07-25.txt'\n",
        "w2vMovieM = '/MYDRIVE/My Drive/Colab Notebooks/Proposal/Data/w2vMovie-MR-99-07-25.h5'\n",
        "\n",
        "lstmModelTxt = '/MYDRIVE/My Drive/Colab Notebooks/Proposal/Data/lstm-99-07-25.txt'\n",
        "lstmModelH = '/MYDRIVE/My Drive/Colab Notebooks/Proposal/Data/lstm-99-07-25.h5'\n",
        "\n",
        "gruModelTxt = '/MYDRIVE/My Drive/Colab Notebooks/Proposal/Data/gru-99-07-25.txt'\n",
        "gruModelH = '/MYDRIVE/My Drive/Colab Notebooks/Proposal/Data/gru-99-07-25.h5'\n",
        "\n",
        "bilstmModelTxt = '/MYDRIVE/My Drive/Colab Notebooks/Proposal/Data/bilstm-99-07-25.txt'\n",
        "bilstmModelH = '/MYDRIVE/My Drive/Colab Notebooks/Proposal/Data/bilstm-99-07-25.h5'\n",
        "\n",
        "conv1ModelTxt = '/MYDRIVE/My Drive/Colab Notebooks/Proposal/Data/conv1-99-07-25.txt'\n",
        "conv1ModelH = '/MYDRIVE/My Drive/Colab Notebooks/Proposal/Data/conv1-99-07-25.h5'\n",
        "\n",
        "CLModelTxt = '/MYDRIVE/My Drive/Colab Notebooks/Proposal/Data/Cl-99-07-25.txt'\n",
        "CLModelH = '/MYDRIVE/My Drive/Colab Notebooks/Proposal/Data/Cl-99-07-25.h5'\n",
        "\n",
        "#This file includes tweets and topic number\n",
        "tweetsTopics = '/MYDRIVE/My Drive/Colab Notebooks/Proposal/Data/Tweets_Topics_990725.csv'\n",
        "\n",
        "\n",
        "\n",
        "#mr8Similar = '/MYDRIVE/My Drive/Colab Notebooks/Proposal/Data/JokerMostSimilar-99-07-26.csv'\n",
        "mr8Similar = '/MYDRIVE/My Drive/Colab Notebooks/Proposal/Data/Topics_8_MostSimilar.csv'\n",
        "mr20Similar = '/MYDRIVE/My Drive/Colab Notebooks/Proposal/Data/Topics_20_MostSimilar.csv'\n",
        "mr40Similar = '/MYDRIVE/My Drive/Colab Notebooks/Proposal/Data/Topics_40_MostSimilar.csv'\n",
        "mr80Similar = '/MYDRIVE/My Drive/Colab Notebooks/Proposal/Data/Topics_80_MostSimilar.csv'\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#allTrainingData = '/MYDRIVE/My Drive/Colab Notebooks/Proposal/Data/M-A-S140-Processed-98-11-09-03.csv'\n",
        "#allTrainingData = '/MYDRIVE/My Drive/Colab Notebooks/Proposal/Data/movie-pang-processed-98-12-08.csv'\n",
        "\n",
        "allTrainingData = '/MYDRIVE/My Drive/Colab Notebooks/Proposal/Data/Processed-Merged-Dataset-99-08-28.csv'\n",
        "pangLabelledDataset = '/MYDRIVE/My Drive/Colab Notebooks/Proposal/Data/MovieReview-Preprocessed-Pang-Ver2-Labelled.csv'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xH5kk5CeRNy1"
      },
      "source": [
        "##Hyper parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aD5t-DgbRRjp"
      },
      "source": [
        "#Making Word2vec\n",
        "embeddingDim = 500\n",
        "maxLen = embeddingDim\n",
        "testSize=0.2\n",
        "windowSize=5\n",
        "minCount=1 \n",
        "iterCount=10\n",
        "sG = 0 #skip gram wil be used\n",
        "\n",
        "\n",
        "#Making Model\n",
        "validationSplit = 0.2\n",
        "#maxFeatures = numWords\n",
        "maxLen = 500\n",
        "embeddingSize = 500\n",
        "isTrainable = False\n",
        "batchSize = 1000\n",
        "\n",
        "\n",
        "\n",
        "epochsNum = 50\n",
        "\n",
        "unitsNum = 20\n",
        "\n",
        "\n",
        "# Convolution\n",
        "kernelSize = 3\n",
        "\n",
        "filtersNum = 500\n",
        "\n",
        "\n",
        "poolSize = 2\n",
        "stridesNum = 1\n",
        "hidden_dims = 100\n",
        "\n",
        "# LSTM\n",
        "lstm_output_size = 70"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vZfpicuARSxj"
      },
      "source": [
        "#Login to Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wiL9CUU_RICQ"
      },
      "source": [
        "#No need in local\n",
        "from google.colab import drive\n",
        "drive.mount('/MYDRIVE', force_remount=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zk2chpFWcue0"
      },
      "source": [
        "# Sentiment Analysis by W2V Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fw5z3ln3-D_x"
      },
      "source": [
        "## Loading Data to make w2v (Movies)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SfY7vI7eQoA1"
      },
      "source": [
        "## MR8"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "smy2lsmQZ3_H"
      },
      "source": [
        "dfMR8 = pd.DataFrame()\n",
        "dfMR8 = pd.read_csv(mr8Similar, encoding = 'utf-8')\n",
        "print(dfMR8.head(3))\n",
        "dfMR81 = dfMR8\n",
        "print(len(dfMR81))\n",
        "\n",
        "print(dfMR81.columns)\n",
        "\n",
        "dfMR81.columns = [ 'i', 'docid','Probability']\n",
        "\n",
        "print(dfMR81.columns)\n",
        "\n",
        "print(dfMR81.head(3))\n",
        "\n",
        "print(len(dfMR81))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bBd3mKCOWnur"
      },
      "source": [
        "len(dfMR81['docid'].unique())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fP1soi0P7w4B"
      },
      "source": [
        "## Loading all Training labelled data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UONqNxvkOttf"
      },
      "source": [
        "allTrainingData"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mqzViwef75B2"
      },
      "source": [
        "dfALLTraining = pd.DataFrame()\n",
        "dfALLTraining = pd.read_csv(allTrainingData, encoding = 'utf-8', header=None)\n",
        "print(dfALLTraining.head(3))\n",
        "print(len(dfALLTraining))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S_Gh5cAy7-3S"
      },
      "source": [
        "## Finding Moview docs from Training Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AFKtj3ucRYB9"
      },
      "source": [
        "len(dfMR81)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8FasYQjc8EsJ"
      },
      "source": [
        "#This data is similar to topic 11\n",
        "dfTrainingMovies = pd.DataFrame()\n",
        "\n",
        "dfTrainingMovies = dfALLTraining.loc[dfALLTraining[0].isin(dfMR81['docid'])]\n",
        "print(len(dfTrainingMovies))\n",
        "print(dfTrainingMovies.head(3))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Og_e-oZ81d0"
      },
      "source": [
        "## Making Word2Vec Model of movies"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y2fimfOr824U"
      },
      "source": [
        "#embeddingDim = 100\n",
        "#Remove repeated rows\n",
        "dfMoviesNoRep = dfTrainingMovies\n",
        "dfMoviesNoRep.columns = ['i','TweetId', 'Tweet', 'Sentiment']\n",
        "print(dfMoviesNoRep.head(3))  \n",
        "dfMoviesNoRep.sort_values([\"TweetId\"], inplace = True) \n",
        "print(dfMoviesNoRep.head(3))  \n",
        "# dropping ALL duplicte values \n",
        "dfMoviesNoRep.drop_duplicates(subset =\"TweetId\", \n",
        "                     keep = 'first', inplace = True) \n",
        "print(len(dfMoviesNoRep))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qa2dDtwBAQXu"
      },
      "source": [
        "print(len(dfMoviesNoRep[dfMoviesNoRep['Sentiment'] == 0]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DGxjHGYwAqs-"
      },
      "source": [
        "print(len(dfMoviesNoRep[dfMoviesNoRep['Sentiment'] == 1]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IjSZ9vj_P9RK"
      },
      "source": [
        "# Making a balance dataset with equal number of positive and negative tweets\n",
        "df1 = dfMoviesNoRep[dfMoviesNoRep['Sentiment'] == 1].sample(73178)\n",
        "df2 = dfMoviesNoRep[dfMoviesNoRep['Sentiment'] == 0].sample(73178)\n",
        "df = df1.append(df2)\n",
        "print(len(df[df['Sentiment'] == 1]))\n",
        "print(df.head())\n",
        "print(len(df))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LPBWd4-s6Dhd"
      },
      "source": [
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KQxrfNyF5aev"
      },
      "source": [
        "dfPang = pd.DataFrame()\n",
        "dfPang = pd.read_csv(pangLabelledDataset, encoding = 'utf-8', header=None)\n",
        "print(dfPang.head(3))\n",
        "print(len(dfPang))\n",
        "dfPang.insert(0,'i',0)\n",
        "dfPang.columns = ['i' , 'TweetId', 'Tweet', 'Sentiment']\n",
        "df = df.append(dfPang)\n",
        "print(len(df))\n",
        "print(len(df[df['Sentiment'] == 1]))\n",
        "print(len(df[df['Sentiment'] == 0]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1b359Ruv6IHx"
      },
      "source": [
        "dfPang.head(3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PmS2TzUM6aFy"
      },
      "source": [
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7AkcksOm9GrL"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "x_train_m, x_test_m, y_train_m, y_test_m = train_test_split( df['Tweet'], df['Sentiment'], test_size=0.2, random_state=10)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qao0W_pL9R7l"
      },
      "source": [
        "print(len(x_test_m))\n",
        "print(len(x_train_m))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ENuvrNGaD0b1"
      },
      "source": [
        "print(len(y_test_m[y_test_m == 0]))\n",
        "print(len(y_test_m[y_test_m == 1]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QN6GDBRMmED1"
      },
      "source": [
        "print(len(y_train_m[y_train_m == 0]))\n",
        "print(len(y_train_m[y_train_m == 1]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zxjq4l2XhdZD"
      },
      "source": [
        "reviewMovieLines = list()\n",
        "for line in x_train_m:\n",
        "  reviewMovieLines.append( str(line).split() )\n",
        "\n",
        "for line in x_test_m:\n",
        "  reviewMovieLines.append( str(line).split() )\n",
        "\n",
        "print(len(reviewMovieLines))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JMET9VLG9eun"
      },
      "source": [
        "'''\n",
        "reviewMovieLines = list()\n",
        "\n",
        "lines = df['Tweet'].values.tolist()\n",
        "print(str(lines[0]).split())\n",
        "for line in lines:\n",
        "  \n",
        "  reviewMovieLines.append( str(line).split() )\n",
        "  '''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PAYD3fv99i_X"
      },
      "source": [
        "import gensim\n",
        "\n",
        "\n",
        "w2vMovie = gensim.models.Word2Vec( sentences=reviewMovieLines, size=embeddingDim,  window=windowSize, workers=4, min_count=minCount, iter=iterCount, sg = sG )\n",
        "\n",
        "w2vGensimModel = gensim.models.Word2Vec()\n",
        "#vocab size\n",
        "\n",
        "words = list( w2vMovie.wv.vocab )\n",
        "\n",
        "print('Vocabulary size: %d' % len(words))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zKy7YUy504rl"
      },
      "source": [
        "w2vMovieTxt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YAQlz8Rk-nUU"
      },
      "source": [
        "w2vMovie.wv.save_word2vec_format( w2vMovieTxt, binary = False)\n",
        "w2vMovie.save(w2vMovieM)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6_jkEURK-1tX"
      },
      "source": [
        "w2vMovie.wv.most_similar('film')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AFoWPQeSa7vi"
      },
      "source": [
        "w2vMovie.wv.most_similar('movi')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3t0lR8HC_QCN"
      },
      "source": [
        "## Desining new network by w2v"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "91tbeVAsc243"
      },
      "source": [
        "import os\n",
        "embeddingsIndex = {}\n",
        "f = open(os.path.join('', w2vMovieTxt ), encoding = \"utf-8\")\n",
        "\n",
        "for line in f:\n",
        "  values = line.split()\n",
        "  word = values[0]\n",
        "  coefs = np.asarray(values[1:])\n",
        "  embeddingsIndex[word] = coefs\n",
        "f.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JO9Rk-6CdYrX"
      },
      "source": [
        "tokenizerObj = Tokenizer()\n",
        "tokenizerObj.fit_on_texts(reviewMovieLines)\n",
        "sequences = tokenizerObj.texts_to_sequences(reviewMovieLines)\n",
        "\n",
        "print(type(sequences))\n",
        "\n",
        "reviewPad = pad_sequences( sequences, maxlen=maxLen)\n",
        "\n",
        "wordIndex = tokenizerObj.word_index\n",
        "print('Found %s unique tokens.' % len(wordIndex))\n",
        "\n",
        "#reviewPad = pad_sequences( sequences, maxlen=maxLen)\n",
        "#sentiment = df ['Sentiment'].values\n",
        "print(type(reviewPad))\n",
        "print(len(reviewPad))\n",
        "\n",
        "x_train_pad = reviewPad[ : len(x_train_m)]\n",
        "x_test_pad = reviewPad[ len(x_train_m) : ]\n",
        "y_train = y_train_m\n",
        "y_test = y_test_m\n",
        "\n",
        "print(type(x_train_pad))\n",
        "\n",
        "print('Shape of x_train_pad tensor:' , len(x_train_pad))\n",
        "print('Shape of y_train tensor:' , len(y_train))\n",
        "\n",
        "\n",
        "print('Shape of x_test_pad tensor:' , len(x_test_pad))\n",
        "print('Shape of y_test tensor:' , len( y_test ))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VWUL1vhAixBt"
      },
      "source": [
        "\n",
        "'''\n",
        "trainTemp = list()\n",
        "testTemp = list()\n",
        "\n",
        "for line in x_test_m:\n",
        "  testTemp.append( str(line).split() )\n",
        "\n",
        "for line in x_train_m:\n",
        "  trainTemp.append( str(line).split() )\n",
        "  \n",
        "\n",
        "tokenizerObj1 = Tokenizer()\n",
        "tokenizerObj1.fit_on_texts(testTemp)\n",
        "seqTest = tokenizerObj1.texts_to_sequences(testTemp)\n",
        "\n",
        "wordIndex = tokenizerObj1.word_index\n",
        "print('Found %s unique tokens.' % len(wordIndex))\n",
        "\n",
        "tokenizerObj1 = Tokenizer()\n",
        "tokenizerObj1.fit_on_texts(trainTemp)\n",
        "seqTrain = tokenizerObj1.texts_to_sequences(trainTemp)\n",
        "\n",
        "\n",
        "wordIndex = tokenizerObj1.word_index\n",
        "print('Found %s unique tokens.' % len(wordIndex))\n",
        "\n",
        "\n",
        "x_test_pad = pad_sequences( seqTest , maxlen=maxLen)\n",
        "x_train_pad = pad_sequences( seqTrain , maxlen=maxLen)\n",
        "\n",
        "y_train = y_train_m\n",
        "y_test = y_test_m\n",
        "\n",
        "\n",
        "\n",
        "print('Shape of x_train_pad tensor:' , x_train_pad.shape)\n",
        "print('Shape of y_train tensor:' , y_train.shape)\n",
        "\n",
        "\n",
        "print('Shape of x_test_pad tensor:' , x_test_pad.shape)\n",
        "print('Shape of y_test tensor:' , y_test.shape)\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WKiYo7J-tX4j"
      },
      "source": [
        "x = y_test[y_test == 0]\n",
        "print(len(x))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Acb97fyUDeTn"
      },
      "source": [
        "x = y_test[y_test == 1]\n",
        "print(len(x))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RiCiwBFGf8r5"
      },
      "source": [
        "x = y_train[y_train == 0]\n",
        "print(len(x))\n",
        "x = y_train[y_train == 1]\n",
        "print(len(x))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XLSV3pFcFEE3"
      },
      "source": [
        "embeddingDim"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "79qVnwskddTl"
      },
      "source": [
        "numWords = len(wordIndex) + 1\n",
        "print(numWords)\n",
        "embeddingMatrix = np.zeros( (numWords, embeddingDim) )\n",
        "#print(embeddingMatrix.shape)\n",
        "for word, i in wordIndex.items():\n",
        "  if i > numWords:\n",
        "    continue\n",
        "  embeddingVector = embeddingsIndex.get(word)\n",
        "  if embeddingVector is not None:\n",
        "    # words not found in embedding index will be all-zeros.\n",
        "    embeddingMatrix[i] = embeddingVector"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MXzYzYzmlbX9"
      },
      "source": [
        "print(embeddingMatrix.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fvc_NhFlp15v"
      },
      "source": [
        "max_features = numWords"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0w9ZPgG6w_ng"
      },
      "source": [
        "# Finding Tweets relevant to topic number"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C1uMuQzBxkrJ"
      },
      "source": [
        "dfTweets = pd.read_csv(tweetsTopics)\n",
        "print (tweetsTopics)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WzciPBuIyoAO"
      },
      "source": [
        "dfTweetsRelevantTopic =  dfTweets[dfTweets['Dominant_Topic'] == topicNumber] "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zbjernxAzK7Y"
      },
      "source": [
        "len(dfTweetsRelevantTopic)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "21gDmcUUGOw2"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "# CNN+BIGRU\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5CORclx3GPMK"
      },
      "source": [
        "# Embedding\n",
        "\n",
        "max_features = numWords\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Convolution\n",
        "\n",
        "print('Build model...')\n",
        "\n",
        "modelCBIGRU = Sequential(name='CNNBIGRU')\n",
        "\n",
        "modelCBIGRU.add(Embedding(numWords, embeddingDim , \n",
        "                      embeddings_initializer = Constant( embeddingMatrix), \n",
        "                      input_length=maxLen,\n",
        "                      trainable = isTrainable))\n",
        "\n",
        "\n",
        "modelCBIGRU.add(Dropout(0.3))\n",
        "\n",
        "modelCBIGRU.add(Conv1D(filters= filtersNum, kernel_size= 3, strides= stridesNum, padding='same', activation='relu'))\n",
        "modelCBIGRU.add(MaxPooling1D(pool_size = poolSize))\n",
        "\n",
        "modelCBIGRU.add(Dropout(0.5))\n",
        "modelCBIGRU.add(Bidirectional(GRU(20, dropout=0.5 )))\n",
        "modelCBIGRU.add(BatchNormalization())\n",
        "modelCBIGRU.add(Dropout(0.5))\n",
        "\n",
        "\n",
        "modelCBIGRU.add(Dense(1,activation='sigmoid'))\n",
        "\n",
        "\n",
        "\n",
        "opt = SGD(lr=0.1)\n",
        "\n",
        "\n",
        "modelCBIGRU.compile(loss='binary_crossentropy',\n",
        "              optimizer='adam',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "\n",
        "#modelCGRU.summary()\n",
        "\n",
        "print('Train...')\n",
        "\n",
        "rlrp = ReduceLROnPlateau(monitor='val_loss', \n",
        "                                  factor=0.2, \n",
        "                                  patience=4, \n",
        "                                  verbose=1, \n",
        "                                  mode='auto', \n",
        "                                  min_delta=0.0001, \n",
        "                                  cooldown=8, \n",
        "                                  min_lr=0)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        " \n",
        "historyCBIGRU = modelCBIGRU.fit(x_train_pad, y_train,\n",
        "          batch_size=batchSize,\n",
        "          epochs=epochsNum,\n",
        "          validation_data=(x_test_pad, y_test), verbose = 1 )#, callbacks=[rlrp])\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dE2II_wmsKhX"
      },
      "source": [
        "testLossCBIGRU, testAccCBIGRU = modelCBIGRU.evaluate(x_test_pad, y_test, batch_size=batchSize, verbose=1)\n",
        "print('Test loss:', testLossCBIGRU)\n",
        "print('Test accuracy:', testAccCBIGRU)\n",
        "\n",
        "trainLossCBIGRU, trainAccCBIGRU = modelCBIGRU.evaluate(x_train_pad, y_train, batch_size=batchSize, verbose=1)\n",
        "print('Train loss:', trainLossCBIGRU)\n",
        "print('Train accuracy:', trainAccCBIGRU)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LXHg6eWnH1Bq"
      },
      "source": [
        "modelCBIGRU.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "URzFMln7IRx4"
      },
      "source": [
        "plt.title('Accuracy')\n",
        "plt.plot(historyCBIGRU.history['accuracy'], label='train')\n",
        "plt.plot(historyCBIGRU.history['val_accuracy'], label='test')\n",
        "plt.legend()\n",
        "plt.show();"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z6lDpaOGITgj"
      },
      "source": [
        "plt.title('Loss')\n",
        "plt.plot(historyCBIGRU.history['loss'], label='train')\n",
        "plt.plot(historyCBIGRU.history['val_loss'], label='test')\n",
        "plt.legend()\n",
        "plt.show();"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eWdvHvNeIXdi"
      },
      "source": [
        "y_val_pred_cbg=modelCBIGRU.predict_classes(x_test_pad)\n",
        "precisionCBIGRU, recallCBIGRU, fscoreCBIGRU, supportCBIGRU = score(y_test, y_val_pred_cbg)\n",
        "\n",
        "print('precision: {}'.format(precisionCBIGRU))\n",
        "print('recall: {}'.format(recallCBIGRU))\n",
        "print('fscore: {}'.format(fscoreCBIGRU))\n",
        "print('support: {}'.format(supportCBIGRU))\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EF4zLSw-8vuj"
      },
      "source": [
        "from sklearn.metrics import cohen_kappa_score\n",
        "\n",
        "cohen_score = cohen_kappa_score(y_test, y_val_pred_cbg)\n",
        "\n",
        "print(cohen_score)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5SUSzvgM6BUm"
      },
      "source": [
        "import seaborn as sns\n",
        "from sklearn.metrics import confusion_matrix\n",
        "labels = [0,1]\n",
        "def cm_analysis(y_true, y_pred, labels, ymap=None, figsize=(10,10)):\n",
        "    \"\"\"\n",
        "    Generate matrix plot of confusion matrix with pretty annotations.\n",
        "    The plot image is saved to disk.\n",
        "    args: \n",
        "      y_true:    true label of the data, with shape (nsamples,)\n",
        "      y_pred:    prediction of the data, with shape (nsamples,)\n",
        "      filename:  filename of figure file to save\n",
        "      labels:    string array, name the order of class labels in the confusion matrix.\n",
        "                 use `clf.classes_` if using scikit-learn models.\n",
        "                 with shape (nclass,).\n",
        "      ymap:      dict: any -> string, length == nclass.\n",
        "                 if not None, map the labels & ys to more understandable strings.\n",
        "                 Caution: original y_true, y_pred and labels must align.\n",
        "      figsize:   the size of the figure plotted.\n",
        "    \"\"\"\n",
        "    if ymap is not None:\n",
        "        y_pred = [ymap[yi] for yi in y_pred]\n",
        "        y_true = [ymap[yi] for yi in y_true]\n",
        "        labels = [ymap[yi] for yi in labels]\n",
        "    cm = confusion_matrix(y_true, y_pred, labels=labels)\n",
        "    cm_sum = np.sum(cm, axis=1, keepdims=True)\n",
        "    cm_perc = cm / cm_sum.astype(float) * 100\n",
        "    annot = np.empty_like(cm).astype(str)\n",
        "    nrows, ncols = cm.shape\n",
        "    for i in range(nrows):\n",
        "        for j in range(ncols):\n",
        "            c = cm[i, j]\n",
        "            p = cm_perc[i, j]\n",
        "            if i == j:\n",
        "                s = cm_sum[i]\n",
        "                annot[i, j] = '%.1f%%\\n%d/%d' % (p, c, s)\n",
        "            elif c == 0:\n",
        "                annot[i, j] = ''\n",
        "            else:\n",
        "                annot[i, j] = '%.1f%%\\n%d' % (p, c)\n",
        "    cm = pd.DataFrame(cm, index=labels, columns=labels)\n",
        "    cm.index.name = 'Actual'\n",
        "    cm.columns.name = 'Predicted'\n",
        "    #fig, ax = plt.subplots(figsize=figsize)\n",
        "    fig, ax = plt.subplots()\n",
        "    sns.heatmap(cm, annot=annot, fmt='', ax=ax)\n",
        "    #plt.savefig(filename)\n",
        "    plt.show()\n",
        "\n",
        "cm_analysis(y_test, y_val_pred_cbg, labels, ymap=None, figsize=(5,5))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yiq4_lchIOgo"
      },
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt     \n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "\n",
        "labels = [0,1]\n",
        "cm = confusion_matrix(y_test, y_val_pred_cbg, labels)\n",
        "\n",
        "ax= plt.subplot()\n",
        "sns.heatmap(cm, annot=True, fmt=\"g\", ax = ax,); #annot=True to annotate cells\n",
        "\n",
        "# labels, title and ticks\n",
        "ax.set_xlabel('Predicted labels');\n",
        "ax.set_ylabel('True labels'); \n",
        "ax.set_title('Confusion Matrix'); \n",
        "ax.xaxis.set_ticklabels(labels); \n",
        "ax.yaxis.set_ticklabels(labels);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DJewEDzyIaEr"
      },
      "source": [
        "'''\n",
        "print(dfTweetsRelevantTopic.head(3))\n",
        "#testJokerSamples = dfJokerTweets[1]\n",
        "testJokerSamples = dfTweetsRelevantTopic[\"Text\"]\n",
        "testJokerSamples = testJokerSamples.dropna()\n",
        "#print(testSamples[1])\n",
        "tokenizerObj.fit_on_texts(testJokerSamples)\n",
        "testJokerSamplesTokens = tokenizerObj.texts_to_sequences( testJokerSamples )\n",
        "testJokerSamplesTokensPad = pad_sequences( testJokerSamplesTokens, maxlen = maxLen)\n",
        "print(len(testJokerSamplesTokensPad))\n",
        "\n",
        "Xnew = testJokerSamplesTokensPad\n",
        "\n",
        "\n",
        "\n",
        "ynew = modelCBIGRU.predict_classes(Xnew)\n",
        "#ynew = modelCGRU.predict_classes(Xnew)\n",
        "#ynew = modelCGRU.predict_classes(Xnew)\n",
        "print(type(ynew))\n",
        "print(ynew[0])\n",
        "\n",
        "\n",
        "numPositiveCBIGRU = 0\n",
        "numNegativeCBIGRU = 0\n",
        "for i in range(0, len(ynew)):\n",
        "  if ynew[i]==0:\n",
        "    numNegativeCBIGRU = numNegativeCBIGRU + 1\n",
        "  elif  ynew[i] == 1:\n",
        "    numPositiveCBIGRU = numPositiveCBIGRU + 1\n",
        "print('Positive', numPositiveCBIGRU)\n",
        "print('Negatives', numNegativeCBIGRU)\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "saIUOVpdwSQb"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QGEGeZppj-dk"
      },
      "source": [
        "# CNN+GRU\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RxkQ0JORf6ml"
      },
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "np.random.seed(1234)\n",
        "tf.random.set_seed(1234)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fV73BuEej-dm"
      },
      "source": [
        "from keras.layers import GlobalMaxPooling1D\n",
        "\n",
        "max_features = numWords\n",
        "epochsNum = 20\n",
        "batchSize = 500\n",
        "poolSize = 2\n",
        "stridesNum = 1\n",
        "max_features = numWords\n",
        "print('Build model...')\n",
        "modelCGRU = Sequential(name='CNNGRU')\n",
        "modelCGRU.add(Embedding(numWords, embeddingDim , \n",
        "                      embeddings_initializer = Constant( embeddingMatrix), \n",
        "                      input_length=maxLen,\n",
        "                      trainable = isTrainable))\n",
        "\n",
        "modelCGRU.add(Conv1D(filters= 500, kernel_size= 7, strides= stridesNum, padding='same', activation='relu'))\n",
        "#modelCGRU.add(BatchNormalization()) \n",
        "modelCGRU.add(Dropout(0.4)) \n",
        "modelCGRU.add(MaxPooling1D(pool_size = poolSize))\n",
        "#modelCGRU.add(Dropout(0.3))\n",
        "modelCGRU.add(GRU(20, dropout=0.6))\n",
        "#modelCGRU.add(Dropout(0.3))\n",
        "modelCGRU.add(Dense(10, activation='relu'))\n",
        "modelCGRU.add(Dense(1,activation='sigmoid'))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "opt = SGD(lr=0.1)\n",
        "modelCGRU.compile(loss='binary_crossentropy',\n",
        "              optimizer= 'adam',\n",
        "              metrics=['accuracy'])\n",
        "print('Train...')\n",
        "rlrp = ReduceLROnPlateau(monitor='val_loss', \n",
        "                                  factor=0.2, \n",
        "                                  patience=4, \n",
        "                                  verbose=1, \n",
        "                                  mode='auto', \n",
        "                                  min_delta=0.0001, \n",
        "                                  cooldown=8, \n",
        "                                  min_lr=0)\n",
        "\n",
        "historyCGRU = modelCGRU.fit(x_train_pad, y_train,\n",
        "          batch_size=batchSize,\n",
        "          epochs=epochsNum,\n",
        "          validation_data=(x_test_pad, y_test), verbose = 1 )#, callbacks=[rlrp])\n",
        "clear_output()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1kCtamTXj-du"
      },
      "source": [
        "testLossCGRU, testAccCGRU = modelCGRU.evaluate(x_test_pad, y_test, batch_size=batchSize, verbose=1)\n",
        "print('Test loss:', testLossCGRU)\n",
        "print('Test accuracy:', testAccCGRU)\n",
        "\n",
        "trainLossCGRU, trainAccCGRU = modelCGRU.evaluate(x_train_pad, y_train, batch_size=batchSize, verbose=1)\n",
        "print('Train loss:', trainLossCGRU)\n",
        "print('Train accuracy:', trainAccCGRU)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e2vnZgXKgGHq"
      },
      "source": [
        "y_val_pred_cgru=modelCGRU.predict_classes(x_test_pad)\n",
        "precisionCGRU, recallCGRU, fscoreCGRU, supportCGRU = score(y_test, y_val_pred_cgru)\n",
        "\n",
        "print('precision: {}'.format(precisionCGRU))\n",
        "print('recall: {}'.format(recallCGRU))\n",
        "print('fscore: {}'.format(fscoreCGRU))\n",
        "print('support: {}'.format(supportCGRU))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SIHGj1IPj-d3"
      },
      "source": [
        "plt.title('Accuracy')\n",
        "plt.plot(historyCGRU.history['accuracy'], label='train')\n",
        "plt.plot(historyCGRU.history['val_accuracy'], label='test')\n",
        "plt.legend()\n",
        "plt.show();"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dKsvFV9sj-eB"
      },
      "source": [
        "plt.title('Loss')\n",
        "plt.plot(historyCGRU.history['loss'], label='train')\n",
        "plt.plot(historyCGRU.history['val_loss'], label='test')\n",
        "plt.legend()\n",
        "plt.show();"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hWndSlKqj-eF"
      },
      "source": [
        "y_val_pred_cgru=modelCGRU.predict_classes(x_test_pad)\n",
        "precisionCGRU, recallCGRU, fscoreCGRU, supportCGRU = score(y_test, y_val_pred_cgru)\n",
        "\n",
        "print('precision: {}'.format(precisionCGRU))\n",
        "print('recall: {}'.format(recallCGRU))\n",
        "print('fscore: {}'.format(fscoreCGRU))\n",
        "print('support: {}'.format(supportCGRU))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "34KOCnr8JyqX"
      },
      "source": [
        "from sklearn.metrics import cohen_kappa_score\n",
        "cohen_score = cohen_kappa_score(y_test, y_val_pred)\n",
        "print(cohen_score)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XJeGkVk31y1q"
      },
      "source": [
        "len(y_test[y_test==1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Pbp0Iy9j-eJ"
      },
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt     \n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "labels = [0,1]\n",
        "cm = confusion_matrix(y_test, y_val_pred_cgru, labels)\n",
        "\n",
        "ax= plt.subplot()\n",
        "sns.heatmap(cm, annot=True, fmt=\"g\", ax = ax,); #annot=True to annotate cells\n",
        "\n",
        "# labels, title and ticks\n",
        "ax.set_xlabel('Predicted labels');\n",
        "ax.set_ylabel('True labels'); \n",
        "ax.set_title('Confusion Matrix'); \n",
        "ax.xaxis.set_ticklabels(labels); \n",
        "ax.yaxis.set_ticklabels(labels);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oroegyCpj-eM"
      },
      "source": [
        "import seaborn as sns\n",
        "from sklearn.metrics import confusion_matrix\n",
        "labels = [0,1]\n",
        "def cm_analysis(y_true, y_pred, labels, ymap=None, figsize=(10,10)):\n",
        "    \"\"\"\n",
        "    Generate matrix plot of confusion matrix with pretty annotations.\n",
        "    The plot image is saved to disk.\n",
        "    args: \n",
        "      y_true:    true label of the data, with shape (nsamples,)\n",
        "      y_pred:    prediction of the data, with shape (nsamples,)\n",
        "      filename:  filename of figure file to save\n",
        "      labels:    string array, name the order of class labels in the confusion matrix.\n",
        "                 use `clf.classes_` if using scikit-learn models.\n",
        "                 with shape (nclass,).\n",
        "      ymap:      dict: any -> string, length == nclass.\n",
        "                 if not None, map the labels & ys to more understandable strings.\n",
        "                 Caution: original y_true, y_pred and labels must align.\n",
        "      figsize:   the size of the figure plotted.\n",
        "    \"\"\"\n",
        "    if ymap is not None:\n",
        "        y_pred = [ymap[yi] for yi in y_pred]\n",
        "        y_true = [ymap[yi] for yi in y_true]\n",
        "        labels = [ymap[yi] for yi in labels]\n",
        "    cm = confusion_matrix(y_true, y_pred, labels=labels)\n",
        "    cm_sum = np.sum(cm, axis=1, keepdims=True)\n",
        "    cm_perc = cm / cm_sum.astype(float) * 100\n",
        "    annot = np.empty_like(cm).astype(str)\n",
        "    nrows, ncols = cm.shape\n",
        "    for i in range(nrows):\n",
        "        for j in range(ncols):\n",
        "            c = cm[i, j]\n",
        "            p = cm_perc[i, j]\n",
        "            if i == j:\n",
        "                s = cm_sum[i]\n",
        "                annot[i, j] = '%.1f%%\\n%d/%d' % (p, c, s)\n",
        "            elif c == 0:\n",
        "                annot[i, j] = ''\n",
        "            else:\n",
        "                annot[i, j] = '%.1f%%\\n%d' % (p, c)\n",
        "    cm = pd.DataFrame(cm, index=labels, columns=labels)\n",
        "    cm.index.name = 'Actual'\n",
        "    cm.columns.name = 'Predicted'\n",
        "    #fig, ax = plt.subplots(figsize=figsize)\n",
        "    fig, ax = plt.subplots()\n",
        "    sns.heatmap(cm, annot=annot, fmt='', ax=ax)\n",
        "    #plt.savefig(filename)\n",
        "    plt.show()\n",
        "\n",
        "cm_analysis(y_test, y_val_pred_cgru, labels, ymap=None, figsize=(5,5))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v8ic3ZXnj-eQ"
      },
      "source": [
        "'''\n",
        "print(dfTweetsRelevantTopic.head(3))\n",
        "#testJokerSamples = dfJokerTweets[1]\n",
        "testJokerSamples = dfTweetsRelevantTopic[\"Text\"]\n",
        "testJokerSamples = testJokerSamples.dropna()\n",
        "#print(testSamples[1])\n",
        "tokenizerObj.fit_on_texts(testJokerSamples)\n",
        "testJokerSamplesTokens = tokenizerObj.texts_to_sequences( testJokerSamples )\n",
        "testJokerSamplesTokensPad = pad_sequences( testJokerSamplesTokens, maxlen = maxLen)\n",
        "print(len(testJokerSamplesTokensPad))\n",
        "\n",
        "Xnew = testJokerSamplesTokensPad\n",
        "\n",
        "\n",
        "\n",
        "ynew = modelCGRU_s.predict_classes(Xnew)\n",
        "#ynew = modelCGRU.predict_classes(Xnew)\n",
        "#ynew = modelCGRU.predict_classes(Xnew)\n",
        "print(type(ynew))\n",
        "print(ynew[0])\n",
        "\n",
        "\n",
        "numPositiveCGRU = 0\n",
        "numNegativeCGRU = 0\n",
        "for i in range(0, len(ynew)):\n",
        "  if ynew[i]==0:\n",
        "    numNegativeCGRU = numNegativeCGRU + 1\n",
        "  elif  ynew[i] == 1:\n",
        "    numPositiveCGRU = numPositiveCGRU + 1\n",
        "print('Positive', numPositiveCGRU)\n",
        "print('Negatives', numNegativeCGRU)\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hBoZmQRyjrKv"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z6jisWhaVR9A"
      },
      "source": [
        "# CNN+BiLSTM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5phUyoo9VWxu"
      },
      "source": [
        "# Embedding\n",
        "\n",
        "max_features = numWords\n",
        "filtersNum = 400\n",
        "unitsNum = 20\n",
        "\n",
        "# Convolution\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "print('Build model...')\n",
        "\n",
        "modelCBiLSTM = Sequential(name='CNNBiLSTM')\n",
        "modelCBiLSTM.add(Embedding(numWords, embeddingDim , \n",
        "                      embeddings_initializer = Constant( embeddingMatrix), \n",
        "                      input_length=maxLen,\n",
        "                      trainable = isTrainable))\n",
        "\n",
        "\n",
        "modelCBiLSTM.add(Conv1D(filters= filtersNum, kernel_size= kernelSize, strides= stridesNum, padding='same', activation='relu'))\n",
        "\n",
        "\n",
        "modelCBiLSTM.add(Dropout(0.5))\n",
        "modelCBiLSTM.add(MaxPooling1D(pool_size = poolSize))\n",
        "\n",
        "modelCBiLSTM.add(Dropout(0.5))\n",
        "modelCBiLSTM.add(Bidirectional(LSTM(20, dropout=0.5 )))\n",
        "modelCBiLSTM.add(BatchNormalization())\n",
        "modelCBiLSTM.add(Dropout(0.5))\n",
        "\n",
        "modelCBiLSTM.add(Dense(1,activation='sigmoid'))\n",
        "\n",
        "\n",
        "\n",
        "opt = SGD(lr=0.1)\n",
        "\n",
        "\n",
        "modelCBiLSTM.compile(loss='binary_crossentropy',\n",
        "              optimizer='adam',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "\n",
        "#modelCBiLSTM.summary()\n",
        "\n",
        "print('Train...')\n",
        "\n",
        "rlrp = ReduceLROnPlateau(monitor='val_loss', \n",
        "                                  factor=0.1, \n",
        "                                  patience=8, \n",
        "                                  verbose=1, \n",
        "                                  mode='auto', \n",
        "                                  min_delta=0.0001, \n",
        "                                  cooldown=8, \n",
        "                                  min_lr=0)\n",
        "#lrm = LearningRateMonitor()\n",
        " \n",
        "historyCBiLSTM = modelCBiLSTM.fit(x_train_pad, y_train,\n",
        "          batch_size=batchSize,\n",
        "          epochs=epochsNum,\n",
        "          validation_data=(x_test_pad, y_test), verbose = 2 )#, callbacks=[rlrp])\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qIhlM7OZV7Rg"
      },
      "source": [
        "testLossCBiLSTM, testAccCBiLSTM = modelCBiLSTM.evaluate(x_test_pad, y_test, batch_size=batchSize, verbose=1)\n",
        "print('Test loss:', testLossCBiLSTM)\n",
        "print('Test accuracy:', testAccCBiLSTM)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "azSiqhM1WFCc"
      },
      "source": [
        "trainLossCBiLSTM, trainAccCBiLSTM = modelCBiLSTM.evaluate(x_train_pad, y_train, batch_size=batchSize, verbose=1)\n",
        "print('Train loss:', trainLossCBiLSTM)\n",
        "print('Train accuracy:', trainAccCBiLSTM)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hwFdy-bfWMTt"
      },
      "source": [
        "modelCBiLSTM.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y5D13bBIWO7G"
      },
      "source": [
        "plt.title('Accuracy')\n",
        "plt.plot(historyCBiLSTM.history['accuracy'], label='train')\n",
        "plt.plot(historyCBiLSTM.history['val_accuracy'], label='test')\n",
        "plt.legend()\n",
        "plt.show();"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ucEBpXrkWRL8"
      },
      "source": [
        "plt.title('Loss')\n",
        "plt.plot(historyCBiLSTM.history['loss'], label='train')\n",
        "plt.plot(historyCBiLSTM.history['val_loss'], label='test')\n",
        "plt.legend()\n",
        "plt.show();"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EtRbWLCpWW0n"
      },
      "source": [
        "y_val_pred_cbl = modelCBiLSTM.predict_classes(x_test_pad)\n",
        "precisionCBiLSTM, recallCBiLSTM, fscoreCBiLSTM, supportCBiLSTM = score(y_test, y_val_pred_cbl)\n",
        "\n",
        "print('precision: {}'.format(precisionCBiLSTM))\n",
        "print('recall: {}'.format(recallCBiLSTM))\n",
        "print('fscore: {}'.format(fscoreCBiLSTM))\n",
        "print('support: {}'.format(supportCBiLSTM))\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bQ8t2Qpd_tOp"
      },
      "source": [
        "from sklearn.metrics import cohen_kappa_score\n",
        "\n",
        "cohen_score = cohen_kappa_score(y_test, y_val_pred_cbl)\n",
        "\n",
        "print(cohen_score)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WzGRaCoac35R"
      },
      "source": [
        "import seaborn as sns\n",
        "from sklearn.metrics import confusion_matrix\n",
        "labels = [0,1]\n",
        "def cm_analysis(y_true, y_pred, labels, ymap=None, figsize=(10,10)):\n",
        "    \"\"\"\n",
        "    Generate matrix plot of confusion matrix with pretty annotations.\n",
        "    The plot image is saved to disk.\n",
        "    args: \n",
        "      y_true:    true label of the data, with shape (nsamples,)\n",
        "      y_pred:    prediction of the data, with shape (nsamples,)\n",
        "      filename:  filename of figure file to save\n",
        "      labels:    string array, name the order of class labels in the confusion matrix.\n",
        "                 use `clf.classes_` if using scikit-learn models.\n",
        "                 with shape (nclass,).\n",
        "      ymap:      dict: any -> string, length == nclass.\n",
        "                 if not None, map the labels & ys to more understandable strings.\n",
        "                 Caution: original y_true, y_pred and labels must align.\n",
        "      figsize:   the size of the figure plotted.\n",
        "    \"\"\"\n",
        "    if ymap is not None:\n",
        "        y_pred = [ymap[yi] for yi in y_pred]\n",
        "        y_true = [ymap[yi] for yi in y_true]\n",
        "        labels = [ymap[yi] for yi in labels]\n",
        "    cm = confusion_matrix(y_true, y_pred, labels=labels)\n",
        "    cm_sum = np.sum(cm, axis=1, keepdims=True)\n",
        "    cm_perc = cm / cm_sum.astype(float) * 100\n",
        "    annot = np.empty_like(cm).astype(str)\n",
        "    nrows, ncols = cm.shape\n",
        "    for i in range(nrows):\n",
        "        for j in range(ncols):\n",
        "            c = cm[i, j]\n",
        "            p = cm_perc[i, j]\n",
        "            if i == j:\n",
        "                s = cm_sum[i]\n",
        "                annot[i, j] = '%.1f%%\\n%d/%d' % (p, c, s)\n",
        "            elif c == 0:\n",
        "                annot[i, j] = ''\n",
        "            else:\n",
        "                annot[i, j] = '%.1f%%\\n%d' % (p, c)\n",
        "    cm = pd.DataFrame(cm, index=labels, columns=labels)\n",
        "    cm.index.name = 'Actual'\n",
        "    cm.columns.name = 'Predicted'\n",
        "    fig, ax = plt.subplots(figsize=figsize)\n",
        "    sns.heatmap(cm, annot=annot, fmt='', ax=ax)\n",
        "    #plt.savefig(filename)\n",
        "    plt.show()\n",
        "\n",
        "cm_analysis(y_test, y_val_pred_cbl, labels, ymap=None, figsize=(5,5))\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P9fYn6IySB2b"
      },
      "source": [
        "'''\n",
        "print(dfTweetsRelevantTopic.head(3))\n",
        "#testJokerSamples = dfJokerTweets[1]\n",
        "testJokerSamples = dfTweetsRelevantTopic[\"Text\"]\n",
        "testJokerSamples = testJokerSamples.dropna()\n",
        "#print(testSamples[1])\n",
        "tokenizerObj.fit_on_texts(testJokerSamples)\n",
        "testJokerSamplesTokens = tokenizerObj.texts_to_sequences( testJokerSamples )\n",
        "testJokerSamplesTokensPad = pad_sequences( testJokerSamplesTokens, maxlen = maxLen)\n",
        "print(len(testJokerSamplesTokensPad))\n",
        "\n",
        "Xnew = testJokerSamplesTokensPad\n",
        "ynew = modelCBiLSTM.predict_classes(Xnew)\n",
        "print(type(ynew))\n",
        "print(ynew[0])\n",
        "\n",
        "\n",
        "numPositiveCBiLSTM = 0\n",
        "numNegativeCBiLSTM = 0\n",
        "for i in range(0, len(ynew)):\n",
        "  if ynew[i]==0:\n",
        "    numNegativeCBiLSTM = numNegativeCBiLSTM + 1\n",
        "  elif  ynew[i] == 1:\n",
        "    numPositiveCBiLSTM = numPositiveCBiLSTM + 1\n",
        "print('Positive', numPositiveCBiLSTM)\n",
        "print('Negatives', numNegativeCBiLSTM)\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9mlePpnIt0rm"
      },
      "source": [
        "# CNN+LSTM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LFj9y14WezRQ"
      },
      "source": [
        "# Embedding\n",
        "\n",
        "\n",
        "\n",
        "max_features = numWords\n",
        "\n",
        "\n",
        "filtersNum = 500\n",
        "\n",
        "\n",
        "unitsNum = 20\n",
        "\n",
        "\n",
        "# Convolution\n",
        "\n",
        "\n",
        "\n",
        "'''\n",
        "Note:\n",
        "batch_size is highly sensitive.\n",
        "Only 2 epochs are needed as the dataset is very small.\n",
        "'''\n",
        "print('Build model...')\n",
        "\n",
        "modelCL = Sequential(name='CNNLSTM')\n",
        "modelCL.add(Embedding(numWords, embeddingDim , \n",
        "                      embeddings_initializer = Constant( embeddingMatrix), \n",
        "                      input_length=maxLen,\n",
        "                      trainable = isTrainable))\n",
        "\n",
        "\n",
        "modelCL.add(Conv1D(filters= filtersNum, kernel_size= kernelSize, strides= stridesNum, padding='same', activation='relu'))\n",
        "\n",
        "\n",
        "modelCL.add(Dropout(0.5))\n",
        "modelCL.add(MaxPooling1D(pool_size = poolSize))\n",
        "\n",
        "modelCL.add(Dropout(0.5))\n",
        "modelCL.add(LSTM(20, dropout=0.5 ))\n",
        "modelCL.add(BatchNormalization())\n",
        "modelCL.add(Dropout(0.5))\n",
        "\n",
        "\n",
        "\n",
        "modelCL.add(Dense(1,activation='sigmoid'))\n",
        "\n",
        "\n",
        "opt = SGD(lr=0.1)\n",
        "\n",
        "\n",
        "modelCL.compile(loss='binary_crossentropy',\n",
        "              optimizer='adam',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "\n",
        "#modelCL.summary()\n",
        "\n",
        "print('Train...')\n",
        "\n",
        "rlrp = ReduceLROnPlateau(monitor='val_loss', \n",
        "                                  factor=0.1, \n",
        "                                  patience=8, \n",
        "                                  verbose=1, \n",
        "                                  mode='auto', \n",
        "                                  min_delta=0.0001, \n",
        "                                  cooldown=8, \n",
        "                                  min_lr=0)\n",
        "#lrm = LearningRateMonitor()\n",
        " \n",
        "historyCL = modelCL.fit(x_train_pad, y_train,\n",
        "          batch_size=batchSize,\n",
        "          epochs=epochsNum,\n",
        "          validation_data=(x_test_pad, y_test), verbose = 1 )#, callbacks=[rlrp])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qIQjanOVrRpC"
      },
      "source": [
        "testLossCL, testAccCL = modelCL.evaluate(x_test_pad, y_test, batch_size=batchSize, verbose=1)\n",
        "print('Test loss:', testLossCL)\n",
        "print('Test accuracy:', testAccCL)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oS9ZA_i_22Ap"
      },
      "source": [
        "trainLossCL, trainAccCL = modelCL.evaluate(x_train_pad, y_train, batch_size=batchSize, verbose=1)\n",
        "print('Train loss:', trainLossCL)\n",
        "print('Train accuracy:', trainAccCL)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y6pIdwG6Zru-"
      },
      "source": [
        "#modelCL.save(CLModelTxt)\n",
        "#modelCL.save(CLModelH)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vcb5OtVf2cBB"
      },
      "source": [
        "modelCL.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p4aXRgoPSVyH"
      },
      "source": [
        "plt.title('Accuracy')\n",
        "plt.plot(historyCL.history['accuracy'], label='train')\n",
        "plt.plot(historyCL.history['val_accuracy'], label='test')\n",
        "plt.legend()\n",
        "plt.show();"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "llxtTnsWqeFQ"
      },
      "source": [
        "plt.title('Loss')\n",
        "plt.plot(historyCL.history['loss'], label='train')\n",
        "plt.plot(historyCL.history['val_loss'], label='test')\n",
        "plt.legend()\n",
        "plt.show();"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WQIe0TwHaNh9"
      },
      "source": [
        "y_val_pred_cl=modelCL.predict_classes(x_test_pad)\n",
        "precisionCL, recallCL, fscoreCL, supportCL = score(y_test, y_val_pred_cl)\n",
        "\n",
        "print('precision: {}'.format(precisionCL))\n",
        "print('recall: {}'.format(recallCL))\n",
        "print('fscore: {}'.format(fscoreCL))\n",
        "print('support: {}'.format(supportCL))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KzXf1PliAZdI"
      },
      "source": [
        "from sklearn.metrics import cohen_kappa_score\n",
        "\n",
        "cohen_score = cohen_kappa_score(y_test, y_val_pred_cl)\n",
        "\n",
        "print(cohen_score)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "glUpNo4XAiO8"
      },
      "source": [
        "import seaborn as sns\n",
        "from sklearn.metrics import confusion_matrix\n",
        "labels = [0,1]\n",
        "def cm_analysis(y_true, y_pred, labels, ymap=None, figsize=(10,10)):\n",
        "    \"\"\"\n",
        "    Generate matrix plot of confusion matrix with pretty annotations.\n",
        "    The plot image is saved to disk.\n",
        "    args: \n",
        "      y_true:    true label of the data, with shape (nsamples,)\n",
        "      y_pred:    prediction of the data, with shape (nsamples,)\n",
        "      filename:  filename of figure file to save\n",
        "      labels:    string array, name the order of class labels in the confusion matrix.\n",
        "                 use `clf.classes_` if using scikit-learn models.\n",
        "                 with shape (nclass,).\n",
        "      ymap:      dict: any -> string, length == nclass.\n",
        "                 if not None, map the labels & ys to more understandable strings.\n",
        "                 Caution: original y_true, y_pred and labels must align.\n",
        "      figsize:   the size of the figure plotted.\n",
        "    \"\"\"\n",
        "    if ymap is not None:\n",
        "        y_pred = [ymap[yi] for yi in y_pred]\n",
        "        y_true = [ymap[yi] for yi in y_true]\n",
        "        labels = [ymap[yi] for yi in labels]\n",
        "    cm = confusion_matrix(y_true, y_pred, labels=labels)\n",
        "    cm_sum = np.sum(cm, axis=1, keepdims=True)\n",
        "    cm_perc = cm / cm_sum.astype(float) * 100\n",
        "    annot = np.empty_like(cm).astype(str)\n",
        "    nrows, ncols = cm.shape\n",
        "    for i in range(nrows):\n",
        "        for j in range(ncols):\n",
        "            c = cm[i, j]\n",
        "            p = cm_perc[i, j]\n",
        "            if i == j:\n",
        "                s = cm_sum[i]\n",
        "                annot[i, j] = '%.1f%%\\n%d/%d' % (p, c, s)\n",
        "            elif c == 0:\n",
        "                annot[i, j] = ''\n",
        "            else:\n",
        "                annot[i, j] = '%.1f%%\\n%d' % (p, c)\n",
        "    cm = pd.DataFrame(cm, index=labels, columns=labels)\n",
        "    cm.index.name = 'Actual'\n",
        "    cm.columns.name = 'Predicted'\n",
        "    fig, ax = plt.subplots(figsize=figsize)\n",
        "    sns.heatmap(cm, annot=annot, fmt='', ax=ax)\n",
        "    #plt.savefig(filename)\n",
        "    plt.show()\n",
        "\n",
        "cm_analysis(y_test, y_val_pred_cl, labels, ymap=None, figsize=(5,5))\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mdo6H-cTAm5N"
      },
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt     \n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "\n",
        "labels = [0,1]\n",
        "cm = confusion_matrix(y_test, y_val_pred_cl, labels)\n",
        "\n",
        "ax= plt.subplot()\n",
        "sns.heatmap(cm, annot=True, fmt=\"g\", ax = ax,); #annot=True to annotate cells\n",
        "\n",
        "# labels, title and ticks\n",
        "ax.set_xlabel('Predicted labels');\n",
        "ax.set_ylabel('True labels'); \n",
        "ax.set_title('Confusion Matrix'); \n",
        "ax.xaxis.set_ticklabels(labels); \n",
        "ax.yaxis.set_ticklabels(labels);\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2KGN6Huo6S-j"
      },
      "source": [
        "## Predicting Joker"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "71P5W_pJ1rI5"
      },
      "source": [
        "*Here predicting will be done based on one topic*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T7A56kw26WnD"
      },
      "source": [
        "'''\n",
        "print(dfTweetsRelevantTopic.head(3))\n",
        "#testJokerSamples = dfJokerTweets[1]\n",
        "testJokerSamples = dfTweetsRelevantTopic[\"Text\"]\n",
        "testJokerSamples = testJokerSamples.dropna()\n",
        "#print(testSamples[1])\n",
        "tokenizerObj.fit_on_texts(testJokerSamples)\n",
        "testJokerSamplesTokens = tokenizerObj.texts_to_sequences( testJokerSamples )\n",
        "testJokerSamplesTokensPad = pad_sequences( testJokerSamplesTokens, maxlen = maxLen)\n",
        "print(len(testJokerSamplesTokensPad))\n",
        "\n",
        "Xnew = testJokerSamplesTokensPad\n",
        "ynew = modelCL.predict_classes(Xnew)\n",
        "print(type(ynew))\n",
        "print(ynew[0])\n",
        "\n",
        "\n",
        "numPositiveCL = 0\n",
        "numNegativeCL = 0\n",
        "for i in range(0, len(ynew)):\n",
        "  if ynew[i]==0:\n",
        "    numNegativeCL = numNegativeCL + 1\n",
        "  elif  ynew[i] == 1:\n",
        "    numPositiveCL = numPositiveCL + 1\n",
        "print('Positive', numPositiveCL)\n",
        "print('Negatives', numNegativeCL)\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u3vqpI5_4aaH"
      },
      "source": [
        "# LSTM"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OK4wsRQg4QDa"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "10qSvJ2E4gmV"
      },
      "source": [
        "max_features = numWords\n",
        "\n",
        "# LSTM\n",
        "lstm_output_size = 70\n",
        "\n",
        "unitsNum = 10\n",
        "batchSize = 32\n",
        "\n",
        "\n",
        "\n",
        "opt = SGD(lr=0.01, momentum=0.9)\n",
        "#\n",
        "print('Build model...')\n",
        "modelLSTM = Sequential()\n",
        "\n",
        "\n",
        "\n",
        "lstm_out = 196\n",
        "\n",
        "\n",
        "embeddingLayer = Embedding( numWords,\n",
        "                           embeddingDim,\n",
        "                           embeddings_initializer = Constant( embeddingMatrix),\n",
        "                           input_length = maxLen,\n",
        "                           trainable = isTrainable)\n",
        "modelLSTM.add(embeddingLayer)\n",
        "modelLSTM.add(Dropout(0.5))\n",
        "modelLSTM.add(LSTM(unitsNum, dropout=0.5))\n",
        "\n",
        "modelLSTM.add(Dropout(0.5))\n",
        "\n",
        "modelLSTM.add(Dense(1,activation='sigmoid'))\n",
        "#modelLSTM.compile(loss = 'categorical_crossentropy', optimizer='adam',metrics = ['accuracy'])\n",
        "\n",
        "opt = SGD(lr=0.1)\n",
        "modelLSTM.compile(loss='binary_crossentropy',optimizer=opt, metrics=['accuracy'])\n",
        "\n",
        "\n",
        "\n",
        "#modelLSTM.summary()\n",
        "\n",
        "rlrp = ReduceLROnPlateau(monitor='val_loss', \n",
        "                                  factor=0.1, \n",
        "                                  patience=8, \n",
        "                                  verbose=1, \n",
        "                                  mode='auto', \n",
        "                                  min_delta=0.0001, \n",
        "                                  cooldown=8, \n",
        "                                  min_lr=0)\n",
        "\n",
        "historyLSTM = modelLSTM.fit(x_train_pad, y_train,\n",
        "          batch_size=batchSize,\n",
        "          epochs=epochsNum,\n",
        "          validation_data=(x_test_pad, y_test),  verbose = 1)#, callbacks=[rlrp])\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CWXJ291CrWSu"
      },
      "source": [
        "testLossLSTM, testAccLSTM = modelLSTM.evaluate(x_test_pad, y_test,\n",
        "                            batch_size=batchSize)\n",
        "print('Test loss:', testLossLSTM)\n",
        "print('Test accuracy:', testAccLSTM)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d7kwIayU4Cep"
      },
      "source": [
        "trainLossLSTM, trainAccLSTM = modelLSTM.evaluate(x_train_pad, y_train, batch_size=batchSize, verbose=1)\n",
        "print('Train loss:', trainLossLSTM)\n",
        "print('Train accuracy:', trainAccLSTM)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jRTpj3OA4xV1"
      },
      "source": [
        "#modelLSTM.save(lstmModelTxt)\n",
        "#modelLSTM.save(lstmModelH)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aBWVGhXl4yxU"
      },
      "source": [
        "modelLSTM.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p-2rj9vr41Sv"
      },
      "source": [
        "plt.title('Accuracy')\n",
        "plt.plot(historyLSTM.history['accuracy'], label='train')\n",
        "plt.plot(historyLSTM.history['val_accuracy'], label='test')\n",
        "plt.legend()\n",
        "plt.show();"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YOWgLfwv47s7"
      },
      "source": [
        "plt.title('Loss')\n",
        "plt.plot(historyLSTM.history['loss'], label='train')\n",
        "plt.plot(historyLSTM.history['val_loss'], label='test')\n",
        "plt.legend()\n",
        "plt.show();"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oTtwEsm94_Zo"
      },
      "source": [
        "y_val_pred_l=modelLSTM.predict_classes(x_test_pad)\n",
        "precisionLSTM, recallLSTM, fscoreLSTM, supportLSTM = score(y_test, y_val_pred_l)\n",
        "\n",
        "print('precision: {}'.format(precisionLSTM))\n",
        "print('recall: {}'.format(recallLSTM))\n",
        "print('fscore: {}'.format(fscoreLSTM))\n",
        "print('support: {}'.format(supportLSTM))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RiBDb-2fA56F"
      },
      "source": [
        "from sklearn.metrics import cohen_kappa_score\n",
        "\n",
        "cohen_score = cohen_kappa_score(y_test, y_val_pred_l)\n",
        "\n",
        "print(cohen_score)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ORe42WjCA63X"
      },
      "source": [
        "import seaborn as sns\n",
        "from sklearn.metrics import confusion_matrix\n",
        "labels = [0,1]\n",
        "def cm_analysis(y_true, y_pred, labels, ymap=None, figsize=(10,10)):\n",
        "    \"\"\"\n",
        "    Generate matrix plot of confusion matrix with pretty annotations.\n",
        "    The plot image is saved to disk.\n",
        "    args: \n",
        "      y_true:    true label of the data, with shape (nsamples,)\n",
        "      y_pred:    prediction of the data, with shape (nsamples,)\n",
        "      filename:  filename of figure file to save\n",
        "      labels:    string array, name the order of class labels in the confusion matrix.\n",
        "                 use `clf.classes_` if using scikit-learn models.\n",
        "                 with shape (nclass,).\n",
        "      ymap:      dict: any -> string, length == nclass.\n",
        "                 if not None, map the labels & ys to more understandable strings.\n",
        "                 Caution: original y_true, y_pred and labels must align.\n",
        "      figsize:   the size of the figure plotted.\n",
        "    \"\"\"\n",
        "    if ymap is not None:\n",
        "        y_pred = [ymap[yi] for yi in y_pred]\n",
        "        y_true = [ymap[yi] for yi in y_true]\n",
        "        labels = [ymap[yi] for yi in labels]\n",
        "    cm = confusion_matrix(y_true, y_pred, labels=labels)\n",
        "    cm_sum = np.sum(cm, axis=1, keepdims=True)\n",
        "    cm_perc = cm / cm_sum.astype(float) * 100\n",
        "    annot = np.empty_like(cm).astype(str)\n",
        "    nrows, ncols = cm.shape\n",
        "    for i in range(nrows):\n",
        "        for j in range(ncols):\n",
        "            c = cm[i, j]\n",
        "            p = cm_perc[i, j]\n",
        "            if i == j:\n",
        "                s = cm_sum[i]\n",
        "                annot[i, j] = '%.1f%%\\n%d/%d' % (p, c, s)\n",
        "            elif c == 0:\n",
        "                annot[i, j] = ''\n",
        "            else:\n",
        "                annot[i, j] = '%.1f%%\\n%d' % (p, c)\n",
        "    cm = pd.DataFrame(cm, index=labels, columns=labels)\n",
        "    cm.index.name = 'Actual'\n",
        "    cm.columns.name = 'Predicted'\n",
        "    #fig, ax = plt.subplots(figsize=figsize)\n",
        "    fig, ax = plt.subplots()\n",
        "    sns.heatmap(cm, annot=annot, fmt='', ax=ax)\n",
        "    #plt.savefig(filename)\n",
        "    plt.show()\n",
        "\n",
        "cm_analysis(y_test, y_val_pred_l, labels, ymap=None, figsize=(5,5))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3P7Q2hHXA3KQ"
      },
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt     \n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "\n",
        "labels = [0,1]\n",
        "cm = confusion_matrix(y_test, y_val_pred_l, labels)\n",
        "\n",
        "ax= plt.subplot()\n",
        "sns.heatmap(cm, annot=True, fmt=\"g\", ax = ax,); #annot=True to annotate cells\n",
        "\n",
        "# labels, title and ticks\n",
        "ax.set_xlabel('Predicted labels');\n",
        "ax.set_ylabel('True labels'); \n",
        "ax.set_title('Confusion Matrix'); \n",
        "ax.xaxis.set_ticklabels(labels); \n",
        "ax.yaxis.set_ticklabels(labels);\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OWyW35CU5DC2"
      },
      "source": [
        "'''\n",
        "print(dfTweetsRelevantTopic.head(3))\n",
        "testJokerSamples = dfTweetsRelevantTopic[\"Text\"]\n",
        "testJokerSamples = testJokerSamples.dropna()\n",
        "#print(testSamples[1])\n",
        "tokenizerObj.fit_on_texts(testJokerSamples)\n",
        "testJokerSamplesTokens = tokenizerObj.texts_to_sequences( testJokerSamples )\n",
        "testJokerSamplesTokensPad = pad_sequences( testJokerSamplesTokens, maxlen = maxLen)\n",
        "print(len(testJokerSamplesTokensPad))\n",
        "\n",
        "Xnew = testJokerSamplesTokensPad\n",
        "ynew = modelLSTM.predict_classes(Xnew)\n",
        "print(type(ynew))\n",
        "print(ynew[0])\n",
        "\n",
        "\n",
        "numPositiveLSTM = 0\n",
        "numNegativeLSTM = 0\n",
        "for i in range(0, len(ynew)):\n",
        "  if ynew[i]==0:\n",
        "    numNegativeLSTM = numNegativeLSTM + 1\n",
        "  elif  ynew[i] == 1:\n",
        "    numPositiveLSTM = numPositiveLSTM + 1\n",
        "print('Positive', numPositiveLSTM)\n",
        "print('Negatives', numNegativeLSTM)\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "byUw4FUoJeWg"
      },
      "source": [
        "# GRU"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BGfuOfCpJg9c"
      },
      "source": [
        "print('Build model...')\n",
        "\n",
        "\n",
        "modelGRU = Sequential()\n",
        "\n",
        "# we start off with an efficient embedding layer which maps\n",
        "# our vocab indices into embedding_dims dimensions\n",
        "embeddingLayer = Embedding( numWords,\n",
        "                           embeddingDim,\n",
        "                           embeddings_initializer = Constant( embeddingMatrix),\n",
        "                           input_length = maxLen,\n",
        "                           trainable = isTrainable)\n",
        "\n",
        "modelGRU.add(embeddingLayer)\n",
        "\n",
        "modelGRU.add(Dropout(0.5))\n",
        "modelGRU.add(GRU(units= 128 , dropout = 0.5))\n",
        "\n",
        "modelGRU.add(Dropout(0.5))\n",
        "modelGRU.add(Dense(1,activation='sigmoid'))\n",
        "\n",
        "\n",
        "opt = SGD(lr=0.1)\n",
        "\n",
        "modelGRU.compile(loss='binary_crossentropy',\n",
        "              optimizer= opt,\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "modelGRU.summary()\n",
        "\n",
        "\n",
        "rlrp = ReduceLROnPlateau(monitor='val_loss', \n",
        "                                  factor=0.1, \n",
        "                                  patience=8, \n",
        "                                  verbose=1, \n",
        "                                  mode='auto', \n",
        "                                  min_delta=0.0001, \n",
        "                                  cooldown=8, \n",
        "                                  min_lr=0)\n",
        "\n",
        "historyGRU = modelGRU.fit(x_train_pad, y_train,\n",
        "          batch_size=batchSize,\n",
        "          epochs=100,\n",
        "          validation_data=(x_test_pad, y_test),\n",
        "          verbose = 1)  # , callbacks=[rlrp])\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SiejgZLCra1u"
      },
      "source": [
        "testLossGRU, testAccGRU = modelGRU.evaluate(x_test_pad, y_test,\n",
        "                            batch_size=batchSize)\n",
        "print('Test loss:', testLossGRU)\n",
        "print('Test accuracy:', testAccGRU)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "10AUJp9b6snd"
      },
      "source": [
        "trainLossGRU, trainAccGRU = modelGRU.evaluate(x_train_pad, y_train,\n",
        "                            batch_size=batchSize)\n",
        "print('Train loss:', trainLossGRU)\n",
        "print('Train accuracy:', trainAccGRU)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5YBithYlJz-A"
      },
      "source": [
        "#modelGRU.save(gruModelTxt)\n",
        "#modelGRU.save(gruModelH)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u2nIq98HLy77"
      },
      "source": [
        "modelGRU.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zFb4PMFoL1Ov"
      },
      "source": [
        "plt.title('Accuracy')\n",
        "plt.plot(historyGRU.history['accuracy'], label='train')\n",
        "plt.plot(historyGRU.history['val_accuracy'], label='test')\n",
        "plt.legend()\n",
        "plt.show();"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WUVeBh-jL36o"
      },
      "source": [
        "plt.title('Loss')\n",
        "plt.plot(historyGRU.history['loss'], label='train')\n",
        "plt.plot(historyGRU.history['val_loss'], label='test')\n",
        "plt.legend()\n",
        "plt.show();"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6VwMh0jZMDZY"
      },
      "source": [
        "from sklearn.metrics import precision_recall_fscore_support as score\n",
        "\n",
        "\n",
        "y_val_pred_g=modelGRU.predict_classes(x_test_pad)\n",
        "precisionGRU, recallGRU, fscoreGRU, supportGRU = score(y_test, y_val_pred_g)\n",
        "\n",
        "print('precision: {}'.format(precisionGRU))\n",
        "print('recall: {}'.format(recallGRU))\n",
        "print('fscore: {}'.format(fscoreGRU))\n",
        "print('support: {}'.format(supportGRU))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L-MGBTQ7BVIn"
      },
      "source": [
        "from sklearn.metrics import cohen_kappa_score\n",
        "\n",
        "cohen_score = cohen_kappa_score(y_test, y_val_pred_g)\n",
        "\n",
        "print(cohen_score)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "84gjiNzCBVXC"
      },
      "source": [
        "import seaborn as sns\n",
        "from sklearn.metrics import confusion_matrix\n",
        "labels = [0,1]\n",
        "def cm_analysis(y_true, y_pred, labels, ymap=None, figsize=(10,10)):\n",
        "    \"\"\"\n",
        "    Generate matrix plot of confusion matrix with pretty annotations.\n",
        "    The plot image is saved to disk.\n",
        "    args: \n",
        "      y_true:    true label of the data, with shape (nsamples,)\n",
        "      y_pred:    prediction of the data, with shape (nsamples,)\n",
        "      filename:  filename of figure file to save\n",
        "      labels:    string array, name the order of class labels in the confusion matrix.\n",
        "                 use `clf.classes_` if using scikit-learn models.\n",
        "                 with shape (nclass,).\n",
        "      ymap:      dict: any -> string, length == nclass.\n",
        "                 if not None, map the labels & ys to more understandable strings.\n",
        "                 Caution: original y_true, y_pred and labels must align.\n",
        "      figsize:   the size of the figure plotted.\n",
        "    \"\"\"\n",
        "    if ymap is not None:\n",
        "        y_pred = [ymap[yi] for yi in y_pred]\n",
        "        y_true = [ymap[yi] for yi in y_true]\n",
        "        labels = [ymap[yi] for yi in labels]\n",
        "    cm = confusion_matrix(y_true, y_pred, labels=labels)\n",
        "    cm_sum = np.sum(cm, axis=1, keepdims=True)\n",
        "    cm_perc = cm / cm_sum.astype(float) * 100\n",
        "    annot = np.empty_like(cm).astype(str)\n",
        "    nrows, ncols = cm.shape\n",
        "    for i in range(nrows):\n",
        "        for j in range(ncols):\n",
        "            c = cm[i, j]\n",
        "            p = cm_perc[i, j]\n",
        "            if i == j:\n",
        "                s = cm_sum[i]\n",
        "                annot[i, j] = '%.1f%%\\n%d/%d' % (p, c, s)\n",
        "            elif c == 0:\n",
        "                annot[i, j] = ''\n",
        "            else:\n",
        "                annot[i, j] = '%.1f%%\\n%d' % (p, c)\n",
        "    cm = pd.DataFrame(cm, index=labels, columns=labels)\n",
        "    cm.index.name = 'Actual'\n",
        "    cm.columns.name = 'Predicted'\n",
        "    fig, ax = plt.subplots()\n",
        "    sns.heatmap(cm, annot=annot, fmt='', ax=ax)\n",
        "    #plt.savefig(filename)\n",
        "    plt.show()\n",
        "\n",
        "cm_analysis(y_test, y_val_pred_g, labels, ymap=None, figsize=(5,5))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mF2uQxpQBVok"
      },
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt     \n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "\n",
        "labels = [0,1]\n",
        "cm = confusion_matrix(y_test, y_val_pred_g, labels)\n",
        "\n",
        "ax= plt.subplot()\n",
        "sns.heatmap(cm, annot=True, fmt=\"g\", ax = ax,); #annot=True to annotate cells\n",
        "\n",
        "# labels, title and ticks\n",
        "ax.set_xlabel('Predicted labels');\n",
        "ax.set_ylabel('True labels'); \n",
        "ax.set_title('Confusion Matrix'); \n",
        "ax.xaxis.set_ticklabels(labels); \n",
        "ax.yaxis.set_ticklabels(labels);\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZE1_VRM2fnXa"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HV0iiCaifvJW"
      },
      "source": [
        "# BIGRU"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6EnvQewffvJd"
      },
      "source": [
        "print('Build model...')\n",
        "\n",
        "\n",
        "modelBIGRU = Sequential()\n",
        "\n",
        "# we start off with an efficient embedding layer which maps\n",
        "# our vocab indices into embedding_dims dimensions\n",
        "embeddingLayer = Embedding( numWords,\n",
        "                           embeddingDim,\n",
        "                           embeddings_initializer = Constant( embeddingMatrix),\n",
        "                           input_length = maxLen,\n",
        "                           trainable = isTrainable)\n",
        "\n",
        "modelBIGRU.add(embeddingLayer)\n",
        "modelBIGRU.add(Dropout(0.5))\n",
        "modelBIGRU.add(Bidirectional(GRU(units=20, dropout = 0.5)))\n",
        "modelBIGRU.add(BatchNormalization())\n",
        "\n",
        "modelBIGRU.add(Dropout(0.5))\n",
        "modelBIGRU.add(Dense(1,activation='sigmoid'))\n",
        "\n",
        "\n",
        "\n",
        "modelBIGRU.compile(loss='binary_crossentropy',\n",
        "              optimizer= 'adam',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "\n",
        "historyBIGRU = modelBIGRU.fit(x_train_pad, y_train,\n",
        "          batch_size=batchSize,\n",
        "          epochs=epochsNum,\n",
        "          validation_data=(x_test_pad, y_test),\n",
        "          verbose = 1)  # , callbacks=[rlrp])\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gMELg0s7fvJ6"
      },
      "source": [
        "testLossBIGRU, testAccBIGRU = modelBIGRU.evaluate(x_test_pad, y_test,\n",
        "                            batch_size=batchSize)\n",
        "print('Test loss:', testLossBIGRU)\n",
        "print('Test accuracy:', testAccBIGRU)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IP6OaOiZfvKI"
      },
      "source": [
        "trainLossBIGRU, trainAccBIGRU = modelBIGRU.evaluate(x_train_pad, y_train,\n",
        "                            batch_size=batchSize)\n",
        "print('Train loss:', trainLossBIGRU)\n",
        "print('Train accuracy:', trainAccBIGRU)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vJxSN5W0fvKk"
      },
      "source": [
        "modelBIGRU.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LkfTedkRfvKy"
      },
      "source": [
        "plt.title('Accuracy')\n",
        "plt.plot(historyBIGRU.history['accuracy'], label='train')\n",
        "plt.plot(historyBIGRU.history['val_accuracy'], label='test')\n",
        "plt.legend()\n",
        "plt.show();"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IzQ23MPTfvK_"
      },
      "source": [
        "plt.title('Loss')\n",
        "plt.plot(historyBIGRU.history['loss'], label='train')\n",
        "plt.plot(historyBIGRU.history['val_loss'], label='test')\n",
        "plt.legend()\n",
        "plt.show();"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aev6iexQfvLM"
      },
      "source": [
        "from sklearn.metrics import precision_recall_fscore_support as score\n",
        "\n",
        "\n",
        "y_val_pred_bg=modelBIGRU.predict_classes(x_test_pad)\n",
        "precisionBIGRU, recallBIGRU, fscoreBIGRU, supportBIGRU = score(y_test, y_val_pred_bg)\n",
        "\n",
        "print('precision: {}'.format(precisionBIGRU))\n",
        "print('recall: {}'.format(recallBIGRU))\n",
        "print('fscore: {}'.format(fscoreBIGRU))\n",
        "print('support: {}'.format(supportBIGRU))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xCk3y2wMCYIF"
      },
      "source": [
        "from sklearn.metrics import cohen_kappa_score\n",
        "\n",
        "cohen_score = cohen_kappa_score(y_test, y_val_pred_bg)\n",
        "\n",
        "print(cohen_score)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qf6RgtpuCYbz"
      },
      "source": [
        "import seaborn as sns\n",
        "from sklearn.metrics import confusion_matrix\n",
        "labels = [0,1]\n",
        "def cm_analysis(y_true, y_pred, labels, ymap=None, figsize=(10,10)):\n",
        "    \"\"\"\n",
        "    Generate matrix plot of confusion matrix with pretty annotations.\n",
        "    The plot image is saved to disk.\n",
        "    args: \n",
        "      y_true:    true label of the data, with shape (nsamples,)\n",
        "      y_pred:    prediction of the data, with shape (nsamples,)\n",
        "      filename:  filename of figure file to save\n",
        "      labels:    string array, name the order of class labels in the confusion matrix.\n",
        "                 use `clf.classes_` if using scikit-learn models.\n",
        "                 with shape (nclass,).\n",
        "      ymap:      dict: any -> string, length == nclass.\n",
        "                 if not None, map the labels & ys to more understandable strings.\n",
        "                 Caution: original y_true, y_pred and labels must align.\n",
        "      figsize:   the size of the figure plotted.\n",
        "    \"\"\"\n",
        "    if ymap is not None:\n",
        "        y_pred = [ymap[yi] for yi in y_pred]\n",
        "        y_true = [ymap[yi] for yi in y_true]\n",
        "        labels = [ymap[yi] for yi in labels]\n",
        "    cm = confusion_matrix(y_true, y_pred, labels=labels)\n",
        "    cm_sum = np.sum(cm, axis=1, keepdims=True)\n",
        "    cm_perc = cm / cm_sum.astype(float) * 100\n",
        "    annot = np.empty_like(cm).astype(str)\n",
        "    nrows, ncols = cm.shape\n",
        "    for i in range(nrows):\n",
        "        for j in range(ncols):\n",
        "            c = cm[i, j]\n",
        "            p = cm_perc[i, j]\n",
        "            if i == j:\n",
        "                s = cm_sum[i]\n",
        "                annot[i, j] = '%.1f%%\\n%d/%d' % (p, c, s)\n",
        "            elif c == 0:\n",
        "                annot[i, j] = ''\n",
        "            else:\n",
        "                annot[i, j] = '%.1f%%\\n%d' % (p, c)\n",
        "    cm = pd.DataFrame(cm, index=labels, columns=labels)\n",
        "    cm.index.name = 'Actual'\n",
        "    cm.columns.name = 'Predicted'\n",
        "    fig, ax = plt.subplots()\n",
        "    sns.heatmap(cm, annot=annot, fmt='', ax=ax)\n",
        "    #plt.savefig(filename)\n",
        "    plt.show()\n",
        "\n",
        "cm_analysis(y_test, y_val_pred_bg, labels, ymap=None, figsize=(5,5))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "20wN-fykCYEO"
      },
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt     \n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "\n",
        "labels = [0,1]\n",
        "cm = confusion_matrix(y_test, y_val_pred_bg, labels)\n",
        "\n",
        "ax= plt.subplot()\n",
        "sns.heatmap(cm, annot=True, fmt=\"g\", ax = ax,); #annot=True to annotate cells\n",
        "\n",
        "# labels, title and ticks\n",
        "ax.set_xlabel('Predicted labels');\n",
        "ax.set_ylabel('True labels'); \n",
        "ax.set_title('Confusion Matrix'); \n",
        "ax.xaxis.set_ticklabels(labels); \n",
        "ax.yaxis.set_ticklabels(labels);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ksnjegLgfvLc"
      },
      "source": [
        "## Predicting Joker"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YL56wiQMfvLf"
      },
      "source": [
        "'''\n",
        "print(dfTweetsRelevantTopic.head(3))\n",
        "testJokerSamples = dfTweetsRelevantTopic[\"Text\"]\n",
        "testJokerSamples = testJokerSamples.dropna()\n",
        "#print(testSamples[1])\n",
        "tokenizerObj.fit_on_texts(testJokerSamples)\n",
        "testJokerSamplesTokens = tokenizerObj.texts_to_sequences( testJokerSamples )\n",
        "testJokerSamplesTokensPad = pad_sequences( testJokerSamplesTokens, maxlen = maxLen)\n",
        "print(len(testJokerSamplesTokensPad))\n",
        "\n",
        "Xnew = testJokerSamplesTokensPad\n",
        "ynew = modelBIGRU.predict_classes(Xnew)\n",
        "print(type(ynew))\n",
        "print(ynew[0])\n",
        "\n",
        "\n",
        "numPositiveBIGRU = 0\n",
        "numNegativeBIGRU = 0\n",
        "for i in range(0, len(ynew)):\n",
        "  if ynew[i]==0:\n",
        "    numNegativeBIGRU = numNegativeBIGRU + 1\n",
        "  elif  ynew[i] == 1:\n",
        "    numPositiveBIGRU = numPositiveBIGRU + 1\n",
        "print('Positive', numPositiveBIGRU )\n",
        "print('Negatives', numNegativeBIGRU )\n",
        "'''\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W8v3DRX0MPJF"
      },
      "source": [
        "# BILSTM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ridZiLWYMa23"
      },
      "source": [
        "maxFeatures = numWords\n",
        "\n",
        "unitsNumber = 20\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "opt = SGD(lr=0.01, momentum=0.9)\n",
        "#model.compile(..., optimizer=opt)\n",
        "print('Build model...')\n",
        "modelBiLSTM = Sequential()\n",
        "\n",
        "# we start off with an efficient embedding layer which maps\n",
        "# our vocab indices into embedding_dims dimensions\n",
        "embeddingLayer = Embedding( numWords,\n",
        "                           embeddingDim,\n",
        "                           embeddings_initializer = Constant( embeddingMatrix),\n",
        "                           input_length = maxLen,\n",
        "                           trainable = isTrainable)\n",
        "\n",
        "modelBiLSTM.add(embeddingLayer)\n",
        "\n",
        "modelBiLSTM.add(Dropout(0.5))\n",
        "\n",
        "modelBiLSTM.add(Bidirectional(LSTM(unitsNum, dropout = 0.5)))\n",
        "\n",
        "modelBiLSTM.add(Dropout(0.5))\n",
        "\n",
        "modelBiLSTM.add(Dense(1,activation='sigmoid'))\n",
        "\n",
        "modelBiLSTM.compile(loss='binary_crossentropy',optimizer= opt, metrics=['accuracy'])\n",
        "\n",
        "\n",
        "#modelBiLSTM.summary()\n",
        "\n",
        "rlrp = ReduceLROnPlateau(monitor='val_loss', \n",
        "                                  factor=0.1, \n",
        "                                  patience=8, \n",
        "                                  verbose=1, \n",
        "                                  mode='auto', \n",
        "                                  min_delta=0.0001, \n",
        "                                  cooldown=8, \n",
        "                                  min_lr=0)\n",
        "\n",
        "historyBILSTM = modelBiLSTM.fit(x_train_pad, y_train,\n",
        "          batch_size=batchSize,\n",
        "          epochs=epochsNum,\n",
        "          validation_data=(x_test_pad, y_test),\n",
        "          verbose = 1 )#, callbacks=[rlrp])\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h8d7UL68r9e-"
      },
      "source": [
        "testLossBilstm, testAccBilstm = modelBiLSTM.evaluate(x_test_pad, y_test,\n",
        "                            batch_size=batchSize)\n",
        "print('Test score:', testLossBilstm)\n",
        "print('Test accuracy:', testAccBilstm)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZKj71OeY8eI4"
      },
      "source": [
        "trainLossBilstm, trainAccBilstm = modelBiLSTM.evaluate(x_train_pad, y_train,\n",
        "                            batch_size=batchSize)\n",
        "print('Train score:', trainLossBilstm)\n",
        "print('Train accuracy:', trainAccBilstm)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZGWWmQAvOAC1"
      },
      "source": [
        "#modelBiLSTM.save(bilstmModelTxt)\n",
        "#modelBiLSTM.save(bilstmModelH)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kugkgIgzPzQF"
      },
      "source": [
        "modelBiLSTM.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "13uhFiPWP0Dt"
      },
      "source": [
        "plt.title('Accuracy')\n",
        "plt.plot(historyBILSTM.history['accuracy'], label='train')\n",
        "plt.plot(historyBILSTM.history['val_accuracy'], label='test')\n",
        "plt.legend()\n",
        "plt.show();"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vh9R4pXGP28a"
      },
      "source": [
        "plt.title('Loss')\n",
        "plt.plot(historyBILSTM.history['loss'], label='train')\n",
        "plt.plot(historyBILSTM.history['val_loss'], label='test')\n",
        "plt.legend()\n",
        "plt.show();"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pZX8sUAbP7Bk"
      },
      "source": [
        "y_val_pred_bl=modelBiLSTM.predict_classes(x_test_pad)\n",
        "precisionBilstm, recallBilstm, fscoreBilstm, supportBilstm = score(y_test, y_val_pred_bl)\n",
        "\n",
        "print('precision: {}'.format(precisionBilstm))\n",
        "print('recall: {}'.format(recallBilstm))\n",
        "print('fscore: {}'.format(fscoreBilstm))\n",
        "print('support: {}'.format(supportBilstm))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BRUtpsvCDhI5"
      },
      "source": [
        "from sklearn.metrics import cohen_kappa_score\n",
        "\n",
        "cohen_score = cohen_kappa_score(y_test, y_val_pred_bl)\n",
        "\n",
        "print(cohen_score)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "um7CgINLDsJl"
      },
      "source": [
        "import seaborn as sns\n",
        "from sklearn.metrics import confusion_matrix\n",
        "labels = [0,1]\n",
        "def cm_analysis(y_true, y_pred, labels, ymap=None, figsize=(10,10)):\n",
        "    \"\"\"\n",
        "    Generate matrix plot of confusion matrix with pretty annotations.\n",
        "    The plot image is saved to disk.\n",
        "    args: \n",
        "      y_true:    true label of the data, with shape (nsamples,)\n",
        "      y_pred:    prediction of the data, with shape (nsamples,)\n",
        "      filename:  filename of figure file to save\n",
        "      labels:    string array, name the order of class labels in the confusion matrix.\n",
        "                 use `clf.classes_` if using scikit-learn models.\n",
        "                 with shape (nclass,).\n",
        "      ymap:      dict: any -> string, length == nclass.\n",
        "                 if not None, map the labels & ys to more understandable strings.\n",
        "                 Caution: original y_true, y_pred and labels must align.\n",
        "      figsize:   the size of the figure plotted.\n",
        "    \"\"\"\n",
        "    if ymap is not None:\n",
        "        y_pred = [ymap[yi] for yi in y_pred]\n",
        "        y_true = [ymap[yi] for yi in y_true]\n",
        "        labels = [ymap[yi] for yi in labels]\n",
        "    cm = confusion_matrix(y_true, y_pred, labels=labels)\n",
        "    cm_sum = np.sum(cm, axis=1, keepdims=True)\n",
        "    cm_perc = cm / cm_sum.astype(float) * 100\n",
        "    annot = np.empty_like(cm).astype(str)\n",
        "    nrows, ncols = cm.shape\n",
        "    for i in range(nrows):\n",
        "        for j in range(ncols):\n",
        "            c = cm[i, j]\n",
        "            p = cm_perc[i, j]\n",
        "            if i == j:\n",
        "                s = cm_sum[i]\n",
        "                annot[i, j] = '%.1f%%\\n%d/%d' % (p, c, s)\n",
        "            elif c == 0:\n",
        "                annot[i, j] = ''\n",
        "            else:\n",
        "                annot[i, j] = '%.1f%%\\n%d' % (p, c)\n",
        "    cm = pd.DataFrame(cm, index=labels, columns=labels)\n",
        "    cm.index.name = 'Actual'\n",
        "    cm.columns.name = 'Predicted'\n",
        "    fig, ax = plt.subplots()\n",
        "    sns.heatmap(cm, annot=annot, fmt='', ax=ax)\n",
        "    #plt.savefig(filename)\n",
        "    plt.show()\n",
        "\n",
        "cm_analysis(y_test, y_val_pred_bl, labels, ymap=None, figsize=(5,5))\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0qp6BYLSDhVT"
      },
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt     \n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "\n",
        "labels = [0,1]\n",
        "cm = confusion_matrix(y_test, y_val_pred_bl, labels)\n",
        "\n",
        "ax= plt.subplot()\n",
        "sns.heatmap(cm, annot=True, fmt=\"g\", ax = ax,); #annot=True to annotate cells\n",
        "\n",
        "# labels, title and ticks\n",
        "ax.set_xlabel('Predicted labels');\n",
        "ax.set_ylabel('True labels'); \n",
        "ax.set_title('Confusion Matrix'); \n",
        "ax.xaxis.set_ticklabels(labels); \n",
        "ax.yaxis.set_ticklabels(labels);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jMpyhxARPt5B"
      },
      "source": [
        "## Predicting Joker"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t2BINqFRPwpL"
      },
      "source": [
        "'''\n",
        "print(dfTweetsRelevantTopic.head(3))\n",
        "testJokerSamples = dfTweetsRelevantTopic[\"Text\"]\n",
        "\n",
        "\n",
        "testJokerSamples = testJokerSamples.dropna()\n",
        "#print(testSamples[1])\n",
        "tokenizerObj.fit_on_texts(testJokerSamples)\n",
        "testJokerSamplesTokens = tokenizerObj.texts_to_sequences( testJokerSamples )\n",
        "testJokerSamplesTokensPad = pad_sequences( testJokerSamplesTokens, maxlen = maxLen)\n",
        "print(len(testJokerSamplesTokensPad))\n",
        "\n",
        "Xnew = testJokerSamplesTokensPad\n",
        "ynew = modelBiLSTM.predict_classes(Xnew)\n",
        "print(type(ynew))\n",
        "print(ynew[0])\n",
        "\n",
        "\n",
        "numPositiveBilstm = 0\n",
        "numNegativeBilstm = 0\n",
        "for i in range(0, len(ynew)):\n",
        "  if ynew[i]==0:\n",
        "    numNegativeBilstm = numNegativeBilstm + 1\n",
        "  elif  ynew[i] == 1:\n",
        "    numPositiveBilstm = numPositiveBilstm + 1\n",
        "print('Positive', numPositiveBilstm)\n",
        "print('Negatives', numNegativeBilstm)\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6gqt70t799a9"
      },
      "source": [
        "# CNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P38k_J2BrCRN"
      },
      "source": [
        "\n",
        "\n",
        "# set parameters:\n",
        "max_features = numWords\n",
        "#maxlen = 400\n",
        "\n",
        "\n",
        "print('Build model...')\n",
        "modelConv1 = Sequential()\n",
        "\n",
        "# we start off with an efficient embedding layer which maps\n",
        "# our vocab indices into embedding_dims dimensions\n",
        "modelConv1.add(Embedding(numWords, embeddingDim , \n",
        "                      embeddings_initializer = Constant( embeddingMatrix), \n",
        "                      input_length=maxLen,\n",
        "                      trainable = isTrainable))\n",
        "\n",
        "# we add a Convolution1D, which will learn filters\n",
        "# word group filters of size filter_length:\n",
        "\n",
        "modelConv1.add(Conv1D(filters=filtersNum,\n",
        "                 kernel_size=kernelSize,\n",
        "                 padding='same',\n",
        "                 activation='relu',\n",
        "                 strides=1))\n",
        "modelConv1.add(Dropout(0.5))\n",
        "\n",
        "modelConv1.add(MaxPooling1D(pool_size=poolSize))\n",
        "modelConv1.add(Dropout(0.5))\n",
        "\n",
        "modelConv1.add(Flatten())\n",
        "modelConv1.add(Dense(10, activation='relu'))\n",
        "modelConv1.add(Dropout(0.5))\n",
        "\n",
        "modelConv1.add(BatchNormalization())\n",
        "\n",
        "modelConv1.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "opt = SGD(lr=0.1)\n",
        "\n",
        "\n",
        "modelConv1.compile(loss='binary_crossentropy',\n",
        "              optimizer= 'adam',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "#modelConv1.summary()\n",
        "\n",
        "\n",
        "historyConv1=modelConv1.fit(x_train_pad, y_train,\n",
        "          batch_size=batchSize,\n",
        "          epochs=epochsNum,\n",
        "          validation_data=(x_test_pad, y_test), verbose = 1)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yvJPRApbsd4i"
      },
      "source": [
        "testLossConv, testAccConv = modelConv1.evaluate(x_test_pad, y_test, batch_size=batchSize)\n",
        "print('Test loss:', testLossConv)\n",
        "print('Test accuracy:', testAccConv)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3XBuMPYg9WUQ"
      },
      "source": [
        "trainLossConv, trainAccConv = modelConv1.evaluate(x_train_pad, y_train, batch_size=batchSize)\n",
        "print('Train loss:', trainLossConv)\n",
        "print('Train accuracy:', trainAccConv)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VFfT1l4U-FSx"
      },
      "source": [
        "#modelConv1.save(conv1ModelTxt)\n",
        "#modelConv1.save(conv1mModelH)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bwftiDG0-gvR"
      },
      "source": [
        "modelConv1.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7TjeuK4v-kPK"
      },
      "source": [
        "plt.title('Accuracy')\n",
        "plt.plot(historyConv1.history['accuracy'], label='train')\n",
        "plt.plot(historyConv1.history['val_accuracy'], label='test')\n",
        "plt.legend()\n",
        "plt.show();"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "czwbnbuq-k3D"
      },
      "source": [
        "plt.title('Loss')\n",
        "plt.plot(historyConv1.history['loss'], label='train')\n",
        "plt.plot(historyConv1.history['val_loss'], label='test')\n",
        "plt.legend()\n",
        "plt.show();"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SW0SSZbB-p_9"
      },
      "source": [
        "y_val_pred_c =modelConv1.predict_classes(x_test_pad)\n",
        "precisionConv1, recallConv1, fscoreConv1, supportConv1 = score(y_test, y_val_pred_c)\n",
        "\n",
        "print('precision: {}'.format(precisionConv1))\n",
        "print('recall: {}'.format(recallConv1))\n",
        "print('fscore: {}'.format(fscoreConv1))\n",
        "print('support: {}'.format(supportConv1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Hu6Ccx4EMyO"
      },
      "source": [
        "from sklearn.metrics import cohen_kappa_score\n",
        "\n",
        "cohen_score = cohen_kappa_score(y_test, y_val_pred_c)\n",
        "\n",
        "print(cohen_score)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y-yqPvvCENA-"
      },
      "source": [
        "import seaborn as sns\n",
        "from sklearn.metrics import confusion_matrix\n",
        "labels = [0,1]\n",
        "def cm_analysis(y_true, y_pred, labels, ymap=None, figsize=(10,10)):\n",
        "    \"\"\"\n",
        "    Generate matrix plot of confusion matrix with pretty annotations.\n",
        "    The plot image is saved to disk.\n",
        "    args: \n",
        "      y_true:    true label of the data, with shape (nsamples,)\n",
        "      y_pred:    prediction of the data, with shape (nsamples,)\n",
        "      filename:  filename of figure file to save\n",
        "      labels:    string array, name the order of class labels in the confusion matrix.\n",
        "                 use `clf.classes_` if using scikit-learn models.\n",
        "                 with shape (nclass,).\n",
        "      ymap:      dict: any -> string, length == nclass.\n",
        "                 if not None, map the labels & ys to more understandable strings.\n",
        "                 Caution: original y_true, y_pred and labels must align.\n",
        "      figsize:   the size of the figure plotted.\n",
        "    \"\"\"\n",
        "    if ymap is not None:\n",
        "        y_pred = [ymap[yi] for yi in y_pred]\n",
        "        y_true = [ymap[yi] for yi in y_true]\n",
        "        labels = [ymap[yi] for yi in labels]\n",
        "    cm = confusion_matrix(y_true, y_pred, labels=labels)\n",
        "    cm_sum = np.sum(cm, axis=1, keepdims=True)\n",
        "    cm_perc = cm / cm_sum.astype(float) * 100\n",
        "    annot = np.empty_like(cm).astype(str)\n",
        "    nrows, ncols = cm.shape\n",
        "    for i in range(nrows):\n",
        "        for j in range(ncols):\n",
        "            c = cm[i, j]\n",
        "            p = cm_perc[i, j]\n",
        "            if i == j:\n",
        "                s = cm_sum[i]\n",
        "                annot[i, j] = '%.1f%%\\n%d/%d' % (p, c, s)\n",
        "            elif c == 0:\n",
        "                annot[i, j] = ''\n",
        "            else:\n",
        "                annot[i, j] = '%.1f%%\\n%d' % (p, c)\n",
        "    cm = pd.DataFrame(cm, index=labels, columns=labels)\n",
        "    cm.index.name = 'Actual'\n",
        "    cm.columns.name = 'Predicted'\n",
        "    fig, ax = plt.subplots()\n",
        "    sns.heatmap(cm, annot=annot, fmt='', ax=ax)\n",
        "    #plt.savefig(filename)\n",
        "    plt.show()\n",
        "\n",
        "cm_analysis(y_test, y_val_pred_c, labels, ymap=None, figsize=(5,5))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "me0KibpuENWt"
      },
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt     \n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "\n",
        "labels = [0,1]\n",
        "cm = confusion_matrix(y_test, y_val_pred_c, labels)\n",
        "\n",
        "ax= plt.subplot()\n",
        "sns.heatmap(cm, annot=True, fmt=\"g\", ax = ax,); #annot=True to annotate cells\n",
        "\n",
        "# labels, title and ticks\n",
        "ax.set_xlabel('Predicted labels');\n",
        "ax.set_ylabel('True labels'); \n",
        "ax.set_title('Confusion Matrix'); \n",
        "ax.xaxis.set_ticklabels(labels); \n",
        "ax.yaxis.set_ticklabels(labels);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sRzssXDYCI-f"
      },
      "source": [
        "# Evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uPTewWYOGtT0"
      },
      "source": [
        "## Accuracy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aq0pbFrLCM67"
      },
      "source": [
        "dicTestAccuracy={\n",
        "      \"LSTM\"      :(\"{:.4f}\".format(testAccLSTM)),\n",
        "      \"Bilstm\":(\"{:.4f}\".format(testAccBilstm)),\n",
        "      \"GRU\"      :(\"{:.4f}\".format(testAccGRU)),\n",
        "      \"CNN\"     :(\"{:.4f}\".format(testAccConv)),\n",
        "      \"CNN+LSTM\"      :(\"{:.4f}\".format(testAccCL)),\n",
        "      \"CNN+GRU\"      :(\"{:.4f}\".format(testAccCGRU)),\n",
        "      \"CNN+BiLSTM\"      :(\"{:.4f}\".format(testAccCBiLSTM))\n",
        "      }"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "asqlP5OjCxSt"
      },
      "source": [
        "dfTestAccuracy = pd.DataFrame.from_dict(dicTestAccuracy, orient='index', columns=[\"Accuracy\"] )\n",
        "dfTestAccuracy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gCfheXn3Ggv7"
      },
      "source": [
        "dfTestAccuracy[\"Accuracy\"] = dfTestAccuracy[\"Accuracy\"].astype(float)\n",
        "dfTestAccuracy[\"Accuracy\"].plot(kind=\"bar\", grid=True , color=tuple([\"g\", \"b\", \"r\", \"y\", \"k\"]), width=0.3)\n",
        "plt.title(\"Accuracy\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eX1hY2cdGwv3"
      },
      "source": [
        "## Precision"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nRS3G7w4bI9G"
      },
      "source": [
        "precisionLSTM"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4QPgyTPjG4Ip"
      },
      "source": [
        "dicPrecision={\n",
        "      \"LSTM\"      :(\"{:.4f}\".format(precisionLSTM[0])),\n",
        "      \"Bilstm\":(\"{:.4f}\".format(precisionBilstm[0])),\n",
        "      \"GRU\"      :(\"{:.4f}\".format(precisionGRU[0])),\n",
        "      \"CNN\"     :(\"{:.4f}\".format(precisionConv1[0])),\n",
        "      \"CNN+LSTM\"      :(\"{:.4f}\".format(precisionCL[0])),\n",
        "      \"CNN+GRU\"      :(\"{:.4f}\".format(precisionCGRU[0])),\n",
        "      \"CNN+BiLSTM\"      :(\"{:.4f}\".format(precisionCBiLSTM[0]))\n",
        "      }\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ep1NaC5-HcgX"
      },
      "source": [
        "dfPrecision = pd.DataFrame.from_dict(dicPrecision, orient='index', columns=[\"Precision\"] )\n",
        "dfPrecision"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9zBwERtVHela"
      },
      "source": [
        "dfPrecision[\"Precision\"] = dfPrecision[\"Precision\"].astype(float)\n",
        "dfPrecision[\"Precision\"].plot(kind=\"bar\", grid=True , color=tuple([\"g\", \"b\", \"r\", \"y\", \"k\"]), width=0.3)\n",
        "plt.title(\"Precision\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QZXeaWWaIPnQ"
      },
      "source": [
        "## Recall"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eukDAiFVISpT"
      },
      "source": [
        "dicRecall={\n",
        "      \"LSTM\"      :(\"{:.4f}\".format(recallLSTM[0])),\n",
        "      \"Bilstm\":(\"{:.4f}\".format(recallBilstm[0])),\n",
        "      \"GRU\"      :(\"{:.4f}\".format(recallGRU[0])),\n",
        "      \"CNN\"     :(\"{:.4f}\".format(recallConv1[0])),\n",
        "      \"CNN+LSTM\"      :(\"{:.4f}\".format(recallCL[0])),\n",
        "      \"CNN+GRU\"      :(\"{:.4f}\".format(recallCGRU[0])),\n",
        "      \"CNN+BiLSTM\"      :(\"{:.4f}\".format(recallCBiLSTM[0]))\n",
        "      }"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3w591WirIUrN"
      },
      "source": [
        "dfRecall = pd.DataFrame.from_dict(dicRecall, orient='index', columns=[\"Recall\"] )\n",
        "dfRecall"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Io6r23LtIZuq"
      },
      "source": [
        "dfRecall[\"Recall\"] = dfRecall[\"Recall\"].astype(float)\n",
        "dfRecall[\"Recall\"].plot(kind=\"bar\", grid=True , color=tuple([\"g\", \"b\", \"r\", \"y\", \"k\"]), width=0.3)\n",
        "plt.title(\"Recall\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Beq11JiYJB54"
      },
      "source": [
        "## Fscore"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MTaSeEtzJFWc"
      },
      "source": [
        "dicFscore={\n",
        "      \"LSTM\"      :(\"{:.4f}\".format(fscoreLSTM[0])),\n",
        "      \"Bilstm\":(\"{:.4f}\".format(fscoreBilstm[0])),\n",
        "      \"GRU\"      :(\"{:.4f}\".format(fscoreGRU[0])),\n",
        "      \"CNN\"     :(\"{:.4f}\".format(fscoreConv1[0])),\n",
        "      \"CNN+LSTM\"      :(\"{:.4f}\".format(fscoreCL[0])),\n",
        "      \"CNN+GRU\"      :(\"{:.4f}\".format(fscoreCGRU[0])),\n",
        "      \"CNN+BiLSTM\"      :(\"{:.4f}\".format(fscoreCBiLSTM[0]))\n",
        "      }"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nh2EXieZJJwx"
      },
      "source": [
        "dfFscore = pd.DataFrame.from_dict(dicFscore, orient='index', columns=[\"Fscore\"] )\n",
        "dfFscore"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VHmGS-GcJLkV"
      },
      "source": [
        "dfFscore[\"Fscore\"] = dfFscore[\"Fscore\"].astype(float)\n",
        "dfFscore[\"Fscore\"].plot(kind=\"bar\", grid=True , color=tuple([\"g\", \"b\", \"r\", \"y\", \"k\"]), width=0.3)\n",
        "plt.title(\"Fscore\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-CQaBfqPeVZN"
      },
      "source": [
        "##ALL"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CP7uCkl2eYNL"
      },
      "source": [
        "dicALL={\n",
        "      \"Accuracy\":[\n",
        "                  (\"{:.4f}\".format(testAccLSTM)),\n",
        "                  (\"{:.4f}\".format(testAccBilstm)),\n",
        "                  (\"{:.4f}\".format(testAccGRU)),\n",
        "                  (\"{:.4f}\".format(testAccConv)),\n",
        "                  (\"{:.4f}\".format(testAccCL)),\n",
        "                  (\"{:.4f}\".format(testAccCGRU)),\n",
        "                  (\"{:.4f}\".format(testAccCBiLSTM))\n",
        "                 ],\n",
        "      \n",
        "      \"Precision\":[\n",
        "                  (\"{:.4f}\".format(precisionLSTM[0])),\n",
        "                  (\"{:.4f}\".format(precisionBilstm[0])),\n",
        "                  (\"{:.4f}\".format(precisionGRU[0])),\n",
        "                  (\"{:.4f}\".format(precisionConv1[0])),\n",
        "                  (\"{:.4f}\".format(precisionCL[0])),\n",
        "                  (\"{:.4f}\".format(precisionCGRU[0])),\n",
        "                  (\"{:.4f}\".format(precisionCBiLSTM[0])) \n",
        "                 ],\n",
        "      \n",
        "      \"Recall\":[\n",
        "                (\"{:.4f}\".format(recallLSTM[0])),\n",
        "                (\"{:.4f}\".format(recallBilstm[0])),\n",
        "                (\"{:.4f}\".format(recallGRU[0])),\n",
        "                (\"{:.4f}\".format(recallConv1[0])),\n",
        "                (\"{:.4f}\".format(recallCL[0])),\n",
        "                (\"{:.4f}\".format(recallCGRU[0])),\n",
        "                (\"{:.4f}\".format(recallCBiLSTM[0]))\n",
        "                 ],\n",
        "        \n",
        "      \"F1\":[\n",
        "            (\"{:.4f}\".format(fscoreLSTM[0])),\n",
        "            (\"{:.4f}\".format(fscoreBilstm[0])),\n",
        "            (\"{:.4f}\".format(fscoreGRU[0])),\n",
        "            (\"{:.4f}\".format(fscoreConv1[0])),\n",
        "            (\"{:.4f}\".format(fscoreCL[0])),\n",
        "            (\"{:.4f}\".format(fscoreCGRU[0])),\n",
        "            (\"{:.4f}\".format(fscoreCBiLSTM[0]))\n",
        "           ],\n",
        "     \"Loss\":[\n",
        "            (\"{:.4f}\".format(testLossLSTM)),\n",
        "            (\"{:.4f}\".format(testLossBilstm)),\n",
        "            (\"{:.4f}\".format(testLossGRU)),\n",
        "            (\"{:.4f}\".format(testLossConv)),\n",
        "            (\"{:.4f}\".format(testLossCL)),\n",
        "            (\"{:.4f}\".format(testLossCGRU)),\n",
        "            (\"{:.4f}\".format(testLossCBiLSTM))\n",
        "           ]\n",
        "        \n",
        "        \n",
        "        }\n",
        "\n",
        "\n",
        "dfAllMeasures=pd.DataFrame.from_dict( dicALL , orient='index')#,index=['SIMPLE RNN','LSTM MODEL','GRU MODEL','SVR MODEL','SARIMAX MODEL'],\n",
        "                                 #columns=[\"MSE\",\"MAE\"] )\n",
        "dfAllMeasures.columns=['LSTM','Bilstm','GRU','Conv1','CNN+LSTM', 'CNN+GRU', 'CNN+BiLSTM']\n",
        "dfAllMeasures = dfAllMeasures.T\n",
        "dfAllMeasures"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}