{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yT7Rfe0eKG3x"
      },
      "source": [
        "This module is resposible for\n",
        "\n",
        "\n",
        "*   Preprocessing\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4CZ8jTPXsX-E"
      },
      "source": [
        "# Configuration and imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1WxrHiDfKG4V",
        "outputId": "8f0dda11-31e6-440e-cd37-93a8cd0b5311"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: spacy in /usr/local/lib/python3.7/dist-packages (2.2.4)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy) (57.4.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.23.0)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.0.6)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (4.62.3)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy) (3.0.6)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.9.0)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.0.0)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (7.4.0)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.21.5)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.0.6)\n",
            "Requirement already satisfied: importlib-metadata>=0.20 in /usr/local/lib/python3.7/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy) (4.11.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy) (3.7.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy) (3.10.0.2)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2021.10.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.0.4)\n"
          ]
        }
      ],
      "source": [
        "pip install spacy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lRmIqv4K6R9W",
        "outputId": "a82b3233-703c-4252-b163-9069bccaac51"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Package words is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        }
      ],
      "source": [
        "import csv\n",
        "import pprint\n",
        "import time\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.corpus import words\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from gensim import corpora, models, similarities\n",
        "from itertools import chain\n",
        "from collections import Counter\n",
        "import spacy\n",
        "import gensim, logging\n",
        "import nltk\n",
        "from time import time\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import string\n",
        "from nltk.corpus import wordnet as wnpi\n",
        "import pickle\n",
        "from gensim.models import Word2Vec\n",
        "\n",
        "\n",
        "import re\n",
        "from functools import partial\n",
        "from collections import Counter\n",
        "import nltk\n",
        "from nltk.corpus import wordnet\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "\n",
        "\n",
        "#import techniques\n",
        "#from techniques import *\n",
        "nltk.download('wordnet')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('words')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qo5fkNy0slaZ"
      },
      "outputs": [],
      "source": [
        "txtSpellFile = '/MYDRIVE/My Drive/Colab Notebooks/SecondPaper/data/corporaForSpellCorrection.txt'\n",
        "txtSlangFile = '/MYDRIVE/My Drive/Colab Notebooks/SecondPaper/data/slang.txt'\n",
        "csvDestinationFileName = '/MYDRIVE/My Drive/Colab Notebooks/SecondPaper/data/PreProcessedPang-Ver2-00-11-27.csv'\n",
        "csvSourceFileName = '/MYDRIVE/My Drive/Colab Notebooks/SecondPaper/data/Pang-Ver2-00-10-24.csv'\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hK5wVyhfssBQ",
        "outputId": "5f2dd98d-4534-4ee7-f6ec-de60be19d22a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /MYDRIVE\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/MYDRIVE', force_remount=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y0RVxyoVmpGs"
      },
      "source": [
        "# commpn functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gjRRHjkyBprM"
      },
      "outputs": [],
      "source": [
        "\n",
        "\"\"\" Creates a dictionary with slangs and their equivalents and replaces them \"\"\"\n",
        "with open(txtSlangFile) as file:\n",
        "    slang_map = dict(map(str.strip, line.partition('\\t')[::2])\n",
        "    for line in file if line.strip())\n",
        "\n",
        "slang_words = sorted(slang_map, key=len, reverse=True) # longest first for regex\n",
        "regex = re.compile(r\"\\b({})\\b\".format(\"|\".join(map(re.escape, slang_words))))\n",
        "replaceSlang = partial(regex.sub, lambda m: slang_map[m.group(1)])\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_b-wIpdmCZr0"
      },
      "outputs": [],
      "source": [
        "\"\"\" Replaces contractions from a string to their equivalents \"\"\"\n",
        "contraction_patterns = [ (r'won\\'t', 'will not'), (r'can\\'t', 'cannot'), (r'i\\'m', 'i am'), (r'ain\\'t', 'is not'), (r'(\\w+)\\'ll', '\\g<1> will'), (r'(\\w+)n\\'t', '\\g<1> not'),\n",
        "                         (r'(\\w+)\\'ve', '\\g<1> have'), (r'(\\w+)\\'s', '\\g<1> is'), (r'(\\w+)\\'re', '\\g<1> are'), (r'(\\w+)\\'d', '\\g<1> would'), (r'&', 'and'), (r'dammit', 'damn it'), (r'dont', 'do not'), (r'wont', 'will not') ]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FphdRandA9q4"
      },
      "outputs": [],
      "source": [
        "def removeUnicode(text):\n",
        "    \"\"\" Removes unicode strings like \"\\u002c\" and \"x96\" \"\"\"\n",
        "    text = re.sub(r'(\\\\u[0-9A-Fa-f]+)',r'', text)       \n",
        "    text = re.sub(r'[^\\x00-\\x7f]',r'',text)\n",
        "    return text\n",
        "\n",
        "def replaceURL(text):\n",
        "    \"\"\" Replaces url address with \"url\" \"\"\"\n",
        "    #edited by seilsepour\n",
        "    text = re.sub('((www\\.[^\\s]+)|(https?://[^\\s]+))','',text)\n",
        "    text = re.sub(r'#([^\\s]+)', r'\\1', text)\n",
        "    return text\n",
        "\n",
        "def replaceAtUser(text):\n",
        "    \"\"\" Replaces \"@user\" with \"atUser\" \"\"\"\n",
        "    #edited by seilsepour\n",
        "    #text = re.sub('@[^\\s]+','atUser',text)\n",
        "    text = re.sub('@[^\\s]+','',text)\n",
        "    return text\n",
        "\n",
        "def removeHashtagInFrontOfWord(text):\n",
        "    \"\"\" Removes hastag in front of a word \"\"\"\n",
        "    text = re.sub(r'#([^\\s]+)', r'\\1', text)\n",
        "    return text\n",
        "\n",
        "def removeNumbers(text):\n",
        "    \"\"\" Removes integers \"\"\"\n",
        "    text = ''.join([i for i in text if not i.isdigit()])         \n",
        "    return text\n",
        "\n",
        "def replaceMultiExclamationMark(text):\n",
        "    \"\"\" Replaces repetitions of exlamation marks \"\"\"\n",
        "    #edited by seilsepour\n",
        "    #text = re.sub(r\"(\\!)\\1+\", ' multiExclamation ', text)\n",
        "    text = re.sub(r\"(\\!)\\1+\", '', text)\n",
        "    return text\n",
        "\n",
        "def replaceMultiQuestionMark(text):\n",
        "    \"\"\" Replaces repetitions of question marks \"\"\"\n",
        "    #edited by seilsepour\n",
        "    #text = re.sub(r\"(\\?)\\1+\", ' multiQuestion ', text)\n",
        "    text = re.sub(r\"(\\?)\\1+\", '', text)\n",
        "    return text\n",
        "\n",
        "def replaceMultiStopMark(text):\n",
        "    \"\"\" Replaces repetitions of stop marks \"\"\"\n",
        "    #edited by seilsepour\n",
        "    #text = re.sub(r\"(\\.)\\1+\", ' multiStop ', text)\n",
        "    text = re.sub(r\"(\\.)\\1+\", '', text)\n",
        "    return text\n",
        "\n",
        "def countMultiExclamationMarks(text):\n",
        "    \"\"\" Replaces repetitions of exlamation marks \"\"\"\n",
        "    return text #len(re.findall(r\"(\\!)\\1+\", text))\n",
        "\n",
        "def countMultiQuestionMarks(text):\n",
        "  return text\n",
        "  #return len(re.findall(r\"(\\?)\\1+\", text))\n",
        "\n",
        "def countMultiStopMarks(text):\n",
        "  return text\n",
        "  #return len(re.findall(r\"(\\.)\\1+\", text))\n",
        "\n",
        "def countElongated(text):\n",
        "  return text\n",
        "  #regex = re.compile(r\"(.)\\1{2}\")\n",
        "  #return len([word for word in text.split() if regex.search(word)])\n",
        "\n",
        "def countAllCaps(text):\n",
        "  return text\n",
        "  #return len(re.findall(\"[A-Z0-9]{3,}\", text))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oFJVrAvlBIiH"
      },
      "outputs": [],
      "source": [
        "def countSlang(text):\n",
        "    \"\"\" Input: a text, Output: how many slang words and a list of found slangs \"\"\"\n",
        "    slangCounter = 0\n",
        "    slangsFound = []\n",
        "    tokens = nltk.word_tokenize(text)\n",
        "    for word in tokens:\n",
        "        if word in slang_words:\n",
        "            slangsFound.append(word)\n",
        "            slangCounter += 1\n",
        "    return 0, 0 #slangCounter, slangsFound\n",
        "\n",
        "def replaceContraction(text):\n",
        "    patterns = [(re.compile(regex), repl) for (regex, repl) in contraction_patterns]\n",
        "    for (pattern, repl) in patterns:\n",
        "        (text, count) = re.subn(pattern, repl, text)\n",
        "    return text\n",
        "\n",
        "def replaceElongated(word):\n",
        "    \"\"\" Replaces an elongated word with its basic form, unless the word exists in the lexicon \"\"\"\n",
        "    #return word\n",
        "    repeat_regexp = re.compile(r'(\\w*)(\\w)\\2(\\w*)')\n",
        "    repl = r'\\1\\2\\3'\n",
        "    if wordnet.synsets(word):\n",
        "        return word\n",
        "    repl_word = repeat_regexp.sub(repl, word)\n",
        "    if repl_word != word:      \n",
        "        return replaceElongated(repl_word)\n",
        "    else:       \n",
        "        return repl_word\n",
        "\n",
        "def removeEmoticons(text):\n",
        "    \"\"\" Removes emoticons from text \"\"\"\n",
        "    text = re.sub(':\\)|;\\)|:-\\)|\\(-:|:-D|=D|:P|xD|X-p|\\^\\^|:-*|\\^\\.\\^|\\^\\-\\^|\\^\\_\\^|\\,-\\)|\\)-:|:\\'\\(|:\\(|:-\\(|:\\S|T\\.T|\\.\\_\\.|:<|:-\\S|:-<|\\*\\-\\*|:O|=O|=\\-O|O\\.o|XO|O\\_O|:-\\@|=/|:/|X\\-\\(|>\\.<|>=\\(|D:', '', text)\n",
        "    return text\n",
        "\n",
        "def countEmoticons(text):\n",
        "    \"\"\" Input: a text, Output: how many emoticons \"\"\"\n",
        "    return 0 #len(re.findall(':\\)|;\\)|:-\\)|\\(-:|:-D|=D|:P|xD|X-p|\\^\\^|:-*|\\^\\.\\^|\\^\\-\\^|\\^\\_\\^|\\,-\\)|\\)-:|:\\'\\(|:\\(|:-\\(|:\\S|T\\.T|\\.\\_\\.|:<|:-\\S|:-<|\\*\\-\\*|:O|=O|=\\-O|O\\.o|XO|O\\_O|:-\\@|=/|:/|X\\-\\(|>\\.<|>=\\(|D:', text))\n",
        "\n",
        "\n",
        "### Spell Correction begin ###\n",
        "\"\"\" Spell Correction http://norvig.com/spell-correct.html \"\"\"\n",
        "def words(text): \n",
        "  return re.findall(r'\\w+', text.lower())\n",
        "  #return text\n",
        "\n",
        "WORDS = Counter(words(open(txtSpellFile).read()))\n",
        "\n",
        "def P(word, N=sum(WORDS.values())): \n",
        "    return WORDS[word] / N\n",
        "\n",
        "def spellCorrection(word): \n",
        "    \"\"\" Most probable spelling correction for word. \"\"\"\n",
        "    return max(candidates(word), key=P)\n",
        "    #return word\n",
        "\n",
        "def candidates(word): \n",
        "    \"\"\" Generate possible spelling corrections for word. \"\"\"\n",
        "    return (known([word]) or known(edits1(word)) or known(edits2(word)) or [word])\n",
        "    #return word\n",
        "\n",
        "def known(words): \n",
        "    \"\"\" The subset of `words` that appear in the dictionary of WORDS. \"\"\"\n",
        "    return set(w for w in words if w in WORDS)\n",
        "    #return words\n",
        "\n",
        "def edits1(word):\n",
        "    \"\"\" All edits that are one edit away from `word`. \"\"\"\n",
        "    letters    = 'abcdefghijklmnopqrstuvwxyz'\n",
        "    splits     = [(word[:i], word[i:])    for i in range(len(word) + 1)]\n",
        "    deletes    = [L + R[1:]               for L, R in splits if R]\n",
        "    transposes = [L + R[1] + R[0] + R[2:] for L, R in splits if len(R)>1]\n",
        "    replaces   = [L + c + R[1:]           for L, R in splits if R for c in letters]\n",
        "    inserts    = [L + c + R               for L, R in splits for c in letters]\n",
        "    return set(deletes + transposes + replaces + inserts)\n",
        "\n",
        "def edits2(word): \n",
        "    \"\"\" All edits that are two edits away from `word`. \"\"\"\n",
        "    return (e2 for e1 in edits1(word) for e2 in edits1(e1))\n",
        "\n",
        "### Spell Correction End ###\n",
        "\n",
        "### Replace Negations Begin ###\n",
        "\n",
        "def replace(word, pos=None):\n",
        "    \"\"\" Creates a set of all antonyms for the word and if there is only one antonym, it returns it \"\"\"\n",
        "    antonyms = set()\n",
        "    for syn in wordnet.synsets(word, pos=pos):\n",
        "      for lemma in syn.lemmas():\n",
        "        for antonym in lemma.antonyms():\n",
        "          antonyms.add(antonym.name())\n",
        "    if len(antonyms) == 1:\n",
        "      return antonyms.pop()\n",
        "    else:\n",
        "      return None\n",
        "\n",
        "def replaceNegations(text):\n",
        "    \"\"\" Finds \"not\" and antonym for the next word and if found, replaces not and the next word with the antonym \"\"\"\n",
        "    i, l = 0, len(text)\n",
        "    words = []\n",
        "    while i < l:\n",
        "      word = text[i]\n",
        "      if word == 'not' and i+1 < l:\n",
        "        ant = replace(text[i+1])\n",
        "        if ant:\n",
        "          words.append(ant)\n",
        "          i += 2\n",
        "          continue\n",
        "      words.append(word)\n",
        "      i += 1\n",
        "    return words\n",
        "\n",
        "### Replace Negations End ###\n",
        "\n",
        "def addNotTag(text):\n",
        "\t\"\"\" Finds \"not,never,no\" and adds the tag NEG_ to all words that follow until the next punctuation \"\"\"\n",
        "\treturn text\n",
        "  #transformed = re.sub(r'\\b(?:not|never|no)\\b[\\w\\s]+[^\\w\\s]', \n",
        "  #     lambda match: re.sub(r'(\\s+)(\\w+)', r'\\1NEG_\\2', match.group(0)), \n",
        "  #     text,\n",
        "  #     flags=re.IGNORECASE)\n",
        "\t#return transformed\n",
        "  #return text\n",
        "\n",
        "def addCapTag(word):\n",
        "    \"\"\" Finds a word with at least 3 characters capitalized and adds the tag ALL_CAPS_ \"\"\"\n",
        "    return word\n",
        "    #if(len(re.findall(\"[A-Z]{3,}\", word))):\n",
        "    #    word = word.replace('\\\\', '' )\n",
        "    #    transformed = re.sub(\"[A-Z]{3,}\", \"ALL_CAPS_\"+word, word)\n",
        "    #   return transformed\n",
        "    #else:\n",
        "    #    return word\n",
        "    #return word\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hTme210DSVv1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nU2VrIGo6R9l",
        "outputId": "4a922e4f-c521-479f-b4fb-645ac9e1830f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting preprocess..\n",
            "\n"
          ]
        }
      ],
      "source": [
        "\"\"\" Copyright 2017, Dimitrios Effrosynidis, All rights reserved. \"\"\"\n",
        "\n",
        "\n",
        "print(\"Starting preprocess..\\n\")\n",
        "\n",
        "\"\"\" Tokenizes a text to its words, removes and replaces some of them \"\"\"    \n",
        "finalTokens = [] # all tokens\n",
        "stoplist = stopwords.words('english')\n",
        "my_stopwords = \"multiexclamation multiquestion multistop url atuser st rd nd th am pm of for\" # my extra stopwords\n",
        "stoplist = stoplist + my_stopwords.split()\n",
        "allowedWordTypes = [\"J\",\"R\",\"V\",\"N\"] #  J is Adject, R is Adverb, V is Verb, N is Noun. These are used for POS Tagging\n",
        "lemmatizer = WordNetLemmatizer() # set lemmatizer\n",
        "#stemmer = PorterStemmer() # set stemmer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q84H-JpCxqgD"
      },
      "source": [
        "# Loading the input file "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QylbmY0kxo27"
      },
      "outputs": [],
      "source": [
        "csvDestinationFile = open(csvDestinationFileName, 'a+')\n",
        "csvWriter = csv.writer(csvDestinationFile)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fBdnbB2cA2HA",
        "outputId": "2ec222cd-e9c6-4003-b7a6-03f685af66ab"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/MYDRIVE/My Drive/Colab Notebooks/SecondPaper/data/PreProcessedPang-Ver2-00-11-27.csv'"
            ]
          },
          "metadata": {},
          "execution_count": 54
        }
      ],
      "source": [
        "csvDestinationFileName"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wjkH1dZnIqzn",
        "outputId": "2077ae49-71a0-4ef1-b4c8-b28867b989c9"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/MYDRIVE/My Drive/Colab Notebooks/SecondPaper/data/Pang-Ver2-00-10-24.csv'"
            ]
          },
          "metadata": {},
          "execution_count": 55
        }
      ],
      "source": [
        "csvSourceFileName"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jleMu3Fo6R9-",
        "outputId": "d3f15d73-5bc1-403e-f941-a0628afd12b8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading and loading dataset to memory\n",
            "The dataset is loaded\n",
            "the original dataset shape is (2000, 3)\n"
          ]
        }
      ],
      "source": [
        "\"\"\" Copyright 2017, Dimitrios Effrosynidis, All rights reserved. \"\"\"\n",
        "print(\"Reading and loading dataset to memory\")\n",
        "#f = open(\"ss-twitterfinal.txt\",\"r\", encoding=\"utf8\", errors='replace').read()\n",
        "#df_original = pd.read_excel(DATASET_PATH, sheet_name=\"Stream2\")\n",
        "df_original = pd.read_csv(csvSourceFileName ,  header=None)\n",
        "df_proccessed = df_original\n",
        "\n",
        "print(\"The dataset is loaded\")\n",
        "print(\"the original dataset shape is\",df_original.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VUoFa316FkkY",
        "outputId": "178f3db0-8052-4f86-dca3-3c81c071ddcb"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-331a5862-4e6d-41bf-b69c-4bde761ed7e2\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>films adapted from comic books have had plenty...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>every now and then a movie comes along from a ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>you ve got mail works alot better than it dese...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>jaws   is a rare film that grabs your atten...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>moviemaking is a lot like being the general ma...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>on june 30   1960   a self taught   idealistic...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>apparently   director tony kaye had a major ba...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>8</td>\n",
              "      <td>1</td>\n",
              "      <td>one of my colleagues was surprised when i told...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>9</td>\n",
              "      <td>1</td>\n",
              "      <td>after bloody clashes and independence won   lu...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>10</td>\n",
              "      <td>1</td>\n",
              "      <td>the american action film has been slowly drown...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-331a5862-4e6d-41bf-b69c-4bde761ed7e2')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-331a5862-4e6d-41bf-b69c-4bde761ed7e2 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-331a5862-4e6d-41bf-b69c-4bde761ed7e2');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "    0  1                                                  2\n",
              "0   1  1  films adapted from comic books have had plenty...\n",
              "1   2  1  every now and then a movie comes along from a ...\n",
              "2   3  1  you ve got mail works alot better than it dese...\n",
              "3   4  1     jaws   is a rare film that grabs your atten...\n",
              "4   5  1  moviemaking is a lot like being the general ma...\n",
              "5   6  1  on june 30   1960   a self taught   idealistic...\n",
              "6   7  1  apparently   director tony kaye had a major ba...\n",
              "7   8  1  one of my colleagues was surprised when i told...\n",
              "8   9  1  after bloody clashes and independence won   lu...\n",
              "9  10  1  the american action film has been slowly drown..."
            ]
          },
          "metadata": {},
          "execution_count": 57
        }
      ],
      "source": [
        "df_proccessed.head(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SW1ZttnDxzer"
      },
      "source": [
        "# Start to process "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sRCU3YDz6R-f",
        "outputId": "f18b5ff8-59ce-46a9-cc12-75f58f2c6b96"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The processing tweets start...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:20: DeprecationWarning: invalid escape sequence '\\ '\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The processing tweets end\n"
          ]
        }
      ],
      "source": [
        "print(\"The processing tweets start...\")\n",
        "t0 = time()\n",
        "totalSentences = 0\n",
        "totalEmoticons = 0\n",
        "totalSlangs = 0\n",
        "totalSlangsFound = []\n",
        "totalElongated = 0\n",
        "totalMultiExclamationMarks = 0\n",
        "totalMultiQuestionMarks = 0\n",
        "totalMultiStopMarks = 0\n",
        "totalAllCaps = 0\n",
        "\n",
        "i = 0\n",
        "for  row in df_proccessed.itertuples(index=False):\n",
        "    t1 = time()  \n",
        "    strTemp = row[2]\n",
        "    onlyOneSentenceTokens = [] # tokens of one sentence each time\n",
        "    \n",
        "    try:\n",
        "     strTemp = strTemp.encode('ascii', 'ignore').decode('unicode_escape')\n",
        "    except: \n",
        "     pass\n",
        "    \n",
        "    strTemp = str(strTemp).lower()\n",
        "    \n",
        "     \n",
        "    strTemp = re.sub(r'(\\\\u[0-9A-Fa-f]+)',r'', strTemp) \n",
        "      \n",
        "    strTemp = re.sub(r'[^\\x00-\\x7f]',r'',strTemp)\n",
        "    \n",
        "    strTemp = replaceURL(strTemp) # Technique 1\n",
        "    \n",
        "    strTemp = replaceAtUser(strTemp) # Technique 1\n",
        "    \n",
        "    strTemp = removeHashtagInFrontOfWord(strTemp) # Technique 1\n",
        "    \n",
        "    strTemp = removeNumbers(strTemp)\n",
        "    \n",
        "    strTemp = removeEmoticons(strTemp)\n",
        "    #temp_slangs, temp_slangsFound = countSlang(strTemp)\n",
        "    #totalSlangs += temp_slangs # total slangs for all sentences\n",
        "    #for word in temp_slangsFound:\n",
        "    #    totalSlangsFound.append(word) # all the slangs found in all sentences\n",
        "    \n",
        "    #strTemp = replaceSlang(strTemp) # Technique 2: replaces slang words and abbreviations with their equivalents\n",
        "    strTemp = replaceContraction(strTemp) # Technique 3: replaces contractions to their equivalents\n",
        "    strTemp = removeNumbers(strTemp) # Technique 4: remove integers from text\n",
        "\n",
        "    emoticons = countEmoticons(strTemp) # how many emoticons in this sentence\n",
        "    totalEmoticons += emoticons\n",
        "    \n",
        "    strTemp = removeEmoticons(strTemp) # removes emoticons from text\n",
        "\n",
        "    \n",
        "    #totalAllCaps += countAllCaps(strTemp)\n",
        "\n",
        "    \n",
        "    strTemp = replaceMultiExclamationMark(strTemp) # Technique 5: replaces repetitions of exlamation marks with the tag \"multiExclamation\"\n",
        "    strTemp = replaceMultiQuestionMark(strTemp) # Technique 5: replaces repetitions of question marks with the tag \"multiQuestion\"\n",
        "    strTemp = replaceMultiStopMark(strTemp) # Technique 5: replaces repetitions of stop marks with the tag \"multiStop\"\n",
        "    \n",
        "    \n",
        "    translator = str.maketrans('', '', string.punctuation)\n",
        "    strTemp = strTemp.translate(translator) # Technique 7: remove punctuation\n",
        "    \n",
        "    #totalElongated += countElongated(strTemp) # how many elongated words emoticons in this sentence\n",
        "    \n",
        "\n",
        "    tokens = nltk.word_tokenize(strTemp) # it takes a text as an input and provides a list of every token in it\n",
        "    stopwords = stoplist\n",
        "    tokens = [ w for w in tokens if not w in stopwords ]\n",
        "### NO POS TAGGING BEGIN (If you don't want to use POS Tagging keep this section uncommented) ###\n",
        "    \n",
        "    for w in tokens:\n",
        "\n",
        "        if (w not in stoplist): # Technique 10: remove stopwords\n",
        "            #final_word = addCapTag(w) # Technique 8: Finds a word with at least 3 characters capitalized and adds the tag ALL_CAPS_\n",
        "            final_word = w.lower() # Technique 9: lowercases all characters\n",
        "            final_word = replaceElongated(final_word) # Technique 11: replaces an elongated word with its basic form, unless the word exists in the lexicon\n",
        "            #if len(final_word)>1:\n",
        "            #    final_word = spellCorrection(final_word) # Technique 12: correction of spelling errors\n",
        "            #final_word = lemmatizer.lemmatize(final_word) # Technique 14: lemmatizes words\n",
        "            final_word = stemmer.stem(final_word) # Technique 15: apply stemming to words\n",
        "            onlyOneSentenceTokens.append(final_word) \n",
        "    \n",
        "    #tokens = [ w.lower() for w in tokens]\n",
        "    #stopWords = set(stopwords.words('english'))\n",
        "\n",
        "    \n",
        "    \n",
        "    onlyOneSentenceTokens = \" \".join(onlyOneSentenceTokens) \n",
        "\n",
        "    csvWriter.writerow([ str(row[0]) , onlyOneSentenceTokens, str(row[1]) ])\n",
        "    #csvWriter.writerow([  onlyOneSentenceTokens, str(row[0]) ])\n",
        "    #i = i + 1\n",
        "    \n",
        "\n",
        "\n",
        "print(\"The processing tweets end\")\n",
        "    \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "36DdZ9786R-r"
      },
      "outputs": [],
      "source": [
        "csvDestinationFile.close()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}